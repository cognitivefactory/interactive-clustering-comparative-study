{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb684e4e",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : EFFICIENCE STUDY ====\n",
    "> ### Stage 4 : Export experiments for other studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de920f",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2291fea",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at export experiments needed to run other studies**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[PREPROCESSING]/[VECTORIZATION]/[SAMPLING]/[CLUSTERING]/[EXPERIMENT]`.\n",
    "- Experiments have to be run and evaluated in order to analyze convergence speed.\n",
    "\n",
    "Before running, **run the notebook `2_Run_until_convergence_and_evaluate_efficience.ipynb` to run and evaluate each experiment you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17f5a0",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import listing_envs\n",
    "from typing import Any, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9ec13",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5deff24",
   "metadata": {},
   "source": [
    "Find all experiment environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac93bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_experiment_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa485d",
   "metadata": {},
   "source": [
    "Check `.temp/exports` folder exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../.temp/exports\"):\n",
    "    os.mkdir(\"../.temp/exports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "    \n",
    "    # Initialize export.\n",
    "    export_filename: str = \"../.temp/exports/\" + \"_-_\".join(env.split(\"/\")[2:-1]) + \".json\"\n",
    "    export_data: Dict[str, Any] = {}\n",
    "    # Experiment name\n",
    "    export_data[\"_ENV_PATH\"] = env\n",
    "    # Load texts.\n",
    "    with open(env + \"../../../../../dict_of_texts.json\", \"r\") as texts_file_r:\n",
    "        export_data[\"dict_of_texts\"] = json.load(texts_file_r)\n",
    "    # Load true intents.\n",
    "    with open(env + \"../../../../../dict_of_true_intents.json\", \"r\") as true_intents_file_r:\n",
    "        export_data[\"dict_of_true_intents\"] = json.load(true_intents_file_r)\n",
    "    # Load preprocessed texts.\n",
    "    with open(env + \"../../../../dict_of_preprocessed_texts.json\", \"r\") as preprocessed_texts_file_r:\n",
    "        export_data[\"dict_of_preprocessed_texts\"] = json.load(preprocessed_texts_file_r)\n",
    "    # Load constraints.\n",
    "    with open(env + \"dict_of_constraints_annotations.json\", \"r\") as constraints_file_r:\n",
    "        export_data[\"dict_of_constraints_annotations\"] = json.load(constraints_file_r)\n",
    "    # Load clustering.\n",
    "    with open(env + \"dict_of_clustering_results.json\", \"r\") as clustering_file_r:\n",
    "        export_data[\"dict_of_clustering_results\"] = json.load(clustering_file_r)\n",
    "    # Load dataset configurations.\n",
    "    with open(env + \"../../../../../config.json\", \"r\") as dataset_config_file_r:\n",
    "        dataset_config = json.load(dataset_config_file_r)\n",
    "        export_data[\"dataset_config\"] = {\n",
    "            \"file_name\": dataset_config[\"file_name\"],\n",
    "            \"sheet_name\": dataset_config[\"sheet_name\"],\n",
    "            \"language\": dataset_config[\"language\"],\n",
    "        }\n",
    "    # Load preprocessing configurations.\n",
    "    with open(env + \"../../../../config.json\", \"r\") as preprocessing_config_file_r:\n",
    "        preprocessing_config = json.load(preprocessing_config_file_r)\n",
    "        export_data[\"preprocessing_config\"] = {\n",
    "            \"apply_preprocessing\": preprocessing_config[\"apply_preprocessing\"],\n",
    "            \"apply_lemmatization\": preprocessing_config[\"apply_lemmatization\"],\n",
    "            \"apply_parsing_filter\": preprocessing_config[\"apply_parsing_filter\"],\n",
    "            \"spacy_language_model\": preprocessing_config[\"spacy_language_model\"],\n",
    "        }\n",
    "    # Load vectorization configurations.\n",
    "    with open(env + \"../../../config.json\", \"r\") as vectorization_config_file_r:\n",
    "        vectorization_config = json.load(vectorization_config_file_r)\n",
    "        export_data[\"vectorization_config\"] = {\n",
    "            \"vectorizer_type\": vectorization_config[\"vectorizer_type\"],\n",
    "            \"spacy_language_model\": vectorization_config[\"spacy_language_model\"],\n",
    "        }\n",
    "    # Load sampling configurations.\n",
    "    with open(env + \"../../config.json\", \"r\") as sampling_config_file_r:\n",
    "        sampling_config = json.load(sampling_config_file_r)\n",
    "        export_data[\"sampling_config\"] = {\n",
    "            \"algorithm\": sampling_config[\"algorithm\"],\n",
    "            \"nb_to_select\": sampling_config[\"nb_to_select\"],\n",
    "        }\n",
    "    # Load clustering configurations.\n",
    "    with open(env + \"../config.json\", \"r\") as clustering_config_file_r:\n",
    "        clustering_config = json.load(clustering_config_file_r)\n",
    "        export_data[\"clustering_config\"] = {\n",
    "            \"algorithm\": clustering_config[\"algorithm\"],\n",
    "            \"init**kargs\": clustering_config[\"init**kargs\"],\n",
    "            \"nb_clusters\": clustering_config[\"nb_clusters\"],\n",
    "        }\n",
    "    # Load experiment configurations.\n",
    "    with open(env + \"config.json\", \"r\") as experiment_config_file_r:\n",
    "        experiment_config = json.load(experiment_config_file_r)\n",
    "        export_data[\"experiment_config\"] = {\n",
    "            \"random_seed\": experiment_config[\"random_seed\"],\n",
    "            \"manager_type\": experiment_config[\"manager_type\"],\n",
    "        }\n",
    "        \n",
    "    # Store export\n",
    "    with open(export_filename, \"w\") as export_file_w:\n",
    "        json.dump(export_data, export_file_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1d9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
