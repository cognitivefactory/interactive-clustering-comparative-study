{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aaf1c9",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : EFFICIENCE STUDY ====\n",
    "> ### DRAFTS : Plot some figures on computation time evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a43a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26800",
   "metadata": {},
   "source": [
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6d2a",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ca79c",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a40f5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd095b",
   "metadata": {},
   "source": [
    "## 2. Plots somes graphs and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358e732",
   "metadata": {},
   "source": [
    "***WARNING***: _Start by launching the experiment runs, evaluations and synthesis, and launching main effects analysis before this section !_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371333d",
   "metadata": {},
   "source": [
    "### 2.1. Load Python dependencies and Experiements data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cf296",
   "metadata": {},
   "source": [
    "Import Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f876a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import listing_envs\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from scipy import stats as scipystats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651dbdb6",
   "metadata": {},
   "source": [
    "Find all experiment environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_experiment_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"experiment environments in `../experiments`\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da13d0",
   "metadata": {},
   "source": [
    "### 2.2. Display iteration needed to annotated the groundtruth (_efficiency_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb95d8",
   "metadata": {},
   "source": [
    "Define the list of iteration to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare histograms.\n",
    "list_of_090vmeasure_iterations = []\n",
    "list_of_100vmeasure_iterations = []\n",
    "list_of_convergence_iterations = []\n",
    "\n",
    "# For each environment...\n",
    "for env_1 in LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "\n",
    "    # Load annotations for the current experiment.\n",
    "    with open(\n",
    "        env_1 + \"dict_of_constraints_annotations.json\", \"r\"\n",
    "    ) as annotation_file:\n",
    "        dict_of_constraints_annotations: Dict[\n",
    "            str, List[Tuple[str, str, Optional[str]]]\n",
    "        ] = json.load(annotation_file)\n",
    "            \n",
    "    # Load clustering performance for the current experiment.\n",
    "    with open(\n",
    "        env_1 + \"dict_of_clustering_performances.json\", \"r\"\n",
    "    ) as evaluations_file:\n",
    "        dict_of_clustering_evaluations: Dict[\n",
    "            str, Dict[str, float]\n",
    "        ] = json.load(evaluations_file)\n",
    "\n",
    "    # Update histogram for convergence.\n",
    "    current_max_iteration: str = max(dict_of_constraints_annotations.keys())\n",
    "    list_of_convergence_iterations.append(int(current_max_iteration))\n",
    "    \n",
    "    # Update histogram for vmeasure=100.\n",
    "    if dict_of_clustering_evaluations[current_max_iteration][\"v_measure\"] < 1.00:\n",
    "        list_of_100vmeasure_iterations.append(-1)\n",
    "    else:\n",
    "        for iteration in sorted(dict_of_clustering_evaluations.keys(), reverse=True):\n",
    "            if dict_of_clustering_evaluations[iteration][\"v_measure\"] < 1.00:\n",
    "                break\n",
    "            continue\n",
    "        list_of_100vmeasure_iterations.append(int(iteration))\n",
    "\n",
    "    # Update histogram for vmeasure=090.\n",
    "    if dict_of_clustering_evaluations[current_max_iteration][\"v_measure\"] < 0.90:\n",
    "        list_of_090vmeasure_iterations.append(-1)\n",
    "    else:\n",
    "        for iteration in sorted(dict_of_clustering_evaluations.keys(), reverse=True):\n",
    "            if dict_of_clustering_evaluations[iteration][\"v_measure\"] < 0.90:\n",
    "                break\n",
    "            continue\n",
    "        list_of_090vmeasure_iterations.append(int(iteration))\n",
    "            \n",
    "# Get maximum iteration.\n",
    "MAX_ITER: str = str(max(list_of_convergence_iterations)).zfill(4)\n",
    "# If set, force maximum iteration.\n",
    "####if forced_max_iter is not None:\n",
    "####    MAX_ITER = min(MAX_ITER, forced_max_iter)\n",
    "print(\"MAX_ITER:\", MAX_ITER)\n",
    "\n",
    "# Set list of iterations to analyze.\n",
    "LIST_OF_ITERATIONS: List[str] = [str(i).zfill(4) for i in range(int(MAX_ITER))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd531e03",
   "metadata": {},
   "source": [
    "### 2.3. Display mean of computation time per iterations (_efficiency_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec849b",
   "metadata": {},
   "source": [
    "Define main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MEAN_STDEV_SEM_of_computation_time_evolution_per_iteration(\n",
    "    local_LIST_OF_ITERATIONS,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS,\n",
    ") -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute evolution of computation time accross iterations.\n",
    "    Return Mean, Standard deviation and Standard error of the mean evolutions\n",
    "    \n",
    "    Args:\n",
    "        - local_LIST_OF_ITERATIONS (List[str]): The list of iterations to consider.\n",
    "        - local_LIST_OF_EXPERIMENT_ENVIRONMENTS (List[str]): The list of experiments to consider.\n",
    "    Returns:\n",
    "        Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]: Evolutions of Mean, Standard deviation and Standard error of the mean accross iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize storage of experiment computation time for all iterations.\n",
    "    dict_of_global_computation_time_evolution_per_iteration: Dict[str, List[float]] = {\n",
    "        iter_perf: [] for iter_perf in local_LIST_OF_ITERATIONS\n",
    "    }\n",
    "    # Initialize storage of computation time mean for all iterations.\n",
    "    dict_of_global_computation_time_evolution_per_iteration_MEAN: Dict[str, float] = {\n",
    "        iter_mean: 0 for iter_mean in local_LIST_OF_ITERATIONS\n",
    "    }\n",
    "    # Initialize storage of computation time standard deviation for all iterations.\n",
    "    dict_of_global_computation_time_evolution_per_iteration_STDEV: Dict[str, float] = {\n",
    "        iter_stdev: 0 for iter_stdev in local_LIST_OF_ITERATIONS\n",
    "    }\n",
    "    # Initialize storage of computation time standard error of the mean for all iterations.\n",
    "    dict_of_global_computation_time_evolution_per_iteration_SEM: Dict[str, float] = {\n",
    "        iter_sem: 0 for iter_sem in local_LIST_OF_ITERATIONS\n",
    "    }\n",
    "\n",
    "    # For each experiment...\n",
    "    for env_a in local_LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "\n",
    "        # Load computation time evaluations.\n",
    "        with open(env_a + \"dict_of_computation_times.json\", \"r\") as time_file:\n",
    "            dict_of_computation_times: Dict[str, Dict[str, float]] = json.load(\n",
    "                time_file\n",
    "            )\n",
    "\n",
    "        # For each requested iteration...\n",
    "        last_iter: str = \"0000\"\n",
    "        for iter_a in local_LIST_OF_ITERATIONS:\n",
    "\n",
    "            # Append the clustering computation time for the current experiment and for this iteration.\n",
    "            if iter_a in dict_of_computation_times.keys():\n",
    "                last_iter = iter_a\n",
    "                dict_of_global_computation_time_evolution_per_iteration[iter_a].append(\n",
    "                    dict_of_computation_times[iter_a][\"sampling_TOTAL_RUN\"] + dict_of_computation_times[iter_a][\"clustering_TOTAL_RUN\"]\n",
    "                )\n",
    "            # If iteration isn't reached by this experiment, duplicate the last known results.\n",
    "            else:\n",
    "                dict_of_global_computation_time_evolution_per_iteration[iter_a].append(\n",
    "                    0.0 + dict_of_computation_times[last_iter][\"clustering_TOTAL_RUN\"]\n",
    "                )\n",
    "\n",
    "    # Compute mean and sem of computation time for each iteration.\n",
    "    for iter_b in local_LIST_OF_ITERATIONS:\n",
    "\n",
    "        # Compute mean of computation time for this iteration.\n",
    "        dict_of_global_computation_time_evolution_per_iteration_MEAN[iter_b] = np.mean(dict_of_global_computation_time_evolution_per_iteration[iter_b])\n",
    "        \n",
    "        # Compute stdev of computation time for this iteration.\n",
    "        dict_of_global_computation_time_evolution_per_iteration_STDEV[iter_b] = np.std(dict_of_global_computation_time_evolution_per_iteration[iter_b])\n",
    "\n",
    "        # Compute sem of computation time for this iteration.\n",
    "        dict_of_global_computation_time_evolution_per_iteration_SEM[iter_b] = scipystats.sem(dict_of_global_computation_time_evolution_per_iteration[iter_b])\n",
    "        \n",
    "    # Return\n",
    "    return (\n",
    "        dict_of_global_computation_time_evolution_per_iteration_MEAN,\n",
    "        dict_of_global_computation_time_evolution_per_iteration_STDEV,\n",
    "        dict_of_global_computation_time_evolution_per_iteration_SEM\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plot_of_computation_time_evolution_per_iteration_to_graph(\n",
    "    axis,\n",
    "    list_of_x: List[str],\n",
    "    dict_of_y: Dict[str, float],\n",
    "    dict_of_y_err: Optional[Dict[str, float]] = None,\n",
    "    label: str = \"\",\n",
    "    marker: str = \"\",\n",
    "    markersize: int = 5,\n",
    "    color: str = \"black\",\n",
    "    linewidth: int = 2,\n",
    "    linestyle: str = \"-\",\n",
    "    alpha: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a plot to an axis of a graph.\n",
    "    \n",
    "    Args:\n",
    "        - axis (): TODO.\n",
    "        - list_of_x (List[str]): TODO.\n",
    "        - dict_of_y (Dict[str, float]): TODO.\n",
    "        - dict_of_y_err (Optional[Dict[str, float]]): TODO. Defaults to `None`.\n",
    "        - label (str): TODO. Defaults to `\"\"`.\n",
    "        - marker (str): TODO. Defaults to `\"\"`.\n",
    "        - markersize (int): TODO. Defaults to `5`.\n",
    "        - color (str): TODO. Defaults to `\"black\"`.\n",
    "        - linewidth (int): TODO. Defaults to `2`.\n",
    "        - linestyle (str): TODO. Defaults to `\"-\"`.\n",
    "        - alpha (float): TODO. Defaults to `0.2`.\n",
    "    \"\"\"\n",
    "    # Add curve.\n",
    "    axis.plot(\n",
    "        [float(x) for x in list_of_x],  # x\n",
    "        [dict_of_y[x] for x in list_of_x],  # y\n",
    "        label=label,\n",
    "        marker=marker,\n",
    "        markerfacecolor=color,\n",
    "        markersize=markersize,\n",
    "        color=color,\n",
    "        linewidth=linewidth,\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    # Add curve error bars.\n",
    "    if dict_of_y_err is not None:\n",
    "        axis.fill_between(\n",
    "            [float(x) for x in list_of_x],  # x\n",
    "            y1=[(dict_of_y[x] - dict_of_y_err[x]) for x in list_of_x],  # y1\n",
    "            y2=[(dict_of_y[x] + dict_of_y_err[x]) for x in list_of_x],  # y2\n",
    "            color=color,\n",
    "            alpha=alpha,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203fc81c",
   "metadata": {},
   "source": [
    "Evolution of computation time per iteration of MEAN + BEST SETTINGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd928b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_best_anova1: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_best_anova1 = fig_plot_best_anova1.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_best_anova1.set_xlim(xmin=0, xmax=50)\n",
    "axis_plot_best_anova1.set_ylim(ymin=0, ymax=175)\n",
    "\n",
    "# Plot average clustering computation time evolution to reach 90% of vmeasure.\n",
    "MEAN_convergence_BEST_ANOVA_90, STDEV_convergence_BEST_ANOVA_90, SEM_convergence_BEST_ANOVA_90 = get_MEAN_STDEV_SEM_of_computation_time_evolution_per_iteration(\n",
    "    local_LIST_OF_ITERATIONS=LIST_OF_ITERATIONS,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "        env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS\n",
    "        if (\"simple_prep\" in env)\n",
    "        and (\"tfidf\" in env)\n",
    "        and (\"hier_avg\" in env)\n",
    "        and (\"closest\" in env)\n",
    "    ],\n",
    ")\n",
    "add_plot_of_computation_time_evolution_per_iteration_to_graph(\n",
    "    axis=axis_plot_best_anova1,\n",
    "    list_of_x=LIST_OF_ITERATIONS,\n",
    "    dict_of_y=MEAN_convergence_BEST_ANOVA_90,\n",
    "    dict_of_y_err=SEM_convergence_BEST_ANOVA_90,\n",
    "    label=\"Moyenne des tentatives ayant le meilleur paramètrage moyen\\npour atteindre une annotation partielle (90% de v-measure).\",\n",
    "    marker=\"\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "    \n",
    "# Plot average clustering computation time evolution to reach 100% of vmeasure.\n",
    "MEAN_convergence_BEST_ANOVA_100, STDEV_convergence_BEST_ANOVA_100, SEM_convergence_BEST_ANOVA_100 = get_MEAN_STDEV_SEM_of_computation_time_evolution_per_iteration(\n",
    "    local_LIST_OF_ITERATIONS=LIST_OF_ITERATIONS,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "        env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS\n",
    "        if (\"lemma_prep\" in env)\n",
    "        and (\"tfidf\" in env)\n",
    "        and (\"kmeans_COP\" in env)\n",
    "        and (\"closest\" in env)\n",
    "    ],\n",
    ")\n",
    "add_plot_of_computation_time_evolution_per_iteration_to_graph(\n",
    "    axis=axis_plot_best_anova1,\n",
    "    list_of_x=LIST_OF_ITERATIONS,\n",
    "    dict_of_y=MEAN_convergence_BEST_ANOVA_100,\n",
    "    dict_of_y_err=SEM_convergence_BEST_ANOVA_100,\n",
    "    label=\"Moyenne des tentatives ayant le meilleur paramètrage moyen\\npour atteindre une annotation suffisante (100% de v-measure).\",\n",
    "    marker=\"\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot average clustering computation time evolution to reach constraints completude.\n",
    "MEAN_convergence_BEST_ANOVA_MAX, STDEV_convergence_BEST_ANOVA_MAX, SEM_convergence_BEST_ANOVA_MAX = get_MEAN_STDEV_SEM_of_computation_time_evolution_per_iteration(\n",
    "    local_LIST_OF_ITERATIONS=LIST_OF_ITERATIONS,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "        env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS\n",
    "        if (\"lemma_prep\" in env)\n",
    "        and (\"tfidf\" in env)\n",
    "        and (\"kmeans_COP\" in env)\n",
    "        and (\"in_same\" in env)\n",
    "    ],\n",
    ")\n",
    "add_plot_of_computation_time_evolution_per_iteration_to_graph(\n",
    "    axis=axis_plot_best_anova1,\n",
    "    list_of_x=LIST_OF_ITERATIONS,\n",
    "    dict_of_y=MEAN_convergence_BEST_ANOVA_MAX,\n",
    "    dict_of_y_err=SEM_convergence_BEST_ANOVA_MAX,\n",
    "    label=\"Moyenne des tentatives ayant le meilleur paramètrage moyen\\npour atteindre une annotation exhaustive (toutes les contraintes).\",\n",
    "    marker=\"\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot average clustering computation time evolution that have been choosen.\n",
    "MEAN_convergence_BEST_CHOOSEN, STDEV_convergence_BEST_CHOOSEN, SEM_convergence_BEST_CHOOSEN = get_MEAN_STDEV_SEM_of_computation_time_evolution_per_iteration(\n",
    "    local_LIST_OF_ITERATIONS=LIST_OF_ITERATIONS,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "        env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS\n",
    "        if (\"simple_prep\" in env)\n",
    "        and (\"tfidf\" in env)\n",
    "        and (\"kmeans_COP\" in env)\n",
    "        and (\"closest\" in env)\n",
    "    ],\n",
    ")\n",
    "add_plot_of_computation_time_evolution_per_iteration_to_graph(\n",
    "    axis=axis_plot_best_anova1,\n",
    "    list_of_x=LIST_OF_ITERATIONS,\n",
    "    dict_of_y=MEAN_convergence_BEST_CHOOSEN,\n",
    "    dict_of_y_err=SEM_convergence_BEST_CHOOSEN,\n",
    "    label=\"Moyenne des tentatives ayant le paramétrage favori\\n(atteindre 90% de v-measure avec un coût global minimal).\",\n",
    "    marker=\"\",\n",
    "    markersize=3,\n",
    "    color=\"gold\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_best_anova1.set_xlabel(\"itération [#]\", fontsize=18,)\n",
    "axis_plot_best_anova1.set_ylabel(\"temps de calcul [s]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_best_anova1.legend(fontsize=15,)\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_best_anova1.grid(True)\n",
    "\n",
    "# Store the graph.\n",
    "fig_plot_best_anova1.savefig(\n",
    "    \"../results/etude-temps-calcul-evolution-moyenne-5best-par-iteration.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b7378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
