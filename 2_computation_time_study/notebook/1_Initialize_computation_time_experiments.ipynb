{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : COMPUTATION TIME STUDY ====\n",
    "> ### Stage 1 : Initialize computation environments for experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at create environments needed to run computation time study experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[TASK]/[DATASET]/[ALGORITHM]`.\n",
    "- The path is composed of A. the task concerned (among _preprocessing_, _vectorization_, _sampling_ and _clustering_), B. the dataset used (various size), and C. the algorithm to inspect (impletation and settings of the task)\n",
    "\n",
    "At beginning of the comparative study, **run this notebook to set up experiments you want**.\n",
    "\n",
    "Then, **go to the notebook `2_Run_algorithms_and_Estimate_computation_time.ipynb` to run and evaluate each experiment you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "- 2.1. **Set up `Task` environments**:\n",
    "    - _Description_: Create a subdirectory for each task to evaluate.\n",
    "    - _Setting_: A dictionary define all possible configurations of task environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_texts.json`: texts from the dataset;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available tasks_:\n",
    "        - `preprocessing`\n",
    "        - `vectorization`\n",
    "        - `sampling`\n",
    "        - `clustering`\n",
    "        \n",
    "- 2.2. **Set up `Dataset` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters for the dataset and pre-format dataset for next computations. To get a bigger dataset, fake data can be generate (by adding some spelling errors).\n",
    "    - _Setting_: A dictionary define all possible configurations of datatset environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_texts.json`: texts from the dataset;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available datasets_:\n",
    "        - [French trainset for chatbots dealing with usual requests on bank cards v2.0.0](http://doi.org/10.5281/zenodo.4769949)\n",
    "    - _Available dataset settings_:\n",
    "         - define the dataset size;\n",
    "         - define the dataset generation random seed.\n",
    "\n",
    "- 2.3. **Set up `Algorithm` environments**:\n",
    "    - _Description_: Create a subdirectory and store type of algorithm to test (depending on the task).\n",
    "    - _Setting_: A dictionary define all possible configurations of preprocessing + vectorization + clustering environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all preprocessing parameters;\n",
    "        - `computation_time.json`: a json file with estimated computation time.\n",
    "    - _Available algorithm settings (depending on the task)_:\n",
    "        - _preprocessing_: _simple_, _lemma_, _filtered_;\n",
    "        - _vectorization_: _tfidf_, _spacy_;\n",
    "        - _sampling_: _closest, _farthest_, _2 randoms_;\n",
    "        - _clustering_: _kmeans_, _spectral_, _4 hierarchicals_;\n",
    "        - _random seed_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faker\n",
    "import listing_envs\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import pickle  # noqa: S403\n",
    "from cognitivefactory.interactive_clustering.utils.preprocessing import (\n",
    "    preprocess,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.utils.vectorization import (\n",
    "    vectorize,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.constraints.factory import (\n",
    "    managing_factory\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.sampling.factory import (\n",
    "    sampling_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. CREATE COMPUTATION ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.1. Set `Task` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different `tasks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_TASKS: Dict[str, Any] = {\n",
    "    \"preprocessing\": {\n",
    "        \"_TYPE\": \"task\",\n",
    "        \"_TASK\": \"preprocessing\",\n",
    "        \"_DESCRIPTION\": \"cognitivefactory.interactive_clustering.utils.preprocessing\",\n",
    "    },\n",
    "    \"vectorization\": {\n",
    "        \"_TYPE\": \"task\",\n",
    "        \"_TASK\": \"vectorization\",\n",
    "        \"_DESCRIPTION\": \"cognitivefactory.interactive_clustering.utils.vectorization\",\n",
    "    },\n",
    "    \"sampling\": {\n",
    "        \"_TYPE\": \"task\",\n",
    "        \"_TASK\": \"sampling\",\n",
    "        \"_DESCRIPTION\": \"cognitivefactory.interactive_clustering.sampling.factory\",\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"_TYPE\": \"task\",\n",
    "        \"_TASK\": \"clustering\",\n",
    "        \"_DESCRIPTION\": \"cognitivefactory.interactive_clustering.clustering.factory\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `task` environments using `ENVIRONMENTS_FOR_TASKS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for ENV_NAME_task, CONFIG_task in ENVIRONMENTS_FOR_TASKS.items():\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Name the configuration.\n",
    "    CONFIG_task[\"_ENV_NAME\"] = ENV_NAME_task\n",
    "    CONFIG_task[\"_ENV_PATH\"] = \"../experiments/\" + ENV_NAME_task + \"/\"\n",
    "\n",
    "    # Check if the environment already exists.\n",
    "    if os.path.exists(str(CONFIG_task[\"_ENV_PATH\"])):\n",
    "        continue\n",
    "\n",
    "    # Create directory for this environment.\n",
    "    os.mkdir(str(CONFIG_task[\"_ENV_PATH\"]))\n",
    "\n",
    "    # Store configuration file.\n",
    "    with open(str(CONFIG_task[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_t1:\n",
    "        json.dump(CONFIG_task, file_t1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Task environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.2. Set `Dataset` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `task` environments in which `create` dataset environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of tasks environments.\n",
    "LIST_OF_TASKS_ENVIRONMENTS: List[str] = listing_envs.get_list_of_tasks_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_TASKS_ENVIRONMENTS)) + \"`\",\n",
    "    \"created tasks environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_TASKS_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_DATASETS: Dict[str, Any] = {\n",
    "    # Case of bank cards management.\n",
    "    \"bank_cards_v2-size_{size_str}-rand_{rand_str}\".format(size_str=size, rand_str=rand): {\n",
    "        \"_TYPE\": \"dataset\",\n",
    "        \"_DESCRIPTION\": \"This dataset represents examples of common customer requests relating to bank cards management. It can be used as a training set for a small chatbot intended to process these usual requests.\",\n",
    "        \"file_name\": \"French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0.xlsx\",\n",
    "        \"sheet_name\": \"dataset\",\n",
    "        \"language\": \"fr\",\n",
    "        \"dataset\": \"bank_cards_v2\",\n",
    "        \"size\": size,\n",
    "        \"random_seed\": rand,\n",
    "    }\n",
    "    for size in [1000, 2000, 3000, 4000, 5000,]\n",
    "    for rand in [1, 2, 3, 4, 5,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `dataset` environments using `ENVIRONMENTS_FOR_DATASETS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_task in LIST_OF_TASKS_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_dataset,\n",
    "        CONFIG_dataset,\n",
    "    ) in ENVIRONMENTS_FOR_DATASETS.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_dataset[\"_ENV_NAME\"] = ENV_NAME_dataset\n",
    "        CONFIG_dataset[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_task + ENV_NAME_dataset + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_dataset[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_dataset[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_d1:\n",
    "            json.dump(CONFIG_dataset, file_d1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### LOAD DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load dataset.\n",
    "        df_dataset: pd.DataFrame = pd.read_excel(\n",
    "            io=\"../../datasets/\" + CONFIG_dataset[\"file_name\"],\n",
    "            sheet_name=CONFIG_dataset[\"sheet_name\"],\n",
    "            engine=\"openpyxl\",\n",
    "        )\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### LOAD DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load base for `dict_of_texts` and `dict_of_true_intents`.\n",
    "        # > Force `str` type to avoid typing errors.\n",
    "\n",
    "        base_dict_of_texts: Dict[str, str] = {\n",
    "            str(data_id): str(value[\"QUESTION\"])\n",
    "            for data_id, value in df_dataset.to_dict(\"index\").items()\n",
    "        }\n",
    "\n",
    "        base_dict_of_true_intents: Dict[str, str] = {\n",
    "            str(data_id): str(value[\"INTENT\"])\n",
    "            for data_id, value in df_dataset.to_dict(\"index\").items()\n",
    "        }\n",
    "            \n",
    "        # Fake dataset if needed (i.e. artificially add data by generating random spelling errors).\n",
    "        faker_results: Tuple[Dict[str, str], Dict[str, str]] = faker.fake_dataset(\n",
    "            dict_of_texts=base_dict_of_texts,\n",
    "            dict_of_true_intents=base_dict_of_true_intents,\n",
    "            size=CONFIG_dataset[\"size\"],\n",
    "            random_seed=CONFIG_dataset[\"random_seed\"],\n",
    "        )\n",
    "        dict_of_texts: Dict[str, str] = faker_results[0]\n",
    "        dict_of_true_intents: Dict[str, str] = faker_results[1]\n",
    "\n",
    "        # Store `dict_of_texts` and `dict_of_true_intents`.\n",
    "        with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_texts.json\", \"w\") as file_d2:\n",
    "            json.dump(dict_of_texts, file_d2)\n",
    "\n",
    "        with open(\n",
    "            str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_true_intents.json\", \"w\"\n",
    "        ) as file_d3:\n",
    "            json.dump(dict_of_true_intents, file_d3)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Dataset environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.3. Set `algorithm` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `dataset` environments in which create `algorithm` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of dataset environments.\n",
    "LIST_OF_DATASET_ENVIRONMENTS: List[str] = listing_envs.get_list_of_dataset_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_DATASET_ENVIRONMENTS)) + \"`\",\n",
    "    \"created dataset environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_DATASET_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_ALGORITHM: Dict[str, Any] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case of preprocessing.\n",
    "for rand in [1, 2, 3, 4, 5,]:\n",
    "    ENVIRONMENTS_FOR_ALGORITHM.update({\n",
    "        \"simple_prep-rand_{rand_str}\".format(rand_str=rand): {\n",
    "            \"_TYPE\": \"algorithm\",\n",
    "            \"_TASK\": \"preprocessing\",\n",
    "            \"_ALGORITHM\": \"simple_prep\",\n",
    "            \"_DESCRIPTION\": \"Simple preprocessing (lowercase, accents, punctuation, whitspace)\",\n",
    "            \"preprocessing\": {\n",
    "                \"apply_preprocessing\": True,\n",
    "                \"apply_lemmatization\": False,\n",
    "                \"apply_parsing_filter\": False,\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"random_seed\": rand,\n",
    "        },\n",
    "        \"lemma_prep-rand_{rand_str}\".format(rand_str=rand): {\n",
    "            \"_TYPE\": \"algorithm\",\n",
    "            \"_TASK\": \"preprocessing\",\n",
    "            \"_ALGORITHM\": \"lemma_prep\",\n",
    "            \"_DESCRIPTION\": \"Lemmatized preprocessing (lowercase, accents, punctuation, whitspace, lemmatization)\",\n",
    "            \"preprocessing\": {\n",
    "                \"apply_preprocessing\": True,\n",
    "                \"apply_lemmatization\": True,\n",
    "                \"apply_parsing_filter\": False,\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"random_seed\": rand,\n",
    "        },\n",
    "        \"filter_prep-rand_{rand_str}\".format(rand_str=rand): {\n",
    "            \"_TYPE\": \"algorithm\",\n",
    "            \"_TASK\": \"preprocessing\",\n",
    "            \"_ALGORITHM\": \"filter_prep\",\n",
    "            \"_DESCRIPTION\": \"Filtered preprocessing (lowercase, accents, punctuation, whitspace, dependency filter)\",\n",
    "            \"preprocessing\": {\n",
    "                \"apply_preprocessing\": True,\n",
    "                \"apply_lemmatization\": False,\n",
    "                \"apply_parsing_filter\": True,\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"random_seed\": rand,\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case of vectorization.\n",
    "for rand in [1, 2, 3, 4, 5,]:\n",
    "    ENVIRONMENTS_FOR_ALGORITHM.update({\n",
    "        \"tfidf-rand_{rand_str}\".format(\n",
    "            rand_str=rand\n",
    "        ): {\n",
    "            \"_TYPE\": \"algorithm\",\n",
    "            \"_TASK\": \"vectorization\",\n",
    "            \"_ALGORITHM\": \"tfidf\",\n",
    "            \"_DESCRIPTION\": \"TFIDF vectorization.\",\n",
    "            \"preprocessing\": {\n",
    "                \"apply_preprocessing\": True,\n",
    "                \"apply_lemmatization\": False,\n",
    "                \"apply_parsing_filter\": False,\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"vectorization\": {\n",
    "                \"vectorizer_type\": \"tfidf\",\n",
    "                \"spacy_language_model\": None,\n",
    "            },\n",
    "            \"random_seed\": rand,\n",
    "        },\n",
    "        \"spacy-rand_{rand_str}\".format(\n",
    "            rand_str=rand\n",
    "        ): {\n",
    "            \"_TYPE\": \"algorithm\",\n",
    "            \"_TASK\": \"vectorization\",\n",
    "            \"_ALGORITHM\": \"spacy\",\n",
    "            \"_DESCRIPTION\": \"Spacy vectorization.\",\n",
    "            \"preprocessing\": {\n",
    "                \"apply_preprocessing\": True,\n",
    "                \"apply_lemmatization\": False,\n",
    "                \"apply_parsing_filter\": False,\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"vectorization\": {\n",
    "                \"vectorizer_type\": \"spacy\",\n",
    "                \"spacy_language_model\": \"fr_core_news_md\",\n",
    "            },\n",
    "            \"random_seed\": rand,\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case of sampling.\n",
    "for nb_to_select in range(50, 251, 50):\n",
    "    for previous_constraints in range(0, 5001, 500):\n",
    "        for previous_clustering in range(10, 51, 10):\n",
    "            for rand in [1, 2, 3, 4, 5,]:\n",
    "                ENVIRONMENTS_FOR_ALGORITHM.update({\n",
    "                    \"random-select_{select_str}-rand_{rand_str}-prev_const{const_str}_clu{clu_str}\".format(\n",
    "                        select_str=nb_to_select,\n",
    "                        rand_str=rand,\n",
    "                        const_str=previous_constraints,\n",
    "                        clu_str=previous_clustering,\n",
    "                    ): {\n",
    "                        \"_TYPE\": \"algorithm\",\n",
    "                        \"_TASK\": \"sampling\",\n",
    "                        \"_ALGORITHM\": \"random\",\n",
    "                        \"_DESCRIPTION\": \"Random sampling, {select_str} combinations to select, {const_str} previous constraints, {clu_str} clusters.\".format(\n",
    "                            select_str=nb_to_select,\n",
    "                            const_str=previous_constraints,\n",
    "                            clu_str=previous_clustering,\n",
    "                        ),\n",
    "                        \"preprocessing\": {\n",
    "                            \"apply_preprocessing\": True,\n",
    "                            \"apply_lemmatization\": False,\n",
    "                            \"apply_parsing_filter\": False,\n",
    "                            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                        },\n",
    "                        \"vectorization\": {\n",
    "                            \"vectorizer_type\": \"tfidf\",\n",
    "                            \"spacy_language_model\": None,\n",
    "                        },\n",
    "                        \"sampling\": {\n",
    "                            \"algorithm\": \"random\",\n",
    "                            \"nb_to_select\": nb_to_select,\n",
    "                        },\n",
    "                        \"previous\": {\n",
    "                            \"clustering\": previous_clustering, \n",
    "                            \"constraints\": previous_constraints,\n",
    "                        },\n",
    "                        \"random_seed\": rand,\n",
    "                    },\n",
    "                    \"in_same-select_{select_str}-rand_{rand_str}-prev_const{const_str}_clu{clu_str}\".format(\n",
    "                        select_str=nb_to_select,\n",
    "                        rand_str=rand,\n",
    "                        const_str=previous_constraints,\n",
    "                        clu_str=previous_clustering,\n",
    "                    ): {\n",
    "                        \"_TYPE\": \"algorithm\",\n",
    "                        \"_TASK\": \"sampling\",\n",
    "                        \"_ALGORITHM\": \"in_same\",\n",
    "                        \"_DESCRIPTION\": \"Random in same cluster sampling, {select_str} combinations to select, {const_str} previous constraints, {clu_str} clusters.\".format(\n",
    "                            select_str=nb_to_select,\n",
    "                            const_str=previous_constraints,\n",
    "                            clu_str=previous_clustering,\n",
    "                        ),\n",
    "                        \"preprocessing\": {\n",
    "                            \"apply_preprocessing\": True,\n",
    "                            \"apply_lemmatization\": False,\n",
    "                            \"apply_parsing_filter\": False,\n",
    "                            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                        },\n",
    "                        \"vectorization\": {\n",
    "                            \"vectorizer_type\": \"tfidf\",\n",
    "                            \"spacy_language_model\": None,\n",
    "                        },\n",
    "                        \"sampling\": {\n",
    "                            \"algorithm\": \"random_in_same_cluster\",\n",
    "                            \"nb_to_select\": nb_to_select,\n",
    "                        },\n",
    "                        \"previous\": {\n",
    "                            \"clustering\": previous_clustering, \n",
    "                            \"constraints\": previous_constraints,\n",
    "                        },\n",
    "                        \"random_seed\": rand,\n",
    "                    },\n",
    "                    \"closest-select_{select_str}-rand_{rand_str}-prev_const{const_str}_clu{clu_str}\".format(\n",
    "                        select_str=nb_to_select,\n",
    "                        rand_str=rand,\n",
    "                        const_str=previous_constraints,\n",
    "                        clu_str=previous_clustering,\n",
    "                    ): {\n",
    "                        \"_TYPE\": \"algorithm\",\n",
    "                        \"_TASK\": \"sampling\",\n",
    "                        \"_ALGORITHM\": \"closest\",\n",
    "                        \"_DESCRIPTION\": \"Closest in different clusters sampling, {select_str} combinations to select, {const_str} previous constraints, {clu_str} clusters.\".format(\n",
    "                            select_str=nb_to_select,\n",
    "                            const_str=previous_constraints,\n",
    "                            clu_str=previous_clustering,\n",
    "                        ),\n",
    "                        \"preprocessing\": {\n",
    "                            \"apply_preprocessing\": True,\n",
    "                            \"apply_lemmatization\": False,\n",
    "                            \"apply_parsing_filter\": False,\n",
    "                            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                        },\n",
    "                        \"vectorization\": {\n",
    "                            \"vectorizer_type\": \"tfidf\",\n",
    "                            \"spacy_language_model\": None,\n",
    "                        },\n",
    "                        \"sampling\": {\n",
    "                            \"algorithm\": \"closest_in_different_clusters\",\n",
    "                            \"nb_to_select\": nb_to_select,\n",
    "                        },\n",
    "                        \"previous\": {\n",
    "                            \"clustering\": previous_clustering, \n",
    "                            \"constraints\": previous_constraints,\n",
    "                        },\n",
    "                        \"random_seed\": rand,\n",
    "                    },\n",
    "                    \"farthest-select_{select_str}-rand_{rand_str}-prev_const{const_str}_clu{clu_str}\".format(\n",
    "                        select_str=nb_to_select,\n",
    "                        rand_str=rand,\n",
    "                        const_str=previous_constraints,\n",
    "                        clu_str=previous_clustering,\n",
    "                    ): {\n",
    "                        \"_TYPE\": \"algorithm\",\n",
    "                        \"_TASK\": \"sampling\",\n",
    "                        \"_ALGORITHM\": \"farthest\",\n",
    "                        \"_DESCRIPTION\": \"Farthest in same cluster sampling, {select_str} combinations to select, {const_str} previous constraints, {clu_str} clusters.\".format(\n",
    "                            select_str=nb_to_select,\n",
    "                            const_str=previous_constraints,\n",
    "                            clu_str=previous_clustering,\n",
    "                        ),\n",
    "                        \"preprocessing\": {\n",
    "                            \"apply_preprocessing\": True,\n",
    "                            \"apply_lemmatization\": False,\n",
    "                            \"apply_parsing_filter\": False,\n",
    "                            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                        },\n",
    "                        \"vectorization\": {\n",
    "                            \"vectorizer_type\": \"tfidf\",\n",
    "                            \"spacy_language_model\": None,\n",
    "                        },\n",
    "                        \"sampling\": {\n",
    "                            \"algorithm\": \"farthest_in_same_cluster\",\n",
    "                            \"nb_to_select\": nb_to_select,\n",
    "                        },\n",
    "                        \"previous\": {\n",
    "                            \"clustering\": previous_clustering, \n",
    "                            \"constraints\": previous_constraints,\n",
    "                        },\n",
    "                        \"random_seed\": rand,\n",
    "                    },\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case of clustering.\n",
    "for nb_clusters in range(5, 51, 5):\n",
    "    for previous_constraints in range(0, 5001, 500):\n",
    "        for rand in [1, 2, 3, 4, 5,]:\n",
    "            ENVIRONMENTS_FOR_ALGORITHM.update({\n",
    "                \"kmeans_COP-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"kmeans_COP\",\n",
    "                    \"_DESCRIPTION\": \"KMeans (COP) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"kmeans\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"model\": \"COP\",\n",
    "                            \"max_iteration\": 150,\n",
    "                            \"tolerance\": 1e-4,\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "                \"hier_ward-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"hier_ward\",\n",
    "                    \"_DESCRIPTION\": \"Hierarchical (WARD) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"hierarchical\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"linkage\": \"ward\",\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "                \"hier_average-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"hier_average\",\n",
    "                    \"_DESCRIPTION\": \"Hierarchical (AVERAGE) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"hierarchical\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"linkage\": \"average\",\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "                \"hier_complete-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"hier_complete\",\n",
    "                    \"_DESCRIPTION\": \"Hierarchical (COMPLETE) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"hierarchical\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"linkage\": \"complete\",\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "                \"hier_single-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"hier_single\",\n",
    "                    \"_DESCRIPTION\": \"Hierarchical (SINGLE) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"hierarchical\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"linkage\": \"single\",\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "                \"spectral_SPEC-clusters_{nb_clusters_str}-rand_{rand_str}-prev_const{const_str}\".format(\n",
    "                    nb_clusters_str=nb_clusters,\n",
    "                    rand_str=rand,\n",
    "                    const_str=previous_constraints,\n",
    "                ): {\n",
    "                    \"_TYPE\": \"algorithm\",\n",
    "                    \"_TASK\": \"clustering\",\n",
    "                    \"_ALGORITHM\": \"spectral_SPEC\",\n",
    "                    \"_DESCRIPTION\": \"Spectral (SPEC) clustering, {nb_clusters_str} clusters, {const_str} previous constraints.\".format(\n",
    "                        nb_clusters_str=nb_clusters,\n",
    "                        const_str=previous_constraints,\n",
    "                    ),\n",
    "                    \"preprocessing\": {\n",
    "                        \"apply_preprocessing\": True,\n",
    "                        \"apply_lemmatization\": False,\n",
    "                        \"apply_parsing_filter\": False,\n",
    "                        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "                    },\n",
    "                    \"vectorization\": {\n",
    "                        \"vectorizer_type\": \"tfidf\",\n",
    "                        \"spacy_language_model\": None,\n",
    "                    },\n",
    "                    \"clustering\": {\n",
    "                        \"algorithm\": \"spectral\",\n",
    "                        \"init**kargs\": {\n",
    "                            \"model\": \"SPEC\",\n",
    "                        },\n",
    "                        \"nb_clusters\": nb_clusters,\n",
    "                    },\n",
    "                    \"previous\": {\n",
    "                        \"constraints\": previous_constraints,\n",
    "                    },\n",
    "                    \"random_seed\": rand,\n",
    "                },\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `algorithm` environments using `ENVIRONMENTS_FOR_ALGORITHM` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_dataset in LIST_OF_DATASET_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_algorithm,\n",
    "        CONFIG_algorithm,\n",
    "    ) in ENVIRONMENTS_FOR_ALGORITHM.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        # Create environments only if its the good task !\n",
    "        if CONFIG_algorithm[\"_TASK\"] != PARENT_ENV_PATH_dataset.split(\"/\")[2]:\n",
    "            continue\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_algorithm[\"_ENV_NAME\"] = ENV_NAME_algorithm\n",
    "        CONFIG_algorithm[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_dataset + ENV_NAME_algorithm + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_algorithm[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_algorithm[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_a1:\n",
    "            json.dump(CONFIG_algorithm, file_a1)\n",
    "\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Algorithm environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 3. Get all created environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_algorithm_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "#LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
