{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d5b24f",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : COMPUTATION TIME STUDY ====\n",
    "> ### Stage 3 : Modelize computation time of Interactive Clustering tasks and Plot some figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a43a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26800",
   "metadata": {},
   "source": [
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6d2a",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at modelize interactive clustering computation time over experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[TASK]/[DATASET]/[ALGORITHM]/`.\n",
    "- Experiments have to be run and evaluated in order to analyze convergency speed.\n",
    "\n",
    "Before running, **run the notebook `2_Estimate_computation_time.ipynb` to run each algorithm you have set**.\n",
    "\n",
    "Then, **go to the notebook `4_Plot_some_figures.ipynb` to create figures on interactive clustering computation time**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb0f5d",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, **load experiment synthesis CSV file** that have been computed with the last notebook.\n",
    "- It contains parameters used for each experiment and convergency metric to compare.\n",
    "- Several parameters are studied depending on the task:\n",
    "    - _preprocessing_: `dataset_size`, `algorithm_name`;\n",
    "    - _vectorization_: `dataset_size`, `algorithm_name`;\n",
    "    - _sampling_: `dataset_size`, `algorithm_name`, `previous_nb_constraints`, `previous_nb_clusters`, `algorithm_nb_to_select`;\n",
    "    - _clustering_: `dataset_size`, `algorithm_name`, `previous_nb_constraints`, `previous_nb_clusters`.\n",
    "- Two random effects are used : `dataset_random_seed`, `algorithm_random_seed`.\n",
    "- One values is modelized with these factors : `time_total`.\n",
    "\n",
    "Then, for each task :\n",
    "1. Compute interactions of factors (`1`, `X1`, `X1Â²`, `X1*X2`, ...)\n",
    "2. Sort interactions by correlation in order to choose an efficient modelization of computation time\n",
    "3. Compute GLM to get the modelization parameters\n",
    "4. Plot modelized computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd89d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07718de9",
   "metadata": {},
   "source": [
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f876a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from itertools import combinations_with_replacement, permutations\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy import stats as scipystats\n",
    "import statistics\n",
    "import statsmodels\n",
    "import statsmodels.api\n",
    "import statsmodels.formula.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels.genmod.generalized_linear_model.SET_USE_BIC_LLF(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combinations_of_interactions_of_factors(\n",
    "    df: pd.DataFrame,\n",
    "    factors: List[str],\n",
    "    range_of_powers: List[int] = [0, 1, 2, 3],\n",
    "    max_power: int = 3,\n",
    ") -> Tuple[List[Dict[str, str]], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "        Compute combinations of interactions of factors in dataframe.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe of results.\n",
    "            factors (List[str]): Column of dataframe considered as factors.\n",
    "            range_of_powers (List[int]): Range of power to compute. Defaults to `[0, 1, 2, 3]`.\n",
    "            max_power (int): Maximum interaction levl, i.e. maximum sum of powers. Defaults to `3`.\n",
    "        \n",
    "        Return:\n",
    "            Tuple[Dict[str, str], pd.DataFrame]: The factors interactions (dictionary) and the updated dataframe of results.\n",
    "    \"\"\"\n",
    "\n",
    "    # The columns computed.\n",
    "    factors_interactions: Dict[str, str] = {}\n",
    "    \n",
    "    # Define combinations of powers.\n",
    "    for power_combination in combinations_with_replacement(range_of_powers, r=len(factors)):\n",
    "        \n",
    "        # Define max combination of powers.\n",
    "        if sum(power_combination)<=0 or max_power<sum(power_combination):\n",
    "            continue\n",
    "\n",
    "        # Define permutations of combinations of powers.\n",
    "        for powers in sorted(set(permutations(power_combination)), reverse=True):\n",
    "            \n",
    "            # Compute column and value of the combination of factors.\n",
    "            list_of_Xi: List[Dict] = []\n",
    "            for i, factor in enumerate(factors):\n",
    "                if powers[i] != 0:\n",
    "                    list_of_Xi.append({\n",
    "                        \"key_name\": \"X{0}POW{1}\".format(i+1, powers[i]).replace(\".\", \"_\"),\n",
    "                        \"full_name\": (\n",
    "                            \"{0}\".format(factor)\n",
    "                            if powers[i] == 1\n",
    "                            else \"{0}**{1}\".format(factor, powers[i])\n",
    "                        ),\n",
    "                        \"factor\": factor,\n",
    "                        \"power\": powers[i],\n",
    "                    })\n",
    "            \n",
    "            # Define names (key name and full name) of factors interactions.\n",
    "            column_key_name: str = \"_\".join([x[\"key_name\"] for x in list_of_Xi])\n",
    "            column_full_name: str = (\n",
    "                list_of_Xi[0][\"full_name\"]\n",
    "                if len(list_of_Xi) == 1\n",
    "                else \" * \".join([\"({0})\".format(x[\"full_name\"]) for x in list_of_Xi])\n",
    "            )\n",
    "            factors_interactions[column_key_name] = column_full_name\n",
    "            \n",
    "            # Update dataframe of results by store the factors interaction in a column.\n",
    "            df[column_key_name] = df.apply(\n",
    "                lambda row: np.prod([\n",
    "                    np.power(row[x[\"factor\"]], x[\"power\"])\n",
    "                    for x in list_of_Xi\n",
    "                ]),\n",
    "                axis=1,\n",
    "            )\n",
    "    return factors_interactions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_of_factors(\n",
    "    df: pd.DataFrame,\n",
    "    factors_interactions: Dict[str, str],\n",
    "    algorithm_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Compute Pearson (standard) correlation coefficient between factors and computed time.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe of results.\n",
    "            factors_interactions (Dict[str, str]): Dictionary of columns in dataframe that correspond to computed interaction between factors.\n",
    "            algorithm_name (str): The algithm name to filter dataframe.\n",
    "        \n",
    "        Return:\n",
    "            pd.DataFrame: The correlation of combination of factors to algorithm computation time.\n",
    "    \"\"\"\n",
    "    # Get columns of factors interactions\n",
    "    columns_of_factors_interactions: List[str] = list(factors_interactions.keys())\n",
    "    \n",
    "    # Filter data on `algorithm_name`.\n",
    "    df_subset = (\n",
    "        df\n",
    "        if algorithm_name is None\n",
    "        else df[df[\"algorithm_name\"]==algorithm_name]\n",
    "    )\n",
    "    \n",
    "    # Compute correlation.\n",
    "    df_correlation = df_subset.loc[\n",
    "        # Exclude columns that are not factors.\n",
    "        :, df_subset.columns.isin([\"time_total\"] + columns_of_factors_interactions)\n",
    "    ].corr(\n",
    "        # Get correlation of factors.\n",
    "    )[\"time_total\"].apply(\n",
    "        # Get the absolute value of correlation.\n",
    "        lambda x: abs(x)\n",
    "    ).sort_values(\n",
    "        # Sort by correlation.\n",
    "        ascending=False\n",
    "    ).to_frame(\n",
    "        # Format in pd.Series\n",
    "        name=\"r\"\n",
    "    ).drop(\n",
    "        # Drop time column.\n",
    "        [\"time_total\"]\n",
    "    )\n",
    "    \n",
    "    # Compute R^2.\n",
    "    df_correlation[\"r^2\"] = np.power(df_correlation[\"r\"], 2)\n",
    "    \n",
    "    # Add factors interactions full name in columns\n",
    "    df_correlation[\"full_name\"] = df_correlation.apply(\n",
    "        lambda row: factors_interactions[row.name],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Show.\n",
    "    return df_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_score_evolution(\n",
    "    df: pd.DataFrame,\n",
    "    df_correlation: pd.DataFrame,\n",
    "    factors_interactions: Dict[str, str],\n",
    "    algorithm_name: str,\n",
    "    with_intercept: bool = True,\n",
    "    graph_filepath: Optional[str] = None,\n",
    "    graph_plot_description: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Compute OLS model based on top correlation factors.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe of results.\n",
    "            df_correlation (pd.DataFrame): The correlation of combination of factors to algorithm computation time.\n",
    "            factors_interactions (Dict[str, str]): Dictionary of columns in dataframe that correspond to computed interaction between factors.\n",
    "            algorithm_name (str): The algorithm name to filter dataframe.\n",
    "            with_intercept (bool): The option to add an intercept in the modelization. Defaults to `True`.\n",
    "            graph_filepath (Optional[str]): The path where to store the information score evolution plot. Defaults to `None`.\n",
    "            graph_plot_description (Optional[str]): The description of the information score evolution plot. Defaults to `None`.\n",
    "        \n",
    "        Return:\n",
    "            Dict: Evolution of information score of OLS model based on top correlation factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    information_score_evolution = []\n",
    "    factors_key_names = []\n",
    "    factors_full_names = []\n",
    "       \n",
    "    # For constant (1) + each interaction factor (df_correlation.index)...\n",
    "    for i, factor in enumerate(\n",
    "        ([\"1\"] + list(df_correlation.index))\n",
    "        if with_intercept\n",
    "        else list(df_correlation.index)\n",
    "    ):\n",
    "        \n",
    "        # Compute modelization with previous factors and this new one.\n",
    "        factors_key_names.append(factor)\n",
    "        factors_full_names.append(\n",
    "            factors_interactions[factor]\n",
    "            if factor in factors_interactions.keys()\n",
    "            else \"intercept\"\n",
    "        )\n",
    "        model = statsmodels.formula.api.ols(\n",
    "            formula=\"time_total ~ 0 + \" + \" + \".join(factors_key_names),\n",
    "            data=(\n",
    "                df\n",
    "                if algorithm_name is None\n",
    "                else df[df[\"algorithm_name\"]==algorithm_name]\n",
    "            ),\n",
    "        )\n",
    "        result = model.fit()\n",
    "        \n",
    "        # Store results.\n",
    "        information_score_evolution.append({\n",
    "            \"factors\": factors_key_names.copy(),\n",
    "            \"scores\": {\"aic\": result.aic, \"bic\": result.bic, \"rsquared\": result.rsquared, \"llf\": result.llf}\n",
    "        })\n",
    "        \n",
    "    # Create a new figure.\n",
    "    fig_plot: Figure = plt.figure(figsize=(15, 7.5), dpi=50)\n",
    "    axis_plot = fig_plot.gca()\n",
    "\n",
    "    # Set range of axis.\n",
    "    axis_plot.set_ylim(ymin=0, ymax=1)\n",
    "\n",
    "    # Plot information criteria.\n",
    "    axis_plot.plot(\n",
    "        [str(i+1) for i in range(len(information_score_evolution))],  # x\n",
    "        [result[\"scores\"][\"rsquared\"] for result in information_score_evolution],  # y\n",
    "        label=(\n",
    "            \"RÂ²\"\n",
    "            if graph_plot_description is None\n",
    "            else graph_plot_description\n",
    "        ),\n",
    "        marker=\"x\",\n",
    "        markerfacecolor=\"red\",\n",
    "        markersize=3,\n",
    "        color=\"red\",\n",
    "        linewidth=1,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "    \n",
    "    # Plot x label names.\n",
    "    axis_plot.set_xticks(\n",
    "        [str(i+1) for i in range(len(information_score_evolution))],\n",
    "        factors_full_names,\n",
    "        rotation=30,\n",
    "        ha='right'\n",
    "    )\n",
    "\n",
    "    # Set axis name.\n",
    "    axis_plot.set_xlabel(\"complexitÃ© de la modÃ©lisation [# de facteurs d'analyse]\", fontsize=18,)\n",
    "    axis_plot.set_ylabel(\"score du critÃ¨re d'informations [%]\", fontsize=18,)\n",
    "    \n",
    "    # Set range of axis.\n",
    "    axis_plot.set_ylim(ymin=0, ymax=1)  # 0.33 -> 1\n",
    "\n",
    "    # Plot the legend.\n",
    "    axis_plot.legend(\n",
    "        loc=\"lower right\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "    # Plot the grid.\n",
    "    axis_plot.grid(True)\n",
    "    \n",
    "    # Store the graph.\n",
    "    if graph_filepath is not None:\n",
    "        fig_plot.savefig(\n",
    "            graph_filepath,\n",
    "            dpi=300,\n",
    "            transparent=True,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    \n",
    "    return information_score_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e40848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best GLM model:\n",
    "def compare_glm_models(\n",
    "    df: pd.DataFrame,\n",
    "    algorithm_name: str,\n",
    "    formulas: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Compare GLM models.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe of results.\n",
    "            algorithm_name (str): The algithm name to filter dataframe.\n",
    "            formulas (List[str]): The list of formulas used to train GLM models.\n",
    "        \n",
    "        Return:\n",
    "            pd.DataFrame: Results of GLM models (pseudo RÂ², log-likelihood)\n",
    "    \"\"\"\n",
    "    df_scores = pd.DataFrame(columns=[\"formula\", \"degree of freedom\", \"pseudo RÂ² (Cox-Snell)\", \"log-likelihood\"])\n",
    "    for formula in formulas:\n",
    "        # Fit the model to the data.\n",
    "        model = statsmodels.formula.api.glm(\n",
    "            formula=formula,\n",
    "            data=(\n",
    "                df\n",
    "                if algorithm_name is None\n",
    "                else df[df[\"algorithm_name\"]==algorithm_name]\n",
    "            ),\n",
    "        )\n",
    "        # Store results.\n",
    "        results = model.fit()\n",
    "        df_scores = pd.concat(\n",
    "            [\n",
    "                df_scores,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"formula\": formula,\n",
    "                        \"degree of freedom\": results.df_model,\n",
    "                        \"pseudo RÂ² (Cox-Snell)\": results.pseudo_rsquared(),\n",
    "                        \"log-likelihood\": results.llf,\n",
    "                    },\n",
    "                    index=[0]\n",
    "                )\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    # Return results\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afbf86",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568aa26",
   "metadata": {},
   "source": [
    "## 2.1. ANALYSIS FOR PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c48b0e",
   "metadata": {},
   "source": [
    "> - algorithms: `simple_prep`, `lemma_prep`, `filter_prep`\n",
    "> - factors: `dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f6499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preprocessing = pd.read_csv(\"../results/experiments_synthesis_for_preprocessing.csv\", sep=\";\", index_col=0)\n",
    "df_preprocessing[\"time_total\"] = df_preprocessing[\"time_total\"].str.replace(\",\", \".\").astype(float)\n",
    "df_preprocessing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b560f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction of factors for preprocessing.\n",
    "factors_interactions_for_preprocessing, df_preprocessing = compute_combinations_of_interactions_of_factors(\n",
    "    df=df_preprocessing,\n",
    "    factors=[\"dataset_size\"],\n",
    "    range_of_powers=[0, 1, 2],\n",
    "    max_power=3,\n",
    ")\n",
    "factors_interactions_for_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68183ab",
   "metadata": {},
   "source": [
    "### 2.1.0. Dertermine if preprocessing computation time is `algorithm_name` dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_prep_ALGONAME = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + C(algorithm_name)\",\n",
    "    data=df_preprocessing,\n",
    ")\n",
    "results_prep_ALGONAME = model_prep_ALGONAME.fit()\n",
    "print(results_prep_ALGONAME.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f2c59",
   "metadata": {},
   "source": [
    "### 2.1.1. Modelize all preprocessing `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72182ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for preprocessing.\n",
    "df_correlation_preprocessing: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_preprocessing,\n",
    "    factors_interactions=factors_interactions_for_preprocessing,\n",
    "    algorithm_name=None,\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_preprocessing = compute_information_score_evolution(\n",
    "    df=df_preprocessing,\n",
    "    df_correlation=df_correlation_preprocessing,\n",
    "    factors_interactions=factors_interactions_for_preprocessing,\n",
    "    algorithm_name=None,\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-1prep.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser les prÃ©traitements 'prep.simple', 'prep.lemma', 'prep.filter'\",\n",
    ")\n",
    "df_correlation_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05561413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_prep = compare_glm_models(\n",
    "    df=df_preprocessing,\n",
    "    algorithm_name=None,\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 0 + X1POW1\",\n",
    "        \"time_total ~ 1 + X1POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475b734",
   "metadata": {},
   "source": [
    "> `preprocessing ~ 1 + dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d333bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_prep = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW1\",  # X1POW1=dataset_size\n",
    "    data=df_preprocessing,\n",
    ")\n",
    "best_results_prep = best_model_prep.fit()\n",
    "print(\"==============================================================================\")\n",
    "print(\">>> formula:\", best_model_prep.formula)\n",
    "print(\"==============================================================================\")\n",
    "print(\"   \", best_results_prep.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"preprocessing ~\",\n",
    "    \"{0:.2E}\".format(best_results_prep.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_prep.params[\"X1POW1\"], factors_interactions_for_preprocessing[\"X1POW1\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_prep(dataset_size) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_prep.params[\"Intercept\"] - best_results_prep.bse[\"Intercept\"])\n",
    "    res += best_results_prep.params[\"Intercept\"]\n",
    "    res_high += (best_results_prep.params[\"Intercept\"] + best_results_prep.bse[\"Intercept\"])\n",
    "    # dataset_size.\n",
    "    res_low += (best_results_prep.params[\"X1POW1\"] - best_results_prep.bse[\"X1POW1\"]) * dataset_size\n",
    "    res += best_results_prep.params[\"X1POW1\"] * dataset_size\n",
    "    res_high += (best_results_prep.params[\"X1POW1\"] + best_results_prep.bse[\"X1POW1\"]) * dataset_size\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ca135",
   "metadata": {},
   "source": [
    "### 3.1.2. Print all preprocessing computation time model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6200874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_preprocessing: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_preprocessing = fig_plot_preprocessing.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_preprocessing.set_xlim(xmin=0, xmax=5500)\n",
    "axis_plot_preprocessing.set_ylim(ymin=0, ymax=35)\n",
    "\n",
    "# Plot computation time for preprocessing.\n",
    "axis_plot_preprocessing.plot(\n",
    "    df_preprocessing[\"dataset_size\"],  # x\n",
    "    df_preprocessing[\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© des prÃ©traitements 'prep.simple', 'prep.lemma', 'prep.filter'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_preprocessing.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        interpolation_prep(x)[1]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y\n",
    "    label=\"Temps de calcul modÃ©lisÃ© des prÃ©traitements 'prep.simple', 'prep.lemma', 'prep.filter'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_preprocessing.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        interpolation_prep(x)[0]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        interpolation_prep(x)[2]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_preprocessing.set_xlabel(\"nombre de donnÃ©es [#]\", fontsize=18,)\n",
    "axis_plot_preprocessing.set_ylabel(\"temps de calcul [secondes]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_preprocessing.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_preprocessing.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_preprocessing.savefig(\n",
    "    \"../results/etude-temps-calcul-modelisation-1prep.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd71c22",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4bc85",
   "metadata": {},
   "source": [
    "## 2.2. ANALYSIS FOR VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7983e",
   "metadata": {},
   "source": [
    "> - algorithms: `tfidf`, `spacy`\n",
    "> - factors: `dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorization = pd.read_csv(\"../results/experiments_synthesis_for_vectorization.csv\", sep=\";\", index_col=0)\n",
    "df_vectorization[\"time_total\"] = df_vectorization[\"time_total\"].str.replace(\",\", \".\").astype(float)\n",
    "df_vectorization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction of factors for preprocessing.\n",
    "factors_interactions_for_vectorization, df_vectorization = compute_combinations_of_interactions_of_factors(\n",
    "    df=df_vectorization,\n",
    "    factors=[\"dataset_size\"],\n",
    "    range_of_powers=[0, 1, 2],\n",
    "    max_power=3\n",
    ")\n",
    "factors_interactions_for_vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e31b04",
   "metadata": {},
   "source": [
    "### 2.2.0. Dertermine if vectorization computation time is `algorithm_name` dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_vect_ALGONAME = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ C(algorithm_name)\",\n",
    "    data=df_vectorization,\n",
    ")\n",
    "results_vect_ALGONAME = model_vect_ALGONAME.fit()\n",
    "print(results_vect_ALGONAME.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dbfd6",
   "metadata": {},
   "source": [
    "### 2.2.1 Modelize `tfidf` vectorization `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4287a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for vectorization.\n",
    "df_correlation_vectorization_tfidf: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_vectorization,\n",
    "    factors_interactions=factors_interactions_for_vectorization,\n",
    "    algorithm_name=\"tfidf\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_vectorization_tfidf = compute_information_score_evolution(\n",
    "    df=df_vectorization,\n",
    "    df_correlation=df_correlation_vectorization_tfidf,\n",
    "    factors_interactions=factors_interactions_for_vectorization,\n",
    "    algorithm_name=\"tfidf\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-2vect-tfidf.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser la vectorisation 'vect.tfidf'\",\n",
    ")\n",
    "df_correlation_vectorization_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_vectorization_tfidf = compare_glm_models(\n",
    "    df=df_vectorization,\n",
    "    algorithm_name=\"tfidf\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 0 + X1POW1\",\n",
    "        \"time_total ~ 1 + X1POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_vectorization_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451f6c6",
   "metadata": {},
   "source": [
    "> `vectorization.tfidf ~ 1 + dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_vect_tfidf = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW1\",  # X1POW1 = dataset_size\n",
    "    data=df_vectorization[df_vectorization[\"algorithm_name\"]==\"tfidf\"],\n",
    ")\n",
    "best_results_vect_tfidf = best_model_vect_tfidf.fit()\n",
    "print(\"==============================================================================\")\n",
    "print(\">>> formula:\", best_model_vect_tfidf.formula)\n",
    "print(\"==============================================================================\")\n",
    "print(best_results_vect_tfidf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"vectorisation.tfidf ~\",\n",
    "    \"{0:.2E}\".format(best_results_vect_tfidf.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_vect_tfidf.params[\"X1POW1\"], factors_interactions_for_vectorization[\"X1POW1\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_vect_tfidf(dataset_size) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_vect_tfidf.params[\"Intercept\"] - best_results_vect_tfidf.bse[\"Intercept\"])\n",
    "    res += best_results_vect_tfidf.params[\"Intercept\"]\n",
    "    res_high += (best_results_vect_tfidf.params[\"Intercept\"] + best_results_vect_tfidf.bse[\"Intercept\"])\n",
    "    # dataset_size.\n",
    "    res_low += (best_results_vect_tfidf.params[\"X1POW1\"] - best_results_vect_tfidf.bse[\"X1POW1\"]) * dataset_size\n",
    "    res += best_results_vect_tfidf.params[\"X1POW1\"] * dataset_size\n",
    "    res_high += (best_results_vect_tfidf.params[\"X1POW1\"] + best_results_vect_tfidf.bse[\"X1POW1\"]) * dataset_size\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea23ef",
   "metadata": {},
   "source": [
    "### 2.2.2. Modelize `frcorenewsmd` vectorization `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for vectorization.\n",
    "df_correlation_vectorization_frcorenewsmd: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_vectorization,\n",
    "    factors_interactions=factors_interactions_for_vectorization,\n",
    "    algorithm_name=\"spacy\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_vectorization_frcorenewsmd = compute_information_score_evolution(\n",
    "    df=df_vectorization,\n",
    "    df_correlation=df_correlation_vectorization_frcorenewsmd,\n",
    "    factors_interactions=factors_interactions_for_vectorization,\n",
    "    algorithm_name=\"spacy\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-2vect-frcorenewsmd.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser la vectorisation 'vect.frcorenewsmd'\",\n",
    ")\n",
    "df_correlation_vectorization_frcorenewsmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_vectorization_frcorenewsmd = compare_glm_models(\n",
    "    df=df_vectorization,\n",
    "    algorithm_name=\"spacy\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 0 + X1POW1\",\n",
    "        \"time_total ~ 1 + X1POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_vectorization_frcorenewsmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbeccb",
   "metadata": {},
   "source": [
    "> `vectorization.spacy ~ 1 + dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2099996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_vect_frcorenewsmd = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW1\",  # X1POW1 = dataset_size\n",
    "    data=df_vectorization[df_vectorization[\"algorithm_name\"]==\"spacy\"],\n",
    ")\n",
    "best_results_vect_frcorenewsmd = best_model_vect_frcorenewsmd.fit()\n",
    "print(\"==============================================================================\")\n",
    "print(\">>> formula:\", best_model_vect_frcorenewsmd.formula)\n",
    "print(\"==============================================================================\")\n",
    "print(best_results_vect_frcorenewsmd.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"vectorization.frcorenewsmd ~\",\n",
    "    \"{0:.2E}\".format(best_results_vect_frcorenewsmd.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_vect_frcorenewsmd.params[\"X1POW1\"], factors_interactions_for_vectorization[\"X1POW1\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cfa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_vect_frcorenewsmd(dataset_size) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_vect_frcorenewsmd.params[\"Intercept\"] - best_results_vect_frcorenewsmd.bse[\"Intercept\"])\n",
    "    res += best_results_vect_frcorenewsmd.params[\"Intercept\"]\n",
    "    res_high += (best_results_vect_frcorenewsmd.params[\"Intercept\"] + best_results_vect_frcorenewsmd.bse[\"Intercept\"])\n",
    "    # dataset_size.\n",
    "    res_low += (best_results_vect_frcorenewsmd.params[\"X1POW1\"] - best_results_vect_frcorenewsmd.bse[\"X1POW1\"]) * dataset_size\n",
    "    res += best_results_vect_frcorenewsmd.params[\"X1POW1\"] * dataset_size\n",
    "    res_high += (best_results_vect_frcorenewsmd.params[\"X1POW1\"] + best_results_vect_frcorenewsmd.bse[\"X1POW1\"]) * dataset_size\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31157f",
   "metadata": {},
   "source": [
    "### 2.2.3. Print all vectorization computation time model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34707e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_vectorization: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_vectorization = fig_plot_vectorization.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_vectorization.set_xlim(xmin=0, xmax=5500)\n",
    "axis_plot_vectorization.set_ylim(ymin=0, ymax=25)\n",
    "\n",
    "# Plot computation time for tfidf.\n",
    "axis_plot_vectorization.plot(\n",
    "    df_vectorization[df_vectorization[\"algorithm_name\"]==\"tfidf\"][\"dataset_size\"],  # x\n",
    "    df_vectorization[df_vectorization[\"algorithm_name\"]==\"tfidf\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de la vectorisation 'vect.tfidf'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_vectorization.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        interpolation_vect_tfidf(x)[1]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de la vectorisation 'vect.tfidf'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_vectorization.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        interpolation_vect_tfidf(x)[0]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        interpolation_vect_tfidf(x)[2]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for frcorenewsmd.\n",
    "axis_plot_vectorization.plot(\n",
    "    df_vectorization[df_vectorization[\"algorithm_name\"]==\"spacy\"][\"dataset_size\"],  # x\n",
    "    df_vectorization[df_vectorization[\"algorithm_name\"]==\"spacy\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de la vectorisation 'vect.frcorenewsmd'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_vectorization.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        interpolation_vect_frcorenewsmd(x)[1]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de la vectorisation 'vect.frcorenewsmd'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_vectorization.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        interpolation_vect_frcorenewsmd(x)[0]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        interpolation_vect_frcorenewsmd(x)[2]\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_vectorization.set_xlabel(\"nombre de donnÃ©es [#]\", fontsize=18,)\n",
    "axis_plot_vectorization.set_ylabel(\"temps de calcul [secondes]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_vectorization.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_vectorization.grid(True)\n",
    "\n",
    "# Store the graph.\n",
    "fig_plot_vectorization.savefig(\n",
    "    \"../results/etude-temps-calcul-modelisation-2vect.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247665c",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d67024",
   "metadata": {},
   "source": [
    "## 2.3. ANALYSIS FOR CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245269f",
   "metadata": {},
   "source": [
    "> - algorithms: `kmeans_COP`, `hier_single`, `hier_complete`, `hier_average`, `hier_ward`, `spectral_SPEC`\n",
    "> - factors: `dataset_size`, `previous_nb_constraints`, `algorithm_nb_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63843bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering = pd.read_csv(\"../results/experiments_synthesis_for_clustering.csv\", sep=\";\", index_col=0)\n",
    "df_clustering[\"time_total\"] = df_clustering[\"time_total\"].str.replace(\",\", \".\").astype(float)\n",
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47de73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute interaction of factors for clustering.\n",
    "factors_interactions_for_clustering, df_clustering = compute_combinations_of_interactions_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors=[\"dataset_size\", \"previous_nb_constraints\", \"algorithm_nb_clusters\"],\n",
    "    range_of_powers=[0, 1, 2],\n",
    "    max_power=3,\n",
    ")\n",
    "factors_interactions_for_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7794e9",
   "metadata": {},
   "source": [
    "### 2.3.0. Dertermine if clustering computation time is `algorithm_name` dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d1506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_clust_ALGONAME = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ C(algorithm_name)\",\n",
    "    data=df_clustering,\n",
    ")\n",
    "results_clust_ALGONAME = model_clust_ALGONAME.fit()\n",
    "print(results_clust_ALGONAME.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b88b9f",
   "metadata": {},
   "source": [
    "### 2.3.1. Modelize `kmeans_COP` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_kmeans_COP: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"kmeans_COP\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_kmeans_COP = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_kmeans_COP,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"kmeans_COP\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-kmeans-cop.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.kmeans.cop'\",\n",
    ")\n",
    "df_correlation_clustering_kmeans_COP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_kmeans_COP = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"kmeans_COP\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW1\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_kmeans_COP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62330f82",
   "metadata": {},
   "source": [
    "> `clustering.kmeans_COP ~ 1 + dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_kmeans_COP = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW1\",  # dataset_size, (dataset_size**2)*(algorithm_nb_clusters)\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"],\n",
    ")\n",
    "best_results_clust_kmeans_COP = best_model_clust_kmeans_COP.fit()\n",
    "print(\"==============================================================================\")\n",
    "print(\">>> formula:\", best_model_clust_kmeans_COP.formula)\n",
    "print(\"==============================================================================\")\n",
    "print(best_results_clust_kmeans_COP.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.kmeans.cop ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_kmeans_COP.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_clust_kmeans_COP.params[\"X1POW1\"], factors_interactions_for_clustering[\"X1POW1\"])\n",
    "    # \"+ {0:.2E}(+/-{1:.2E})*{2}\".format(best_results_clust_kmeans_COP.params[\"X1POW2_X3POW1\"], best_results_clust_kmeans_COP.bse[\"X1POW2_X3POW1\"], factors_interactions_for_clustering[\"X1POW2_X3POW1\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_kmeans_COP(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_clust_kmeans_COP.params[\"Intercept\"] - best_results_clust_kmeans_COP.bse[\"Intercept\"])\n",
    "    res += best_results_clust_kmeans_COP.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_kmeans_COP.params[\"Intercept\"] + best_results_clust_kmeans_COP.bse[\"Intercept\"])\n",
    "    # dataset_size.\n",
    "    res_low += (best_results_clust_kmeans_COP.params[\"X1POW1\"] - best_results_clust_kmeans_COP.bse[\"X1POW1\"]) * dataset_size\n",
    "    res += best_results_clust_kmeans_COP.params[\"X1POW1\"] * dataset_size\n",
    "    res_high += (best_results_clust_kmeans_COP.params[\"X1POW1\"] + best_results_clust_kmeans_COP.bse[\"X1POW1\"]) * dataset_size\n",
    "    # (dataset_size**2)*(algorithm_nb_clusters)\n",
    "    res_low += 0  # (best_results_clust_kmeans_COP.params[\"X1POW2_X3POW1\"] - best_results_clust_kmeans_COP.bse[\"X1POW2_X3POW1\"]) * (dataset_size**2)*(algorithm_nb_clusters)\n",
    "    res += 0  # best_results_clust_kmeans_COP.params[\"X1POW2_X3POW1\"] * (dataset_size**2)*(algorithm_nb_clusters)\n",
    "    res_high += 0  # (best_results_clust_kmeans_COP.params[\"X1POW2_X3POW1\"] + best_results_clust_kmeans_COP.bse[\"X1POW2_X3POW1\"]) * (dataset_size**2)*(algorithm_nb_clusters)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d1144",
   "metadata": {},
   "source": [
    "### 2.3.2. Modelize `hier_single` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3449de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_hier_single: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_single\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_hier_single = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_hier_single,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_single\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-hier-sing.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.hier.sing'\",\n",
    ")\n",
    "df_correlation_clustering_hier_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbe183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_hier_sing = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"hier_single\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_hier_sing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314e8a6",
   "metadata": {},
   "source": [
    "> `clustering.hier_sing ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6988d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_hier_single = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW2\",  # dataset_size**2\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"hier_single\"],\n",
    ")\n",
    "best_results_clust_hier_single = best_model_clust_hier_single.fit()\n",
    "print(best_results_clust_hier_single.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.hier.sing ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_hier_single.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_clust_hier_single.params[\"X1POW2\"], factors_interactions_for_clustering[\"X1POW2\"]),\n",
    "    #\"+ {0:.2E}*{1}\".format(best_results_clust_hier_single.params[\"X1POW1_X2POW1\"], factors_interactions_for_clustering[\"X1POW1_X2POW1\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_hier_single(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_clust_hier_single.params[\"Intercept\"] - best_results_clust_hier_single.bse[\"Intercept\"])\n",
    "    res += best_results_clust_hier_single.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_hier_single.params[\"Intercept\"] + best_results_clust_hier_single.bse[\"Intercept\"])\n",
    "    # dataset_size**2.\n",
    "    res_low += (best_results_clust_hier_single.params[\"X1POW2\"] - best_results_clust_hier_single.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_clust_hier_single.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_clust_hier_single.params[\"X1POW2\"] + best_results_clust_hier_single.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # dataset_size*previous_nb_constraints.\n",
    "    res_low += 0.0  # (best_results_clust_hier_single.params[\"X1POW2\"] - best_results_clust_hier_single.bse[\"X1POW1_X2POW1\"]) * dataset_size*previous_nb_constraints\n",
    "    res += 0.0  # best_results_clust_hier_single.params[\"X1POW1_X2POW1\"] * dataset_size*previous_nb_constraints\n",
    "    res_high += 0.0  # (best_results_clust_hier_single.params[\"X1POW1_X2POW1\"] + best_results_clust_hier_single.bse[\"X1POW2\"]) * dataset_size*previous_nb_constraints\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0cfcd",
   "metadata": {},
   "source": [
    "### 2.3.3. Modelize `hier_complete` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_hier_complete: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_complete\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_hier_complete = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_hier_complete,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_complete\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-hier-comp.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.hier.comp'\",\n",
    ")\n",
    "df_correlation_clustering_hier_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4157d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_hier_comp = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"hier_complete\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_hier_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5151a",
   "metadata": {},
   "source": [
    "> `clustering.hier_complete ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_hier_complete = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW2\",  # dataset_size**2\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"hier_complete\"],\n",
    ")\n",
    "best_results_clust_hier_complete = best_model_clust_hier_complete.fit()\n",
    "print(best_results_clust_hier_complete.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25108934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.hier.comp ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_hier_complete.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}{1}\".format(best_results_clust_hier_complete.params[\"X1POW2\"], factors_interactions_for_clustering[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_hier_complete(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept\n",
    "    res_low += (best_results_clust_hier_complete.params[\"Intercept\"] - best_results_clust_hier_complete.bse[\"Intercept\"])\n",
    "    res += best_results_clust_hier_complete.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_hier_complete.params[\"Intercept\"] + best_results_clust_hier_complete.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2\n",
    "    res_low += (best_results_clust_hier_complete.params[\"X1POW2\"] - best_results_clust_hier_complete.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += (best_results_clust_hier_complete.params[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res_high += (best_results_clust_hier_complete.params[\"X1POW2\"] + best_results_clust_hier_complete.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4463b",
   "metadata": {},
   "source": [
    "### 2.3.4. Modelize `hier_average` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_hier_average: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_average\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_hier_average = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_hier_average,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_average\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-hier-avg.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.hier.avg'\",\n",
    ")\n",
    "df_correlation_clustering_hier_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c89b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_hier_avg = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"hier_average\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_hier_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0f84c",
   "metadata": {},
   "source": [
    "> `clustering.hier_average ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_hier_average = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW2\",  # dataset_size**2\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"hier_average\"],\n",
    ")\n",
    "best_results_clust_hier_average = best_model_clust_hier_average.fit()\n",
    "print(best_results_clust_hier_average.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e714c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.hier.avg ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_hier_average.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}{1}\".format(best_results_clust_hier_average.params[\"X1POW2\"], factors_interactions_for_clustering[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_hier_average(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept\n",
    "    res_low += (best_results_clust_hier_average.params[\"Intercept\"] - best_results_clust_hier_average.bse[\"Intercept\"])\n",
    "    res += best_results_clust_hier_average.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_hier_average.params[\"Intercept\"] + best_results_clust_hier_average.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2\n",
    "    res_low += (best_results_clust_hier_average.params[\"X1POW2\"] - best_results_clust_hier_average.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_clust_hier_average.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_clust_hier_average.params[\"X1POW2\"] + best_results_clust_hier_average.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32646b",
   "metadata": {},
   "source": [
    "### 2.3.5. Modelize `hier_ward` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_hier_ward: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_ward\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_hier_ward = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_hier_ward,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"hier_ward\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-hier-ward.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.hier.ward'\",\n",
    ")\n",
    "df_correlation_clustering_hier_ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_hier_ward = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"hier_ward\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_hier_ward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88cc92",
   "metadata": {},
   "source": [
    "> `clustering.hier_ward ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_hier_ward = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ X1POW2\",  # dataset_size**2\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"hier_ward\"],\n",
    ")\n",
    "best_results_clust_hier_ward = best_model_clust_hier_ward.fit()\n",
    "print(best_results_clust_hier_ward.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf90fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.hier.ward ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_hier_ward.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_clust_hier_ward.params[\"X1POW2\"], factors_interactions_for_clustering[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54cd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_hier_ward(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_clust_hier_ward.params[\"Intercept\"] - best_results_clust_hier_ward.bse[\"Intercept\"])\n",
    "    res += best_results_clust_hier_ward.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_hier_ward.params[\"Intercept\"] + best_results_clust_hier_ward.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_clust_hier_ward.params[\"X1POW2\"] - best_results_clust_hier_ward.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_clust_hier_ward.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_clust_hier_ward.params[\"X1POW2\"] + best_results_clust_hier_ward.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return. \n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed68f4f",
   "metadata": {},
   "source": [
    "### 2.3.6. Modelize `spectral_SPEC` clustering `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for clustering.\n",
    "df_correlation_clustering_spectral_SPEC: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_clustering,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"spectral_SPEC\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_clustering_spectral_SPEC = compute_information_score_evolution(\n",
    "    df=df_clustering,\n",
    "    df_correlation=df_correlation_clustering_spectral_SPEC,\n",
    "    factors_interactions=factors_interactions_for_clustering,\n",
    "    algorithm_name=\"spectral_SPEC\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-3clust-spec.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser le clustering 'clust.spec'\",\n",
    ")\n",
    "df_correlation_clustering_spectral_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_clustering_spec = compare_glm_models(\n",
    "    df=df_clustering,\n",
    "    algorithm_name=\"spectral_SPEC\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_clustering_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781ba97",
   "metadata": {},
   "source": [
    "> `clustering.spectral_SPEC ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19958ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_clust_spectral_SPEC = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ X1POW2\",  # dataset_size**2\n",
    "    data=df_clustering[df_clustering[\"algorithm_name\"]==\"spectral_SPEC\"],\n",
    ")\n",
    "best_results_clust_spectral_SPEC = best_model_clust_spectral_SPEC.fit()\n",
    "print(best_results_clust_spectral_SPEC.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e30d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"clustering.spectral.spec ~\",\n",
    "    \"{0:.2E}\".format(best_results_clust_spectral_SPEC.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_clust_spectral_SPEC.params[\"X1POW2\"], factors_interactions_for_clustering[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_clust_spectral_SPEC(dataset_size, previous_nb_constraints, algorithm_nb_clusters) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_clust_spectral_SPEC.params[\"Intercept\"] - best_results_clust_spectral_SPEC.bse[\"Intercept\"])\n",
    "    res += best_results_clust_spectral_SPEC.params[\"Intercept\"]\n",
    "    res_high += (best_results_clust_spectral_SPEC.params[\"Intercept\"] + best_results_clust_spectral_SPEC.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_clust_spectral_SPEC.params[\"X1POW2\"] - best_results_clust_spectral_SPEC.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_clust_spectral_SPEC.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_clust_spectral_SPEC.params[\"X1POW2\"] + best_results_clust_spectral_SPEC.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f6d86",
   "metadata": {},
   "source": [
    "### 2.3.7. Print all clustering computation time model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_clustering: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_clustering = fig_plot_clustering.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_clustering.set_xlim(xmin=0, xmax=5500)\n",
    "axis_plot_clustering.set_ylim(ymin=0, ymax=20000)\n",
    "\n",
    "# Plot computation time for kmeans_COP.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.kmeans.cop'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_kmeans_COP(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.kmeans.cop'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_kmeans_COP(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_kmeans_COP(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for hier_single.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_single\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_single\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.hier.sing'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_single(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.hier.sing'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_single(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_single(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for hier_complete.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_complete\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_complete\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.hier.comp'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"green\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_complete(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.hier.comp'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"green\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_complete(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_complete(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for hier_average.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_average\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_average\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.hier.avg'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"orange\",\n",
    "    markersize=3,\n",
    "    color=\"orange\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_average(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.hier.avg'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"orange\",\n",
    "    markersize=3,\n",
    "    color=\"orange\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_average(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_average(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"orange\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for hier_ward.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_ward\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"hier_ward\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.hier.ward'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"violet\",\n",
    "    markersize=3,\n",
    "    color=\"violet\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_ward(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.hier.ward'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"violet\",\n",
    "    markersize=3,\n",
    "    color=\"violet\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_ward(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_hier_ward(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"violet\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for spectral_SPEC.\n",
    "axis_plot_clustering.plot(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"spectral_SPEC\"][\"dataset_size\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"spectral_SPEC\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© du clustering 'clust.spec'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"cyan\",\n",
    "    markersize=3,\n",
    "    color=\"cyan\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_clustering.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_clust_spectral_SPEC(x, y, z)[1]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© du clustering 'clust.hier.spec'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"cyan\",\n",
    "    markersize=3,\n",
    "    color=\"cyan\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_clustering.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_clust_spectral_SPEC(x, y, z)[0]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_clust_spectral_SPEC(x, y, z)[2]\n",
    "            for y in range(0, 5500, 500)\n",
    "            for z in range(0, 55, 10)\n",
    "        ])\n",
    "        for x in range(0, 5500, 100)\n",
    "    ],  # y2\n",
    "    color=\"cyan\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_clustering.set_xlabel(\"nombre de donnÃ©es [#]\", fontsize=18,)\n",
    "axis_plot_clustering.set_ylabel(\"temps de calcul [secondes]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_clustering.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_clustering.grid(True)\n",
    "\n",
    "# Store the graph.\n",
    "fig_plot_clustering.savefig(\n",
    "    \"../results/etude-temps-calcul-modelisation-3clust.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b687d7ff",
   "metadata": {},
   "source": [
    "# Create a new figure.\n",
    "fig: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Plot computation time for kmeans.\n",
    "mean_time_kmeans_COP = np.mean(df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"time_total\"])\n",
    "std_time_kmeans_COP = statistics.stdev(df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"time_total\"])\n",
    "ax.scatter(\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"algorithm_nb_clusters\"],  # x\n",
    "    #df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"previous_nb_constraints\"],  # x\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"dataset_size\"],  # y\n",
    "    df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"time_total\"],  # z\n",
    "    label=\"clust.kmeans.cop: observations\",\n",
    "    marker=\"x\",\n",
    "    c=cm.RdYlGn(\n",
    "        Normalize(\n",
    "            vmin=-(mean_time_kmeans_COP + 3*std_time_kmeans_COP),\n",
    "            vmax=-(mean_time_kmeans_COP - 3*std_time_kmeans_COP),\n",
    "            clip=True,\n",
    "        )(-df_clustering[df_clustering[\"algorithm_name\"]==\"kmeans_COP\"][\"time_total\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "ax.set_xlabel(\"nombre clusters [#]\", fontsize=18,)\n",
    "#ax.set_ylabel(\"nombre contraintes [#]\", fontsize=18,)\n",
    "ax.set_ylabel(\"nombre de donnÃ©es [#]\", fontsize=18,)\n",
    "ax.set_zlabel(\"temps de calcul [secondes]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "ax.legend(fontsize=15,)\n",
    "\n",
    "# Plot the grid.\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318a770",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28983f2",
   "metadata": {},
   "source": [
    "## 2.4. ANALYSIS FOR SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0878fbf",
   "metadata": {},
   "source": [
    "> - algorithms: `random`, `in_same`, `farthest`, `closest`\n",
    "> - factors: `dataset_size`, `previous_nb_constraints`, `previous_nb_clusters`, `algorithm_nb_to_select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5de06d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sampling = pd.read_csv(\"../results/experiments_synthesis_for_sampling.csv\", sep=\";\", index_col=0)\n",
    "df_sampling[\"time_total\"] = df_sampling[\"time_total\"].str.replace(\",\", \".\").astype(float)\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute interaction of factors for sampling.\n",
    "factors_interactions_for_sampling, df_sampling = compute_combinations_of_interactions_of_factors(\n",
    "    df=df_sampling,\n",
    "    factors=[\"dataset_size\", \"previous_nb_constraints\", \"previous_nb_clusters\", \"algorithm_nb_to_select\"],\n",
    "    range_of_powers=[0, 1, 2],\n",
    "    max_power=3,\n",
    ")\n",
    "factors_interactions_for_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47441edf",
   "metadata": {},
   "source": [
    "### 2.4.0. Dertermine if sampling computation time is `algorithm_name` dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb70faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_samp_ALGONAME = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ C(algorithm_name)\",\n",
    "    data=df_sampling,\n",
    ")\n",
    "results_samp_ALGONAME = model_samp_ALGONAME.fit()\n",
    "print(results_samp_ALGONAME.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b746d6c",
   "metadata": {},
   "source": [
    "### 2.4.1. Modelize `random` sampling `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378357a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for sampling.\n",
    "df_correlation_sampling_random: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_sampling,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"random\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_sampling_random = compute_information_score_evolution(\n",
    "    df=df_sampling,\n",
    "    df_correlation=df_correlation_sampling_random,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"random\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-4samp-rand-full.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser l'Ã©chantillonnage 'samp.rand.full'\",\n",
    ")\n",
    "df_correlation_sampling_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_sampling_random = compare_glm_models(\n",
    "    df=df_sampling,\n",
    "    algorithm_name=\"random\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_sampling_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595aead",
   "metadata": {},
   "source": [
    "> `sampling.random ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_samp_random = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW2\",  # dataset_size**2\n",
    "    data=df_sampling[df_sampling[\"algorithm_name\"]==\"random\"],\n",
    ")\n",
    "best_results_samp_random = best_model_samp_random.fit()\n",
    "print(best_results_samp_random.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"sampling.rand.full ~\",\n",
    "    \"{0:.2E}\".format(best_results_samp_random.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_samp_random.params[\"X1POW2\"], factors_interactions_for_sampling[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_samp_random(dataset_size, previous_nb_constraints, previous_nb_clusters, algorithm_nb_to_select) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_samp_random.params[\"Intercept\"] - best_results_samp_random.bse[\"Intercept\"])\n",
    "    res += best_results_samp_random.params[\"Intercept\"]\n",
    "    res_high += (best_results_samp_random.params[\"Intercept\"] + best_results_samp_random.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_samp_random.params[\"X1POW2\"] - best_results_samp_random.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_samp_random.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_samp_random.params[\"X1POW2\"] + best_results_samp_random.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05ecd6",
   "metadata": {},
   "source": [
    "### 2.4.2. Modelize `random_in_same` sampling `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb55491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for sampling.\n",
    "df_correlation_sampling_random_in_same: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_sampling,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"in_same\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_sampling_random_in_same = compute_information_score_evolution(\n",
    "    df=df_sampling,\n",
    "    df_correlation=df_correlation_sampling_random_in_same,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"in_same\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-4samp-rand-same.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser l'Ã©chantillonnage 'samp.rand.same'\",\n",
    ")\n",
    "df_correlation_sampling_random_in_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03931966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_sampling_random_in_same = compare_glm_models(\n",
    "    df=df_sampling,\n",
    "    algorithm_name=\"in_same\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW1_X4POW1\",\n",
    "        \"time_total ~ 1 + X1POW2 + X1POW1_X4POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_sampling_random_in_same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd5508",
   "metadata": {},
   "source": [
    "> `sampling.random_in_same ~ 1 + dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_samp_random_in_same = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ 1 + X1POW2\",  # dataset_size**2\n",
    "    data=df_sampling[df_sampling[\"algorithm_name\"]==\"in_same\"],\n",
    ")\n",
    "best_results_samp_random_in_same = best_model_samp_random_in_same.fit()\n",
    "print(best_results_samp_random_in_same.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"sampling.rand.same ~\",\n",
    "    \"{0:.2E}\".format(best_results_samp_random_in_same.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_samp_random_in_same.params[\"X1POW2\"], factors_interactions_for_sampling[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_samp_random_in_same(dataset_size, previous_nb_constraints, previous_nb_clusters, algorithm_nb_to_select) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_samp_random_in_same.params[\"Intercept\"] - best_results_samp_random_in_same.bse[\"Intercept\"])\n",
    "    res += best_results_samp_random_in_same.params[\"Intercept\"]\n",
    "    res_high += (best_results_samp_random_in_same.params[\"Intercept\"] + best_results_samp_random_in_same.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_samp_random_in_same.params[\"X1POW2\"] - best_results_samp_random_in_same.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_samp_random_in_same.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_samp_random_in_same.params[\"X1POW2\"] + best_results_samp_random_in_same.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba6392",
   "metadata": {},
   "source": [
    "### 2.4.3. Modelize `farthest_in_same` sampling `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ed0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for sampling.\n",
    "df_correlation_sampling_farthest_in_same: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_sampling,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"farthest\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_sampling_farthest_in_same = compute_information_score_evolution(\n",
    "    df=df_sampling,\n",
    "    df_correlation=df_correlation_sampling_farthest_in_same,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"farthest\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-4samp-farthest-same.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser l'Ã©chantillonnage 'samp.farthest.same'\",\n",
    ")\n",
    "df_correlation_sampling_farthest_in_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e513ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_sampling_farthest_in_same = compare_glm_models(\n",
    "    df=df_sampling,\n",
    "    algorithm_name=\"farthest\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "        \"time_total ~ 1 + X1POW2 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_sampling_farthest_in_same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3102f9",
   "metadata": {},
   "source": [
    "> `sampling.farthest_in_same ~ dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_samp_farthest_in_same = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ X1POW2\",  # dataset_size**2\n",
    "    data=df_sampling[df_sampling[\"algorithm_name\"]==\"farthest\"],\n",
    ")\n",
    "best_results_samp_farthest_in_same = best_model_samp_farthest_in_same.fit()\n",
    "print(best_results_samp_farthest_in_same.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"sampling.farthest.same ~\",\n",
    "    \"{0:.2E}\".format(best_results_samp_farthest_in_same.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_samp_farthest_in_same.params[\"X1POW2\"], factors_interactions_for_sampling[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_samp_farthest_in_same(dataset_size, previous_nb_constraints, previous_nb_clusters, algorithm_nb_to_select) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_samp_farthest_in_same.params[\"Intercept\"] - best_results_samp_farthest_in_same.bse[\"Intercept\"])\n",
    "    res += best_results_samp_farthest_in_same.params[\"Intercept\"]\n",
    "    res_high += (best_results_samp_farthest_in_same.params[\"Intercept\"] + best_results_samp_farthest_in_same.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_samp_farthest_in_same.params[\"X1POW2\"] - best_results_samp_farthest_in_same.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_samp_farthest_in_same.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_samp_farthest_in_same.params[\"X1POW2\"] + best_results_samp_farthest_in_same.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69491555",
   "metadata": {},
   "source": [
    "### 2.4.4. Modelize `closest_in_different` sampling `algorithm_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649913ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of factors for sampling.\n",
    "df_correlation_sampling_closest_in_different: pd.DataFrame = compute_correlation_of_factors(\n",
    "    df=df_sampling,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"closest\",\n",
    ")\n",
    "# Print evolution of information score per model complexity.\n",
    "information_score_evolution_for_sampling_closest_in_different = compute_information_score_evolution(\n",
    "    df=df_sampling,\n",
    "    df_correlation=df_correlation_sampling_closest_in_different,\n",
    "    factors_interactions=factors_interactions_for_sampling,\n",
    "    algorithm_name=\"closest\",\n",
    "    graph_filepath=\"../results/etude-temps-calcul-analyse-facteurs-4samp-closest-diff.png\",\n",
    "    graph_plot_description=\"RÂ² pour modÃ©liser l'Ã©chantillonnage 'samp.closest.diff'\",\n",
    ")\n",
    "df_correlation_sampling_closest_in_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare several GLm models to get the best (lower Deviance, maximum Log-Likelihood)\n",
    "df_scores_sampling_closest_in_different = compare_glm_models(\n",
    "    df=df_sampling,\n",
    "    algorithm_name=\"closest\",\n",
    "    formulas=[\n",
    "        \"time_total ~ 1\",\n",
    "        \"time_total ~ 1 + X1POW2\",\n",
    "        \"time_total ~ 1 + X1POW2_X3POW1\",\n",
    "        \"time_total ~ 1 + X1POW2 + X1POW2_X3POW1\",\n",
    "    ],\n",
    ")\n",
    "df_scores_sampling_closest_in_different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f8b37",
   "metadata": {},
   "source": [
    "> `sampling.closest_in_different ~ dataset_size**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "best_model_samp_closest_in_different = statsmodels.formula.api.glm(\n",
    "    formula=\"time_total ~ X1POW2\",  # dataset_size**2\n",
    "    data=df_sampling[df_sampling[\"algorithm_name\"]==\"closest\"],\n",
    ")\n",
    "best_results_samp_closest_in_different = best_model_samp_closest_in_different.fit()\n",
    "print(best_results_samp_closest_in_different.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d86745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"sampling.closest.diff ~\",\n",
    "    \"{0:.2E}\".format(best_results_samp_closest_in_different.params[\"Intercept\"]),\n",
    "    \"+ {0:.2E}*{1}\".format(best_results_samp_closest_in_different.params[\"X1POW2\"], factors_interactions_for_sampling[\"X1POW2\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_samp_closest_in_different(dataset_size, previous_nb_constraints, previous_nb_clusters, algorithm_nb_to_select) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    res_low += (best_results_samp_closest_in_different.params[\"Intercept\"] - best_results_samp_closest_in_different.bse[\"Intercept\"])\n",
    "    res += best_results_samp_closest_in_different.params[\"Intercept\"]\n",
    "    res_high += (best_results_samp_closest_in_different.params[\"Intercept\"] + best_results_samp_closest_in_different.bse[\"Intercept\"])\n",
    "    # X1POW2: dataset_size**2.\n",
    "    res_low += (best_results_samp_closest_in_different.params[\"X1POW2\"] - best_results_samp_closest_in_different.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    res += best_results_samp_closest_in_different.params[\"X1POW2\"] * np.power(dataset_size,2)\n",
    "    res_high += (best_results_samp_closest_in_different.params[\"X1POW2\"] + best_results_samp_closest_in_different.bse[\"X1POW2\"]) * np.power(dataset_size,2)\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5daf9",
   "metadata": {},
   "source": [
    "### 2.4.5. Print all sampling computation time model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_sampling: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_sampling = fig_plot_sampling.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_sampling.set_xlim(xmin=0, xmax=5500)\n",
    "axis_plot_sampling.set_ylim(ymin=0, ymax=45)\n",
    "\n",
    "# Plot computation time for random.\n",
    "axis_plot_sampling.plot(\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"random\"][\"dataset_size\"],  # x\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"random\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de l'Ã©chantillonnage 'samp.random.full'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_sampling.plot(\n",
    "    range(0, 5500, 100),  # x1\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_samp_random(x1, x2, x3, x4)[0]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de l'Ã©chantillonnage 'samp.random.full'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_sampling.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_samp_random(x1, x2, x3, x4)[1]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_samp_random(x1, x2, x3, x4)[2]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for random_in_same.\n",
    "axis_plot_sampling.plot(\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"in_same\"][\"dataset_size\"],  # x\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"in_same\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de l'Ã©chantillonnage 'samp.random.same'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_sampling.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_samp_random_in_same(x1, x2, x3, x4)[0]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de l'Ã©chantillonnage 'samp.random.same'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_sampling.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_samp_random_in_same(x1, x2, x3, x4)[1]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_samp_random_in_same(x1, x2, x3, x4)[2]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for farthest_in_same.\n",
    "axis_plot_sampling.plot(\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"farthest\"][\"dataset_size\"],  # x\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"farthest\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de l'Ã©chantillonnage 'samp.farthest.same'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"green\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_sampling.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_samp_farthest_in_same(x1, x2, x3, x4)[0]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de l'Ã©chantillonnage 'samp.farthest.same'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"green\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_sampling.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_samp_farthest_in_same(x1, x2, x3, x4)[1]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_samp_farthest_in_same(x1, x2, x3, x4)[2]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot computation time for closest_in_different.\n",
    "axis_plot_sampling.plot(\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"closest\"][\"dataset_size\"],  # x\n",
    "    df_sampling[df_sampling[\"algorithm_name\"]==\"closest\"][\"time_total\"],  # y\n",
    "    label=\"Temps de calcul observÃ© de l'Ã©chantillonnage 'samp.closest.diff'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"orange\",\n",
    "    markersize=3,\n",
    "    color=\"orange\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_sampling.plot(\n",
    "    range(0, 5500, 100),  # x\n",
    "    [\n",
    "        np.mean([\n",
    "            interpolation_samp_closest_in_different(x1, x2, x3, x4)[0]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    label=\"Temps de calcul modÃ©lisÃ© de l'Ã©chantillonnage 'samp.closest.diff'\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"orange\",\n",
    "    markersize=3,\n",
    "    color=\"orange\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_sampling.fill_between(\n",
    "    x=range(0, 5500, 100),  # x\n",
    "    y1=[\n",
    "        np.mean([\n",
    "            interpolation_samp_closest_in_different(x1, x2, x3, x4)[1]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    y2=[\n",
    "        np.mean([\n",
    "            interpolation_samp_closest_in_different(x1, x2, x3, x4)[2]\n",
    "            for x2 in range(0, 5500, 500)\n",
    "            for x3 in range(0, 55, 5)\n",
    "            for x4 in range(0, 300, 50)\n",
    "        ])\n",
    "        for x1 in range(0, 5500, 100)\n",
    "    ],\n",
    "    color=\"orange\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_sampling.set_xlabel(\"nombre de donnÃ©es [#]\", fontsize=18,)\n",
    "axis_plot_sampling.set_ylabel(\"temps de calcul [secondes]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_sampling.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_sampling.grid(True)\n",
    "\n",
    "# Store the graph.\n",
    "fig_plot_sampling.savefig(\n",
    "    \"../results/etude-temps-calcul-modelisation-4samp.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af12fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
