{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d5b24f",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : ANNOTATION TIME STUDY ====\n",
    "> ### Stage 1 : Modelize annotation time with Interactive Clustering Methodology and Plot some figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a43a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26800",
   "metadata": {},
   "source": [
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6d2a",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at modelize interactive clustering annotation time experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder.\n",
    "- Each subdirectories of `/experiments` folder represents an annotation experiment with several annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb0f5d",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, **load experiment synthesis XLSX file** that have made during annotation experiment.\n",
    "- It contains sessions of annotation for each annotator.\n",
    "- Each session contains the number of constraints annotated and the time needed for it.\n",
    "\n",
    "Then, several analyses are performed:\n",
    "1. Check hypotheses for parametric modelization\n",
    "2. Modelize annotation time in function of constraints number\n",
    "2. Modelize annotation speed in function of session number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd89d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07718de9",
   "metadata": {},
   "source": [
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f876a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import json\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy import stats as scipystats\n",
    "import statistics\n",
    "import statsmodels\n",
    "import statsmodels.api\n",
    "import statsmodels.formula.api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afbf86",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df5fdb",
   "metadata": {},
   "source": [
    "## 2. LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c7141",
   "metadata": {},
   "source": [
    "### 2.1. Load data from XLSX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50706701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_time: pd.DataFrame = pd.read_excel(\n",
    "    io=\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/results.xlsx\",\n",
    "    sheet_name=\"time\",\n",
    "    engine=\"openpyxl\",\n",
    ")\n",
    "#df_annotation_time[\"CONSTRAINTS_PER_MINUTE\"] = df_annotation_time[\"CONSTRAINTS_PER_MINUTE\"].replace(\",\", \".\").astype(float)\n",
    "#df_annotation_time[\"CONSTRAINTS_PER_HOUR\"] = df_annotation_time[\"CONSTRAINTS_PER_HOUR\"].replace(\",\", \".\").astype(float)\n",
    "#df_annotation_time[\"SECONDS_PER_CONSTRAINT\"] = df_annotation_time[\"SECONDS_PER_CONSTRAINT\"].replace(\",\", \".\").astype(float)\n",
    "df_annotation_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e43e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Constraints number: mean={0:.2f}, median={1:.2f}, min={2:.2f}, max={3:.2f}, sigma={4:.2f}, sem={5:.2f}\".format(\n",
    "    np.mean(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "    np.median(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "    min(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "    max(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "    np.std(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "    scipystats.sem(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"]),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Session number: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}\".format(\n",
    "    np.mean(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SESSION_ID\"]),\n",
    "    min(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SESSION_ID\"]),\n",
    "    max(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SESSION_ID\"]),\n",
    "    np.std(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SESSION_ID\"]),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Needed seconds per constraints: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}\".format(\n",
    "    np.mean(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SECONDS_PER_CONSTRAINT\"]),\n",
    "    min(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SECONDS_PER_CONSTRAINT\"]),\n",
    "    max(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SECONDS_PER_CONSTRAINT\"]),\n",
    "    np.std(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SECONDS_PER_CONSTRAINT\"]),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec0bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Annotated constraints per minute: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}, sem={4:.2f}\".format(\n",
    "    np.mean(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"]),\n",
    "    min(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"]),\n",
    "    max(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"]),\n",
    "    np.std(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"]),\n",
    "    scipystats.sem(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"]),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb14d60",
   "metadata": {},
   "source": [
    "### 2.2. Check hypotheses to run parametric modelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8f8d5",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipystats.shapiro(x=df_annotation_time[\"CONSTRAINTS_PER_MINUTE\"]).pvalue\n",
    "# 4.17e-05 => \"CONSTRAINTS_PER_MINUTE\" wasn't drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284c2dc",
   "metadata": {},
   "source": [
    "The Kolmogorov-Smirnov test tests the null hypothesis that the data was drawn from a given distribution (here: a normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07019a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipystats.kstest(rvs=df_annotation_time[\"CONSTRAINTS_PER_MINUTE\"], cdf=scipystats.norm.cdf).pvalue\n",
    "# 2.71e-251 => \"CONSTRAINTS_PER_MINUTE\" wasn't drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906fb6f",
   "metadata": {},
   "source": [
    "> Conclusion: Need a non-parametric modelizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa15060",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf478bce",
   "metadata": {},
   "source": [
    "## 3. ANALYZE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41213e1c",
   "metadata": {},
   "source": [
    "### 3.0. Estimation of theorical annotation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f0d31",
   "metadata": {},
   "source": [
    "Load constraints and texts to annotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get texts.\n",
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/texts.json\", \"r\") as text_file:\n",
    "    dict_of_texts = json.load(text_file)\n",
    "# Get constraints.\n",
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/constraints_-_template_to_annotate_1.json\", \"r\") as constraints_file:\n",
    "    dict_of_constraints = json.load(constraints_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7b473",
   "metadata": {},
   "source": [
    "Estimation the time needed to read one text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of text to read.\n",
    "list_of_preprocessed_texts: List[str] = []\n",
    "for constraint in dict_of_constraints.values():\n",
    "    list_of_preprocessed_texts.append(dict_of_texts[constraint[\"data\"][\"id_1\"]][\"text_preprocessed\"])\n",
    "    list_of_preprocessed_texts.append(dict_of_texts[constraint[\"data\"][\"id_2\"]][\"text_preprocessed\"])\n",
    "# Get texts size.\n",
    "list_of_preprocessed_text_sizes: List[int] = [\n",
    "    len(text.split(\" \"))\n",
    "    for text in list_of_preprocessed_texts\n",
    "]\n",
    "print(\"Text size: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}, sem={4:.2f}\".format(\n",
    "    np.mean(list_of_preprocessed_text_sizes),\n",
    "    min(list_of_preprocessed_text_sizes),\n",
    "    max(list_of_preprocessed_text_sizes),\n",
    "    np.std(list_of_preprocessed_text_sizes),\n",
    "    scipystats.sem(list_of_preprocessed_text_sizes),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2bac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant: read speed in word per minutes.\n",
    "mean_word_read_per_minute: float = 238  # https://psycnet.apa.org/record/2019-59523-001\n",
    "mean_word_read_per_second: float = mean_word_read_per_minute / 60  # https://psycnet.apa.org/record/2019-59523-001\n",
    "\n",
    "# Estimate texts read time.\n",
    "list_of_preprocessed_text_read_times: List[float] = [\n",
    "    text_size / mean_word_read_per_second\n",
    "    for text_size in list_of_preprocessed_text_sizes\n",
    "]\n",
    "print(\"Read time: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}, sem={4:.2f}\".format(\n",
    "    np.mean(list_of_preprocessed_text_read_times),\n",
    "    min(list_of_preprocessed_text_read_times),\n",
    "    max(list_of_preprocessed_text_read_times),\n",
    "    np.std(list_of_preprocessed_text_read_times),\n",
    "    scipystats.sem(list_of_preprocessed_text_read_times),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbded4",
   "metadata": {},
   "source": [
    "Estimate the time needed to understand a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4521988",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroscience_P600: float = 0.600  # 600 ms, https://en.wikipedia.org/wiki/P600_(neuroscience)\n",
    "needed_time_to_understand_a_text: float = neuroscience_P600\n",
    "print(\"Understand one text:\", needed_time_to_understand_a_text)\n",
    "needed_time_to_understand_two_texts_concordance: float = neuroscience_P600\n",
    "print(\"Understand two texts concordance:\", needed_time_to_understand_two_texts_concordance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab1d7d",
   "metadata": {},
   "source": [
    "Estimate the time of reaction and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed_time_to_act: float = 1\n",
    "#print(\"Motor reaction:\", needed_time_to_act)\n",
    "application_delays: float = 1\n",
    "print(\"Wait for application:\", application_delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ede63",
   "metadata": {},
   "source": [
    "Complete estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate texts read time.\n",
    "list_of_preprocessed_text_read_times: List[float] = [\n",
    "    (\n",
    "        # Read and understand two text.\n",
    "        2 * text_read_time  # 2 * (needed_time_to_read_a_text + needed_time_to_understand_a_text)\n",
    "        # Estimate similarity and choose constraint to add.\n",
    "        + needed_time_to_understand_two_texts_concordance\n",
    "        # Add the constraint in the application.\n",
    "        + application_delays\n",
    "    )\n",
    "    for text_read_time in list_of_preprocessed_text_read_times\n",
    "]\n",
    "print(\"Total theorical needed time: mean={0:.2f}, min={1:.2f}, max={2:.2f}, sigma={3:.2f}, sem={4:.2f}\".format(\n",
    "    np.mean(list_of_preprocessed_text_read_times),\n",
    "    min(list_of_preprocessed_text_read_times),\n",
    "    max(list_of_preprocessed_text_read_times),\n",
    "    np.std(list_of_preprocessed_text_read_times),\n",
    "    scipystats.sem(list_of_preprocessed_text_read_times),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635c1fd",
   "metadata": {},
   "source": [
    "Some reference from `Snow et al (2008). Article Cheap and Fast — But is it Good?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Sense Disambiguation\n",
    "annotation_time_needed: float = 8.59 * 3600  # in second\n",
    "dataset_size: int = 1770\n",
    "time_for_one_annotation: float = annotation_time_needed / dataset_size\n",
    "print(\"time_for_one_annotation:\", time_for_one_annotation)\n",
    "theorical_approximation: float = 2 * 15/mean_word_read_per_second + needed_time_to_understand_two_texts_concordance + application_delays\n",
    "print(\"theorical_approximation:\", theorical_approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03288b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Similarity\n",
    "annotation_time_needed: float = 0.17 * 3600  # in second\n",
    "dataset_size: int = 300\n",
    "time_for_one_annotation: float = annotation_time_needed / dataset_size\n",
    "print(\"time_for_one_annotation:\", time_for_one_annotation)\n",
    "theorical_approximation: float = 2 * 1/mean_word_read_per_second + needed_time_to_understand_two_texts_concordance + application_delays\n",
    "print(\"theorical_approximation:\", theorical_approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognizing Textual Entailment\n",
    "annotation_time_needed: float = 89.3 * 3600  # in second\n",
    "dataset_size: int = 8000\n",
    "time_for_one_annotation: float = annotation_time_needed / dataset_size\n",
    "print(\"time_for_one_annotation:\", time_for_one_annotation)\n",
    "theorical_approximation: float = 2 * 15/mean_word_read_per_second + 10 + application_delays\n",
    "print(\"theorical_approximation:\", theorical_approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf368a",
   "metadata": {},
   "source": [
    "### 3.1. Analyze annotation time par constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_annotation_time = statsmodels.formula.api.glm(\n",
    "    formula=\"NEEDED_SECONDS ~ 0 + CONSTRAINTS_NUMBER\",\n",
    "    #formula=\"NEEDED_SECONDS ~ 1 + CONSTRAINTS_NUMBER\",\n",
    "    data=df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1],\n",
    ")\n",
    "results_annotation_time = model_annotation_time.fit()\n",
    "print(results_annotation_time.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"NEEDED_SECONDS ~\",\n",
    "    \"{0:.2E}\".format(results_annotation_time.params[\"Intercept\"]) if \"Intercept\" in results_annotation_time.params.keys() else \"\",\n",
    "    \"+ {0:.2E}*{1}\".format(results_annotation_time.params[\"CONSTRAINTS_NUMBER\"], \"CONSTRAINTS_NUMBER\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_annotation_time(constraints_number) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    if \"Intercept\" in results_annotation_time.params.keys():\n",
    "        res_low += (results_annotation_time.params[\"Intercept\"] - results_annotation_time.bse[\"Intercept\"])\n",
    "        res += results_annotation_time.params[\"Intercept\"]\n",
    "        res_high += (results_annotation_time.params[\"Intercept\"] + results_annotation_time.bse[\"Intercept\"])\n",
    "    # constraints_number.\n",
    "    res_low += (results_annotation_time.params[\"CONSTRAINTS_NUMBER\"] - results_annotation_time.bse[\"CONSTRAINTS_NUMBER\"]) * constraints_number\n",
    "    res += results_annotation_time.params[\"CONSTRAINTS_NUMBER\"] * constraints_number\n",
    "    res_high += (results_annotation_time.params[\"CONSTRAINTS_NUMBER\"] + results_annotation_time.bse[\"CONSTRAINTS_NUMBER\"]) * constraints_number\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93945a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_annotation_time: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_annotation_time = fig_plot_annotation_time.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_annotation_time.set_xlim(xmin=-3, xmax=555)\n",
    "axis_plot_annotation_time.set_ylim(ymin=-1, ymax=101)\n",
    "\n",
    "# Plot annotation time.\n",
    "axis_plot_annotation_time.plot(\n",
    "    df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_NUMBER\"],  # x\n",
    "    df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"NEEDED_SECONDS\"]/60,  # y\n",
    "    label=\"Temps d'annotation observé\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=5,\n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_annotation_time.plot(\n",
    "    range(50, 501, 10),  # x\n",
    "    [\n",
    "        interpolation_annotation_time(x)[1]/60\n",
    "        for x in range(50, 501, 10)\n",
    "    ],  # y\n",
    "    label=\"Temps d'annotation modélisé\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_annotation_time.fill_between(\n",
    "    x=range(50, 501, 10),  # x\n",
    "    y1=[\n",
    "        interpolation_annotation_time(x)[0]/60\n",
    "        for x in range(50, 501, 10)\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        interpolation_annotation_time(x)[2]/60\n",
    "        for x in range(50, 501, 10)\n",
    "    ],  # y2\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_annotation_time.set_xlabel(\"nombre de contraintes [#]\", fontsize=18,)\n",
    "axis_plot_annotation_time.set_ylabel(\"temps d'annotation [m]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_annotation_time.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_annotation_time.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_annotation_time.savefig(\n",
    "    \"../results/etude-temps-annotation-1-modelisation-temps.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f925c",
   "metadata": {},
   "source": [
    "### 3.2. Modelize annotation speed per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d13efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_annotation_speed = statsmodels.formula.api.glm(\n",
    "    formula=\"CONSTRAINTS_PER_MINUTE ~ 1 + SESSION_ID\",\n",
    "    data=df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1],\n",
    "    family=statsmodels.api.families.Gaussian(\n",
    "        link=statsmodels.genmod.families.links.identity()\n",
    "        #link=statsmodels.genmod.families.links.log()\n",
    "    ),\n",
    ")\n",
    "results_annotation_speed = model_annotation_speed.fit()\n",
    "print(results_annotation_speed.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68af60",
   "metadata": {},
   "source": [
    "> Conclusion : Variance inter-annotators too high, so no conclusion on session id effect."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1c37388",
   "metadata": {},
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"CONSTRAINTS_PER_MINUTE ~\",\n",
    "    \"{0:.2E}(+/-{1:.2E})\".format(results_annotation_speed.params[\"Intercept\"], results_annotation_speed.bse[\"Intercept\"]),\n",
    "    \"+ {0:.2E}(+/-{1:.2E})\".format(results_annotation_speed.params[\"1 | ANNOTATOR_ID\"], results_annotation_speed.bse[\"1 | ANNOTATOR_ID\"]),\n",
    "    \"+ {0:.2E}(+/-{1:.2E})*{2}\".format(results_annotation_speed.params[\"SESSION_ID\"], results_annotation_speed.bse[\"SESSION_ID\"], \"SESSION_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af831580",
   "metadata": {},
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_annotation_speed(session_id) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    if \"Intercept\" in results_annotation_speed.params.keys():\n",
    "        res_low += (results_annotation_speed.params[\"Intercept\"] - results_annotation_speed.bse[\"Intercept\"])\n",
    "        res += results_annotation_speed.params[\"Intercept\"]\n",
    "        res_high += (results_annotation_speed.params[\"Intercept\"] + results_annotation_speed.bse[\"Intercept\"])\n",
    "    # 1|annotation_id.\n",
    "    res_low -= (results_annotation_speed.params[\"1 | ANNOTATOR_ID\"] - results_annotation_speed.bse[\"1 | ANNOTATOR_ID\"])\n",
    "    res_high += (results_annotation_speed.params[\"1 | ANNOTATOR_ID\"] + results_annotation_speed.bse[\"1 | ANNOTATOR_ID\"])\n",
    "    # session_id\n",
    "    #res_low += (results_annotation_speed.params[\"SESSION_ID\"] - results_annotation_speed.bse[\"SESSION_ID\"]) * session_id\n",
    "    #res += results_annotation_speed.params[\"SESSION_ID\"] * session_id\n",
    "    #res_high += (results_annotation_speed.params[\"SESSION_ID\"] + results_annotation_speed.bse[\"SESSION_ID\"]) * session_id\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caa89c2d",
   "metadata": {},
   "source": [
    "# Create a new figure.\n",
    "fig_plot_annotation_speed: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_annotation_speed = fig_plot_annotation_speed.gca()\n",
    "\n",
    "# Plot annotation speed.\n",
    "axis_plot_annotation_speed.plot(\n",
    "    df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"SESSION_ID\"],  # x\n",
    "    df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"],  # y\n",
    "    label=\"Vitesse d'annotation observée\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_annotation_speed.plot(\n",
    "    range(0, 10, 1),  # x\n",
    "    [\n",
    "        interpolation_annotation_speed(x)[1]\n",
    "        for x in range(0, 10, 1)\n",
    "    ],  # y\n",
    "    label=\"Vitesse d'annotation modélisé\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=3,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "if with_error_bars:\n",
    "    axis_plot_annotation_speed.fill_between(\n",
    "        x=range(0, 10, 1),  # x\n",
    "        y1=[\n",
    "            interpolation_annotation_speed(x)[0]\n",
    "            for x in range(0, 10, 1)\n",
    "        ],  # y1\n",
    "        y2=[\n",
    "            interpolation_annotation_speed(x)[2]\n",
    "            for x in range(0, 10, 1)\n",
    "        ],  # y2\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_annotation_speed.set_xlabel(\"nombre de sessions [#]\", fontsize=18,)\n",
    "axis_plot_annotation_speed.set_ylabel(\"vitesse d'annotation [#/m]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_annotation_speed.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_annotation_speed.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "#fig_plot_annotation_speed.savefig(\n",
    "# \"../results/etude-temps-annotation-2-modelisation-vitesse.png\",\n",
    "#    dpi=300,\n",
    "#    transparent=True,\n",
    "#    bbox_inches=\"tight\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a97c4c0",
   "metadata": {},
   "source": [
    "### 3.3. Case study of some annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fee24",
   "metadata": {},
   "source": [
    "> Specific study of annotators `3`,`7`,`9` ; `1`,`5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f56a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_annotator_speed_study: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_annotator_speed_study = fig_plot_annotator_speed_study.gca()\n",
    "\n",
    "# Set axis.\n",
    "axis_plot_annotator_speed_study.set_xlim(xmin=0.9, xmax=9.1)\n",
    "axis_plot_annotator_speed_study.set_ylim(ymin=-0.1, ymax=16.1)\n",
    "\n",
    "# Plot for annotation speed for some annotators.\n",
    "colors = [\n",
    "    \"orange\", \"red\",\n",
    "    \"blue\", \"purple\",\n",
    "]\n",
    "markers = [\n",
    "    \">\", \">\",\n",
    "    \"^\", \"^\",\n",
    "]\n",
    "for i, annotator_id in enumerate([\n",
    "    1, 5,  # constant slope\n",
    "    7, 9, # increasing slope\n",
    "]):\n",
    "    axis_plot_annotator_speed_study.plot(\n",
    "        df_annotation_time[(df_annotation_time[\"ANNOTATOR_ID\"]==annotator_id)&(df_annotation_time[\"EXPERIMENT_ID\"]==1)][\"SESSION_ID\"],  # x\n",
    "        df_annotation_time[(df_annotation_time[\"ANNOTATOR_ID\"]==annotator_id)&(df_annotation_time[\"EXPERIMENT_ID\"]==1)][\"CONSTRAINTS_PER_MINUTE\"],  # y\n",
    "        label=\"Vitesse d'annotation observée pour l'annotateur \"+str(annotator_id),\n",
    "        marker=markers[i],\n",
    "        markerfacecolor=colors[i],\n",
    "        markersize=3,\n",
    "        color=colors[i],\n",
    "        linewidth=1,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    \n",
    "# mean of annotation speed.\n",
    "average_annotation_speed: float = np.mean(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"])\n",
    "sem_annotatiom_speed: float = scipystats.sem(df_annotation_time[df_annotation_time[\"EXPERIMENT_ID\"]==1][\"CONSTRAINTS_PER_MINUTE\"])\n",
    "axis_plot_annotator_speed_study.plot(\n",
    "    range(0, 10),  # x\n",
    "    [\n",
    "        average_annotation_speed\n",
    "        for _ in range(0, 10)\n",
    "    ],  # y\n",
    "    label=\"Vitesse d'annotation moyenne\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"black\",\n",
    "    markersize=5,\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axis_plot_annotator_speed_study.fill_between(\n",
    "    x=range(0, 10),  # x\n",
    "    y1=[\n",
    "        average_annotation_speed-sem_annotatiom_speed\n",
    "        for _ in range(0, 10)\n",
    "    ],  # y\n",
    "    y2=[\n",
    "        average_annotation_speed+sem_annotatiom_speed\n",
    "        for _ in range(0, 10)\n",
    "    ],  # y\n",
    "    color=\"black\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_annotator_speed_study.set_xlabel(\"session d'annotation [#]\", fontsize=18,)\n",
    "axis_plot_annotator_speed_study.set_ylabel(\"vitesse d'annotation [#/m]\", fontsize=18,)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_annotator_speed_study.legend(\n",
    "    loc=\"lower right\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_annotator_speed_study.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_annotator_speed_study.savefig(\n",
    "    \"../results/etude-temps-annotation-3-etude-de-cas.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf5276",
   "metadata": {},
   "source": [
    "----\n",
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725ce3",
   "metadata": {},
   "source": [
    "1. hypothèse temps annotation est linéaire\n",
    "    - OK: afficher temps/constraint\n",
    "\n",
    "2. hypothèse vitesse augmente en fonction du nombre de session\n",
    "    - KO: variation inter-annotateur trop forte\n",
    "    - Stats descriptives\n",
    "    - Discussion de quelques cas : un qui augmente, un qui stagne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a29ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
