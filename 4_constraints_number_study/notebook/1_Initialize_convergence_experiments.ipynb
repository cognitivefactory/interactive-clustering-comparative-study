{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : CONSTRAINTS NUMBER STUDY ====\n",
    "> ### Stage 1 : Initialize computation environments for experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at create environments needed to run constraints number study experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[PREPROCESSING]/[VECTORIZATION]/[SAMPLING]/[CLUSTERING]/[EXPERIMENT]`.\n",
    "- Each subpath corresponds to a part of Interactive Clustering methodology : A. Load dataset ; B. Preprocess dataset ; C. Vectorize dataset ; D. Sample data for constraints annotation ; E. Cluster data with constraints. The last subdirectory define the random seed of the observation.\n",
    "\n",
    "At beginning of the comparative study, **run this notebook to set up experiments you want**.\n",
    "\n",
    "Then, **go to the notebook `2_Run_until_convergence_and_evaluate_efficience.ipynb` to run and evaluate each experiment you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "- 2.1. **Set up `Dataset` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters for the dataset and pre-format dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of datatset environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_texts.json`: texts from the dataset;\n",
    "        - `dict_of_true_intents.json`: true intent from the dataset;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available datasets_:\n",
    "        - [French trainset for chatbots dealing with usual requests on bank cards v2.0.0](http://doi.org/10.5281/zenodo.4769949)\n",
    "        - [MLSUM: The Multilingual Summarization Corpus](https://arxiv.org/abs/2004.14900v1), subsetted and filtered by SCHILD E. (v1.0.0).\n",
    "    - _Available dataset settings_:\n",
    "        - define dataset size;\n",
    "        - define random seed.\n",
    "\n",
    "- 2.2. **Set up `Preprocessing` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters and preprocess the dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of preprocessing environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_preprocessed_texts.json`: preprocessed texts computed from `dict_of_texts.json`;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available preprocessing settings_:\n",
    "        - apply simple preprocessing (lowercase, punctuation, accent, whitespace).\n",
    "\n",
    "- 2.3. **Set up `Vectorization` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters and vectorize the preprocessed dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of vectorization environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_vectors.pkl`: vectors computed from `dict_of_preprocessed_texts.json`;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available vectorization settings_:\n",
    "        - TF-IDF vectorizer;\n",
    "        - spaCy `fr_core_news_md` language model.\n",
    "\n",
    "- 2.4. **Set up `Sampling` environments**:\n",
    "    - _Description_: Create a subdirectory and store parameters.\n",
    "    - _Setting_: A dictionary define all possible configurations of sampling environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json` : a json file with all parameters.\n",
    "    - _Available sampling settings_:\n",
    "        - apply sampling of closest data from two different clusters.\n",
    "\n",
    "- 2.5. **Set up `Clustering` environments**:\n",
    "    - _Description_: Create a subdirectory and store parameters.\n",
    "    - _Setting_: A dictionary define all possible configurations of clustering environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json` : a json file with all parameters.\n",
    "    - _Available clustering settings_:\n",
    "        - apply constrained kmeans clustering (model COP);\n",
    "        - define number of clusters.\n",
    "\n",
    "- 2.6. **Set up `Experiment` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters and initialize results files storage.\n",
    "    - _Setting_: A dictionary define all possible configurations of experiment environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_constraints_annotations.json`: all annotations over interactive-clustering iterations;\n",
    "        - `dict_of_clustering_results.json`: clustering results over interactive-clustering iterations;\n",
    "        - `dict_of_computation_times.json`: computation times over interactive-clustering iterations;\n",
    "        - `dict_of_clustering_performances.json`: clustering performances over interactive-clustering iterations.\n",
    "    - _Available experiment settings_:\n",
    "        - define the random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faker\n",
    "import listing_envs\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle  # noqa: S403\n",
    "from cognitivefactory.interactive_clustering.utils.preprocessing import (\n",
    "    preprocess,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.utils.vectorization import (\n",
    "    vectorize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. CREATE COMPUTATION ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.1. Set `Dataset` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_DATASETS: Dict[str, Any] = {}\n",
    "for size in range(1000, 5000+250, 250):\n",
    "    for rand in [1, 2, 3,]:\n",
    "        # Case of bank cards management.\n",
    "        ENVIRONMENTS_FOR_DATASETS[\"bank_cards_v2-size_{size_str}-rand_{rand_str}\".format(size_str=size, rand_str=rand)] = {\n",
    "            \"_TYPE\": \"dataset\",\n",
    "            \"_DESCRIPTION\": \"This dataset represents examples of common customer requests relating to bank cards management. It can be used as a training set for a small chatbot intended to process these usual requests.\",\n",
    "            \"file_name\": \"French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0.xlsx\",\n",
    "            \"sheet_name\": \"dataset\",\n",
    "            \"columns\": [\"QUESTION\", \"INTENT\"],\n",
    "            \"language\": \"fr\",\n",
    "            \"size\": size,\n",
    "            \"random_seed\": rand,\n",
    "            \"nb_clusters\": 10,\n",
    "        }\n",
    "        # Case of MLSUM.\n",
    "        ENVIRONMENTS_FOR_DATASETS[\"mlsum_fr_train_subset_v1-size_{size_str}-rand_{rand_str}\".format(size_str=size, rand_str=rand)] = {\n",
    "            \"_TYPE\": \"dataset\",\n",
    "            \"_DESCRIPTION\": \"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset. For constraints annotation experiment based on data similarity, this dataset have been subsetted (randomly pick 75 articles in the following 14 most used topics: 'economie', 'politique', 'sport', 'planete' (renamed in 'ecologie'), 'sciences', 'police-justice', 'disparitions', 'emploi', 'sante', 'musiques', 'arts', 'educations', 'climat' (renamed in 'meteo'), 'immobilier') and filtered (keep articles that have an obvious topics regarding their titles, without their bodies). Two reviewers have wrking on this task in order to limit the subjectivity of the filtering. This subsetted dataset is used (1) to estimate needed time to annotate titles similarity with constraints (MUST-LINK, CANNOT-LINK) and (2) to test interactive clustering methodology (constraints annotation and constrained clustering).\",\n",
    "            \"file_name\": \"mlsum_fr_train_subset_v1.0.0.schild.xlsx\",\n",
    "            \"sheet_name\": \"dataset\",\n",
    "            \"columns\": [\"title\", \"topic\"],\n",
    "            \"language\": \"fr\",\n",
    "            \"size\": size,\n",
    "            \"random_seed\": rand,\n",
    "            \"nb_clusters\": 14,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `dataset` environments using `ENVIRONMENTS_FOR_DATASETS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "END - Dataset environments configuration.\n"
     ]
    }
   ],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for ENV_NAME_dataset, CONFIG_dataset in ENVIRONMENTS_FOR_DATASETS.items():\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Name the configuration.\n",
    "    CONFIG_dataset[\"_ENV_NAME\"] = ENV_NAME_dataset\n",
    "    CONFIG_dataset[\"_ENV_PATH\"] = \"../experiments/\" + ENV_NAME_dataset + \"/\"\n",
    "\n",
    "    # Check if the environment already exists.\n",
    "    if os.path.exists(str(CONFIG_dataset[\"_ENV_PATH\"])):\n",
    "        continue\n",
    "\n",
    "    # Create directory for this environment.\n",
    "    os.mkdir(str(CONFIG_dataset[\"_ENV_PATH\"]))\n",
    "\n",
    "    # Store configuration file.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_d1:\n",
    "        json.dump(CONFIG_dataset, file_d1, indent=1)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### LOAD DATASET.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Load dataset.\n",
    "    df_dataset: pd.DataFrame = pd.read_excel(\n",
    "        io=\"../../datasets/\" + CONFIG_dataset[\"file_name\"],\n",
    "        sheet_name=CONFIG_dataset[\"sheet_name\"],\n",
    "        engine=\"openpyxl\",\n",
    "    )\n",
    "    \n",
    "    # Drop duplicates.\n",
    "    df_dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### LOAD DATASET.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Load base for `dict_of_texts` and `dict_of_true_intents`.\n",
    "    # > Force `str` type to avoid typing errors.\n",
    "\n",
    "    base_dict_of_texts: Dict[str, str] = {\n",
    "        str(data_id): str(value[\n",
    "            CONFIG_dataset[\"columns\"][0]\n",
    "        ])\n",
    "        for data_id, value in df_dataset.to_dict(\"index\").items()\n",
    "    }\n",
    "\n",
    "    base_dict_of_true_intents: Dict[str, str] = {\n",
    "        str(data_id): str(value[\n",
    "            CONFIG_dataset[\"columns\"][1]\n",
    "        ])\n",
    "        for data_id, value in df_dataset.to_dict(\"index\").items()\n",
    "    }\n",
    "\n",
    "    # Fake dataset if needed (i.e. artificially add data by generating random spelling errors).\n",
    "    faker_results: Tuple[Dict[str, str], Dict[str, str]] = faker.fake_dataset(\n",
    "        dict_of_texts=base_dict_of_texts,\n",
    "        dict_of_true_intents=base_dict_of_true_intents,\n",
    "        size=CONFIG_dataset[\"size\"],\n",
    "        random_seed=CONFIG_dataset[\"random_seed\"],\n",
    "    )\n",
    "    dict_of_texts: Dict[str, str] = faker_results[0]\n",
    "    dict_of_true_intents: Dict[str, str] = faker_results[1]\n",
    "\n",
    "    # Store `dict_of_texts` and `dict_of_true_intents`.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_texts.json\", \"w\") as file_d2:\n",
    "        json.dump(dict_of_texts, file_d2, indent=1)\n",
    "\n",
    "    with open(\n",
    "        str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_true_intents.json\", \"w\"\n",
    "    ) as file_d3:\n",
    "        json.dump(dict_of_true_intents, file_d3, indent=1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Dataset environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.2. Set `Preprocessing` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `dataset` environments in which create `preprocessing` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are `102` created dataset environments in `../experiments`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../experiments/bank_cards_v2-size_1000-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_3/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_1/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_2/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_3/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_1/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_2/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_3/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of dataset environments.\n",
    "LIST_OF_DATASET_ENVIRONMENTS: List[str] = listing_envs.get_list_of_dataset_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_DATASET_ENVIRONMENTS)) + \"`\",\n",
    "    \"created dataset environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_DATASET_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_PREPROCESSING: Dict[str, Any] = {\n",
    "    # Case of simple preprocessing (lowercase, accents, punctuation, whitspace).\n",
    "    \"simple_prep\": {\n",
    "        \"_TYPE\": \"preprocessing\",\n",
    "        \"_DESCRIPTION\": \"Simple preprocessing (lowercase, accents, punctuation, whitspace)\",\n",
    "        \"apply_preprocessing\": True,\n",
    "        \"apply_lemmatization\": False,\n",
    "        \"apply_parsing_filter\": False,\n",
    "        \"spacy_language_model\": \"fr_core_news_md\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `preprocessing` environments using `ENVIRONMENTS_FOR_PREPROCESSING` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "END - Preprocessing environments configuration.\n"
     ]
    }
   ],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_dataset in LIST_OF_DATASET_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_preprocessing,\n",
    "        CONFIG_preprocessing,\n",
    "    ) in ENVIRONMENTS_FOR_PREPROCESSING.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_preprocessing[\"_ENV_NAME\"] = ENV_NAME_preprocessing\n",
    "        CONFIG_preprocessing[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_dataset + ENV_NAME_preprocessing + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_preprocessing[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_preprocessing[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_preprocessing[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_p1:\n",
    "            json.dump(CONFIG_preprocessing, file_p1, indent=1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### PREPROCESS DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load preprocess dataset.\n",
    "        with open(\n",
    "            str(CONFIG_preprocessing[\"_ENV_PATH\"]) + \"../dict_of_texts.json\", \"r\"\n",
    "        ) as file_p2:\n",
    "            texts: Dict[str, str] = json.load(file_p2)\n",
    "\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = {}\n",
    "\n",
    "        # Case with preprocessing.\n",
    "        if bool(CONFIG_preprocessing[\"apply_preprocessing\"]):\n",
    "            dict_of_preprocessed_texts = preprocess(\n",
    "                dict_of_texts=texts,\n",
    "                apply_lemmatization=bool(CONFIG_preprocessing[\"apply_lemmatization\"]),\n",
    "                apply_parsing_filter=bool(CONFIG_preprocessing[\"apply_parsing_filter\"]),\n",
    "                spacy_language_model=str(CONFIG_preprocessing[\"spacy_language_model\"]),\n",
    "            )\n",
    "\n",
    "        # Case without preprocessing.\n",
    "        else:\n",
    "            dict_of_preprocessed_texts = texts\n",
    "\n",
    "        # Store preprocessed texts.\n",
    "        with open(\n",
    "            str(CONFIG_preprocessing[\"_ENV_PATH\"]) + \"dict_of_preprocessed_texts.json\",\n",
    "            \"w\",\n",
    "        ) as file_p3:\n",
    "            json.dump(dict_of_preprocessed_texts, file_p3, indent=1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Preprocessing environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.3. Set `Vectorization` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `preprocessing` environments in which create `vectorization` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are `102` created preprocessing environments in `../experiments`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../experiments/bank_cards_v2-size_1000-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_3/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_1/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_2/simple_prep/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_3/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_1/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_2/simple_prep/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_3/simple_prep/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of preprocessing environments.\n",
    "LIST_OF_PREPROCESSING_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_preprocessing_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_PREPROCESSING_ENVIRONMENTS)) + \"`\",\n",
    "    \"created preprocessing environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_PREPROCESSING_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `vectorization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_VECTORIZATION: Dict[str, Any] = {\n",
    "    # Case of TFIDF vectorization.\n",
    "    \"tfidf\": {\n",
    "        \"_TYPE\": \"vectorization\",\n",
    "        \"_DESCRIPTION\": \"TFIDF vectorization.\",\n",
    "        \"vectorizer_type\": \"tfidf\",\n",
    "        \"spacy_language_model\": None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `vectorization` environments using `ENVIRONMENTS_FOR_VECTORIZATION` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "END - Vectorization environments configuration.\n"
     ]
    }
   ],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_preprocessing in LIST_OF_PREPROCESSING_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_vectorization,\n",
    "        CONFIG_vectorization,\n",
    "    ) in ENVIRONMENTS_FOR_VECTORIZATION.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_vectorization[\"_ENV_NAME\"] = ENV_NAME_vectorization\n",
    "        CONFIG_vectorization[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_preprocessing + ENV_NAME_vectorization + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_vectorization[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_vectorization[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_vectorization[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_v1:\n",
    "            json.dump(CONFIG_vectorization, file_v1, indent=1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### VECTORIZE DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load preprocess dataset.\n",
    "        with open(\n",
    "            str(CONFIG_vectorization[\"_ENV_PATH\"])\n",
    "            + \"../dict_of_preprocessed_texts.json\",\n",
    "            \"r\",\n",
    "        ) as file_v2:\n",
    "            preprocessed_texts: Dict[str, str] = json.load(file_v2)\n",
    "\n",
    "        # Vectorize dataset.\n",
    "        dict_of_vectors: Dict[str, csr_matrix] = vectorize(\n",
    "            dict_of_texts=preprocessed_texts,\n",
    "            vectorizer_type=str(CONFIG_vectorization[\"vectorizer_type\"]),\n",
    "            spacy_language_model=str(CONFIG_vectorization[\"spacy_language_model\"]),\n",
    "        )\n",
    "\n",
    "        # Store vectors.\n",
    "        with open(\n",
    "            str(CONFIG_vectorization[\"_ENV_PATH\"]) + \"dict_of_vectors.pkl\", \"wb\"\n",
    "        ) as file_v3:\n",
    "            pickle.dump(dict_of_vectors, file_v3)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Vectorization environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.4. Set `Sampling` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `vectorization` environments in which create `sampling` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are `102` created vectorization environments in `../experiments`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../experiments/bank_cards_v2-size_1000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_3/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_1/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_2/simple_prep/tfidf/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_3/simple_prep/tfidf/']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of vectorization environments.\n",
    "LIST_OF_VECTORIZATION_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_vectorization_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_VECTORIZATION_ENVIRONMENTS)) + \"`\",\n",
    "    \"created vectorization environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_VECTORIZATION_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `sampling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_SAMPLING: Dict[str, Any] = {\n",
    "    # Case of Closest in different clusters sampling, 50 selected combination per iteration.\n",
    "    \"closest-50\": {\n",
    "        \"_TYPE\": \"sampling\",\n",
    "        \"_DESCRIPTION\": \"Closest in different clusters sampling, 50 selected combination per iteration.\",\n",
    "        \"algorithm\": \"closest_in_different_clusters\",\n",
    "        \"nb_to_select\": 50,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `sampling` environments using `ENVIRONMENTS_FOR_SAMPLING` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "END - Sampling environments configuration.\n"
     ]
    }
   ],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_vectorization in LIST_OF_VECTORIZATION_ENVIRONMENTS:\n",
    "    for ENV_NAME_sampling, CONFIG_sampling in ENVIRONMENTS_FOR_SAMPLING.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_sampling[\"_ENV_NAME\"] = ENV_NAME_sampling\n",
    "        CONFIG_sampling[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_vectorization + ENV_NAME_sampling + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_sampling[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_sampling[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(str(CONFIG_sampling[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_s1:\n",
    "            json.dump(CONFIG_sampling, file_s1, indent=1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Sampling environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.5. Set `Clustering` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `sampling` environments in which create `clustering` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are `102` created sampling environments in `../experiments`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../experiments/bank_cards_v2-size_1000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_1750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_2750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_3750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_4750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/bank_cards_v2-size_5000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_1750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_2750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_3750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4000-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4250-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4500-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_4750-rand_3/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_1/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_2/simple_prep/tfidf/closest-50/',\n",
       " '../experiments/mlsum_fr_train_subset_v1-size_5000-rand_3/simple_prep/tfidf/closest-50/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of sampling environments.\n",
    "LIST_OF_SAMPLING_ENVIRONMENTS: List[str] = listing_envs.get_list_of_sampling_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_SAMPLING_ENVIRONMENTS)) + \"`\",\n",
    "    \"created sampling environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_SAMPLING_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `clustering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_CLUSTERING: Dict[str, Any] = {\n",
    "    # Case of KMeans clustering, {0} clusters, model 'COP'.\n",
    "    \"kmeans_COP-{0}c\": {\n",
    "        \"_TYPE\": \"clustering\",\n",
    "        \"_DESCRIPTION\": \"KMeans clustering, 10 clusters, model 'COP'.\",\n",
    "        \"algorithm\": \"kmeans\",\n",
    "        \"init**kargs\": {\n",
    "            \"model\": \"COP\",\n",
    "            \"max_iteration\": 150,\n",
    "            \"tolerance\": 1e-4,\n",
    "        },\n",
    "        #\"nb_clusters\": None,  # Will be set in the loop by loading the dataset configuration.\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `clustering` environments using `ENVIRONMENTS_FOR_CLUSTERING` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "END - Clustering environments configuration.\n"
     ]
    }
   ],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_sampling in LIST_OF_SAMPLING_ENVIRONMENTS:\n",
    "    for ENV_NAME_clustering, CONFIG_clustering in ENVIRONMENTS_FOR_CLUSTERING.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        # Get number of clusters from the dataset configuration.\n",
    "        with open(PARENT_ENV_PATH_sampling + \"../../../config.json\", \"r\") as file_c1:\n",
    "            nb_clusters = json.load(file_c1)[\"nb_clusters\"]\n",
    "        \n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_clustering[\"_ENV_NAME\"] = ENV_NAME_clustering.format(nb_clusters)\n",
    "        CONFIG_clustering[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_sampling + ENV_NAME_clustering.format(nb_clusters) + \"/\"\n",
    "        )\n",
    "        CONFIG_clustering[\"nb_clusters\"] = nb_clusters\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_clustering[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_clustering[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(str(CONFIG_clustering[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_c2:\n",
    "            json.dump(CONFIG_clustering, file_c2, indent=1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Clustering environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.6. Set `Experiment ID` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `clustering` environments in which create `experiment` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of clustering environments.\n",
    "LIST_OF_CLUSTERING_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_clustering_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_CLUSTERING_ENVIRONMENTS)) + \"`\",\n",
    "    \"created clustering environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_CLUSTERING_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `experiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANAGER_TYPE: str = \"binary\"\n",
    "LIST_OF_EXPERIMENT_IDS: List[int] = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `experiment` environments until there are `NUMBER_OF_ENVIRONMENTS_FOR_EXPERIMENT_TO_HAVE` experiment environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_clustering in LIST_OF_CLUSTERING_ENVIRONMENTS:\n",
    "    for EXPERIMENT_ID in LIST_OF_EXPERIMENT_IDS:\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_experiment = {\n",
    "            \"_ENV_NAME\": str(EXPERIMENT_ID).zfill(4),\n",
    "            \"_ENV_PATH\": PARENT_ENV_PATH_clustering + str(EXPERIMENT_ID).zfill(4) + \"/\",\n",
    "            \"EXPERIMENT_ID\": EXPERIMENT_ID,\n",
    "            \"random_seed\": EXPERIMENT_ID,\n",
    "            \"manager_type\": MANAGER_TYPE,\n",
    "        }\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_experiment[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_experiment[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(str(CONFIG_experiment[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_e1:\n",
    "            json.dump(CONFIG_experiment, file_e1, indent=1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### INITIALIZE SOME INFORMATION.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Store dictionary of clustering results.\n",
    "        with open(\n",
    "            str(CONFIG_experiment[\"_ENV_PATH\"]) + \"dict_of_clustering_results.json\", \"w\"\n",
    "        ) as file_e2:\n",
    "            json.dump({}, file_e2, indent=1)\n",
    "\n",
    "        # Store dictionary of clustering performances.\n",
    "        with open(\n",
    "            str(CONFIG_experiment[\"_ENV_PATH\"])\n",
    "            + \"dict_of_clustering_performances.json\",\n",
    "            \"w\",\n",
    "        ) as file_e3:\n",
    "            json.dump({}, file_e3, indent=1)\n",
    "\n",
    "        # Store dictionary of computation time.\n",
    "        with open(\n",
    "            str(CONFIG_experiment[\"_ENV_PATH\"]) + \"dict_of_computation_times.json\", \"w\"\n",
    "        ) as file_e4:\n",
    "            json.dump({}, file_e4, indent=1)\n",
    "\n",
    "        # Store dictionary of annotation history.\n",
    "        with open(\n",
    "            str(CONFIG_experiment[\"_ENV_PATH\"])\n",
    "            + \"dict_of_constraints_annotations.json\",\n",
    "            \"w\",\n",
    "        ) as file_e5:\n",
    "            json.dump({}, file_e5, indent=1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Experiment environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 3. Get all created environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_experiment_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
