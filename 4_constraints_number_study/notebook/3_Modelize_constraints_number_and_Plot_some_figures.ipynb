{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aaf1c9",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : CONSTRAINTS NUMBER STUDY ====\n",
    "> ### Stage 3 : Modelize constraints number required to converge and Plot some figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a43a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26800",
   "metadata": {},
   "source": [
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6d2a",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at modelize constraints number required to converge and plot several figures according to previous analyses**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[PREPROCESSING]/[VECTORIZATION]/[SAMPLING]/[CLUSTERING]/[EXPERIMENT]`.\n",
    "- An experiment run is composed of iterations of _interative clustering_.\n",
    "- An experiment evaluation look at each _interative clustering_ iteration of the experiment.\n",
    "\n",
    "Before running, **run the notebook `2_Run_until_convergence_and_evaluate_constraints_number_required.ipynb` to run interactive clustering experiment until convergence and estimate constraints number required to converge.**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ca79c",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, **load experiment synthesis CSV file** that have made during interactive clustering experiments.\n",
    "1. Modelize constraints number in function of dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0ea66",
   "metadata": {},
   "source": [
    "***WARNING***: _Start by launching the experiment runs, evaluations and synthesis, and launching main effects analysis before this section !_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd89d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c66064",
   "metadata": {},
   "source": [
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy import stats as scipystats\n",
    "import statistics\n",
    "import statsmodels\n",
    "import statsmodels.api\n",
    "import statsmodels.formula.api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668c71a",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef321a",
   "metadata": {},
   "source": [
    "## 2. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments: pd.DataFrame = pd.read_csv(\n",
    "    filepath_or_buffer=\"../results/experiments_synthesis.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "#df_experiments[\"V090v__constraints_total\"] = df_experiments[\"V090v__constraints_total\"].replace(\",\", \".\").astype(float)\n",
    "df_experiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ba2a7",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1327d1a",
   "metadata": {},
   "source": [
    "## 3. ANALYZE DATA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06f56f2d",
   "metadata": {},
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_by_dataset = statsmodels.formula.api.glm(\n",
    "    #formula=\"V090v__constraints_total ~ 1 + C(dataset_reference)\",\n",
    "    #formula=\"V090v__constraints_total ~ 0 + dataset_size\",\n",
    "    #formula=\"V090v__constraints_total ~ 1 + dataset_size\",\n",
    "    #formula=\"V090v__constraints_total ~ 0 + dataset_size*C(dataset_reference)\",\n",
    "    #formula=\"V090v__constraints_total ~ 1 + dataset_size*C(dataset_reference)\",\n",
    "    #data=df_experiments,\n",
    "    #data=df_experiments[df_experiments[\"dataset_reference\"]==\"bank_cards_v2\"],\n",
    "    data=df_experiments[df_experiments[\"dataset_reference\"]==\"mlsum_fr_train_subset_v1\"],\n",
    ")\n",
    "results_by_dataset = model_by_dataset.fit()\n",
    "print(results_by_dataset.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbed07",
   "metadata": {},
   "source": [
    "- `pvalue(dataset_reference) < 10**(-3)`\n",
    "- `bank_cards_v2            ~ -52 + 2.91 * dataset_size` / `~  2.90 * dataset_size`\n",
    "- `mlsum_fr_train_subset_v1 ~ 763 + 3.19 * dataset_size` / `~  3.41 * dataset_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3.41-3.15, (3.41-3.15)/3.15*100)\n",
    "print(3.15-2.90, (3.15-2.90)/3.15*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03697024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and print results.\n",
    "model_constraints_number = statsmodels.formula.api.glm(\n",
    "    formula=\"V090v__constraints_total ~ 0 + dataset_size\",\n",
    "    data=df_experiments,\n",
    ")\n",
    "results_constraints_number = model_constraints_number.fit()\n",
    "print(results_constraints_number.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869df1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modelization.\n",
    "print(\n",
    "    \"CONSTRAINTS_NUMBER ~\",\n",
    "    \"{0:.2E}\".format(results_constraints_number.params[\"Intercept\"]) if \"Intercept\" in results_constraints_number.params.keys() else \"\",\n",
    "    \"+ {0:.2E}*{1}\".format(results_constraints_number.params[\"dataset_size\"], \"dataset_size\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8587252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation function.\n",
    "def interpolation_constraints_number(dataset_size) -> Tuple[float, float, float]:\n",
    "    # Initialization.\n",
    "    res_low: float = 0.0\n",
    "    res: float = 0.0\n",
    "    res_high: float = 0.0\n",
    "    # Intercept.\n",
    "    if \"Intercept\" in results_constraints_number.params.keys():\n",
    "        res_low += (results_constraints_number.params[\"Intercept\"] - results_constraints_number.bse[\"Intercept\"])\n",
    "        res += results_constraints_number.params[\"Intercept\"]\n",
    "        res_high += (results_constraints_number.params[\"Intercept\"] + results_constraints_number.bse[\"Intercept\"])\n",
    "    # constraints_number.\n",
    "    res_low += (results_constraints_number.params[\"dataset_size\"] - results_constraints_number.bse[\"dataset_size\"]) * dataset_size\n",
    "    res += results_constraints_number.params[\"dataset_size\"] * dataset_size\n",
    "    res_high += (results_constraints_number.params[\"dataset_size\"] + results_constraints_number.bse[\"dataset_size\"]) * dataset_size\n",
    "    # Return.\n",
    "    return res_low, res, res_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e939783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_constraints_number: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_constraints_number = fig_plot_constraints_number.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_constraints_number.set_xlim(xmin=0, xmax=5250)\n",
    "axis_plot_constraints_number.set_ylim(ymin=0, ymax=20000)\n",
    "\n",
    "# Plot constraints number (observations).\n",
    "axis_plot_constraints_number.plot(\n",
    "    df_experiments[df_experiments[\"dataset_reference\"]==\"bank_cards_v2\"][\"dataset_size\"],  # x\n",
    "    df_experiments[df_experiments[\"dataset_reference\"]==\"bank_cards_v2\"][\"V090v__constraints_total\"],  # y\n",
    "    label=\"Nombre de contraintes observé pour 'Bank Cards (v2.0.0)'\",\n",
    "    marker=\"x\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=5, \n",
    "    color=\"red\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "axis_plot_constraints_number.plot(\n",
    "    df_experiments[df_experiments[\"dataset_reference\"]==\"mlsum_fr_train_subset_v1\"][\"dataset_size\"],  # x\n",
    "    df_experiments[df_experiments[\"dataset_reference\"]==\"mlsum_fr_train_subset_v1\"][\"V090v__constraints_total\"],  # y\n",
    "    label=\"Nombre de contraintes observé pour 'MLSUM FR Train Subset (v1.0.0-schild)'\",\n",
    "    marker=\"+\",\n",
    "    markerfacecolor=\"blue\",\n",
    "    markersize=5,\n",
    "    color=\"blue\",\n",
    "    linewidth=0,\n",
    "    linestyle=\"\",\n",
    ")\n",
    "# Plot constraints number (modelization).\n",
    "axis_plot_constraints_number.plot(\n",
    "    range(1000, 5001, 100),  # x\n",
    "    [\n",
    "        interpolation_constraints_number(x)[1]\n",
    "        for x in range(1000, 5001, 100)  # x\n",
    "    ],  # y\n",
    "    label=\"Nombre de contraintes modélisé\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"purple\",\n",
    "    markersize=3,\n",
    "    color=\"purple\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_constraints_number.fill_between(\n",
    "    x=range(1000, 5001, 100),  # x\n",
    "    y1=[\n",
    "        interpolation_constraints_number(x)[0]\n",
    "        for x in range(1000, 5001, 100)  # x\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        interpolation_constraints_number(x)[2]\n",
    "        for x in range(1000, 5001, 100)  # x\n",
    "    ],  # y2\n",
    "    color=\"purple\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_constraints_number.set_xlabel(\"nombre de données [#]\", fontsize=18,)\n",
    "axis_plot_constraints_number.set_ylabel(\"nombre de contraintes [#]\", fontsize=18,)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_constraints_number.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_constraints_number.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_constraints_number.savefig(\n",
    "    \"../results/etude-nombre-contraintes-1-modelisation-nombre.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c176e8",
   "metadata": {},
   "source": [
    "Estimation of constraints lower and upper limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c988b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_of_constraints_lower_limits(dataset_size: int, nb_clusters: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimation of constraints lower limit:\n",
    "    - first estimate the minimal path to define connected components with MUST_LINK.\n",
    "    - then define clusters by adding minimal number of CANNOT_LINK in order to distinguish clusters.\n",
    "    \n",
    "    Args:\n",
    "        dataset_size (int): number of data.\n",
    "        nb_clusters (int): number of clusters.\n",
    "        \n",
    "    Return:\n",
    "        int: number of constraints\n",
    "    \"\"\"\n",
    "    cluster_size: float = dataset_size/nb_clusters\n",
    "    nb_must_link: float = nb_clusters * (cluster_size-1)\n",
    "    nb_cannot_link: float = sum(\n",
    "        (nb_clusters-1-k)\n",
    "        for k in range(nb_clusters)\n",
    "    )\n",
    "    return int(nb_must_link + nb_cannot_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_of_constraints_upper_limits(dataset_size: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimation of constraints upper limit.\n",
    "    \n",
    "    Args:\n",
    "        dataset_size (int): number of data.\n",
    "        \n",
    "    Return:\n",
    "        int: number of constraints\n",
    "    \"\"\"\n",
    "    return int( (dataset_size**2 - dataset_size) / 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_of_constraints_lower_limits(1000, 10), estimation_of_constraints_upper_limits(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_of_constraints_lower_limits(5000, 50), estimation_of_constraints_upper_limits(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c38f4",
   "metadata": {},
   "source": [
    "-----\n",
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation time.\n",
    "def estimate_annotation_time(batch_size: int = 50) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "        Estimate total time to annotation in an interactive clustering methodology.\n",
    "        \n",
    "        Args:\n",
    "            dataset_size (int): The dataset size.\n",
    "        \n",
    "        Return:\n",
    "            Dict[str, float]: Total estimated annotation time in a dictionnary.\n",
    "    \"\"\"\n",
    "    # return 202 + batch_size * 7  # intercept\n",
    "    return {\n",
    "        \"min\": (7.77-0.29) * batch_size,\n",
    "        \"mean\": 7.77 * batch_size,\n",
    "        \"max\": (7.77+0.29) * batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation time.\n",
    "def estimate_computation_time(dataset_size: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "        Estimate total computation time to apply interactive clustering methodology on a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_size (int): The dataset size.\n",
    "        \n",
    "        Return:\n",
    "            Dict[str, float]: Total computation time in a dictionnary.\n",
    "    \"\"\"\n",
    "    # return -180 + 0.211 * dataset_size  # intercept\n",
    "    return {\n",
    "       \"min\": (0.160 * dataset_size + 1.43*10**(-6) * dataset_size**2),\n",
    "       \"mean\": (0.167 * dataset_size + 1.43*10**(-6) * dataset_size**2),\n",
    "       \"max\": (0.175 * dataset_size + 1.43*10**(-6) * dataset_size**2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5004446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints number.\n",
    "def estimate_constraints_number(dataset_size: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "        Estimate number of constraints requested to converge with an clustering methodology.\n",
    "        \n",
    "        Args:\n",
    "            dataset_size (int): The dataset size.\n",
    "        \n",
    "        Return:\n",
    "            Dict[str, float]: Estimated number of constraints in a dictionnary.\n",
    "    \"\"\"\n",
    "    # return 356 + 3.05 * dataset_size  # intercept\n",
    "    return {\n",
    "        \"min\": (3.15-0.016) * dataset_size,\n",
    "        \"mean\": 3.15 * dataset_size,\n",
    "        \"max\": (3.15+0.016) * dataset_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7858d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time.\n",
    "def estimate_total_time(dataset_size: int, batch_size: Optional[int] = 50, with_parallelization: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "        Estimate total time to apply interactive clustering methodology on a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_size (int): The dataset size.\n",
    "            batch_size (Optional[int]): The annotation batch size. If `None`, then use a batch size for which annotation time an computation time is equivalent. Defaults to `50`.\n",
    "            with_parallelization (bool): The option to simulate parallelization between clustering and annotation. Defaults to `False`.\n",
    "        \n",
    "        Return:\n",
    "            Dict[str, float]: Total estimated time in a dictionnary.\n",
    "    \"\"\"\n",
    "    # Estimate unitary times: total constraints number, one iteration computation, one iteration annottaion.\n",
    "    constraints_number: Dict[str, float] = estimate_constraints_number(dataset_size)\n",
    "    time_of_one_computation_batch: Dict[str, float] = estimate_computation_time(dataset_size)\n",
    "    time_of_one_computation_batch: Dict[str, float]\n",
    "    if batch_size is None:\n",
    "        batch_size = max(50, min(150, estimate_computation_time(dataset_size)[\"mean\"] / estimate_annotation_time(1)[\"mean\"]))\n",
    "    time_of_one_annotation_batch: Dict[str, float] = estimate_annotation_time(batch_size)\n",
    "    \n",
    "    # Estimate total times.\n",
    "    nb_iterations: Dict[str, float] = {key: (constraints_number[key]/batch_size) for key in constraints_number.keys()}\n",
    "    total_computation_time: Dict[str, float] = {key: (time_of_one_computation_batch[key]*nb_iterations[key]) for key in nb_iterations.keys()}\n",
    "    total_annotation_time: Dict[str, float] = {key: (time_of_one_annotation_batch[key]*nb_iterations[key]) for key in nb_iterations.keys()}\n",
    "    total_time: Dict[str, float]\n",
    "    if with_parallelization:\n",
    "        total_time = {key: max(total_annotation_time[key], total_computation_time[key]) for key in nb_iterations.keys()}\n",
    "    else:\n",
    "        total_time = {key: (total_annotation_time[key] + total_computation_time[key]) for key in nb_iterations.keys()}\n",
    "    print(\"nb_iterations\", nb_iterations)\n",
    "    return {\n",
    "        \"total-min\": total_time[\"min\"],\n",
    "        \"total\": total_time[\"mean\"],\n",
    "        \"total-max\": total_time[\"max\"],\n",
    "        \"annotation-min\": total_annotation_time[\"min\"],\n",
    "        \"annotation\": total_annotation_time[\"mean\"],\n",
    "        \"annotation-max\": total_annotation_time[\"max\"],\n",
    "        \"computation-min\": total_computation_time[\"min\"],\n",
    "        \"computation\": total_computation_time[\"mean\"],\n",
    "        \"computation-max\": total_computation_time[\"max\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display total time.\n",
    "def display_total_time(dataset_size: int, batch_size: int, with_parallelization: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Estimate total time to apply interactive clustering methodology on a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_size (int): The dataset size.\n",
    "            batch_size (int): The annotation batch size.\n",
    "            with_parallelization (bool): The option to simulate parallelization between clustering and annotation. Defaults to `False`.\n",
    "        \n",
    "        Return:\n",
    "            pd.DataFrame: Total estimated time in a DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        data={\n",
    "            key: [value]\n",
    "            for key, value in estimate_total_time(dataset_size=dataset_size, batch_size=batch_size, with_parallelization=with_parallelization).items()\n",
    "        },\n",
    "        orient=\"index\",\n",
    "        columns=[\"time [s]\"],\n",
    "    )\n",
    "    df[\"time [m]\"] = df.apply(lambda row: round(row[\"time [s]\"] / 60, 2), axis=1)\n",
    "    df[\"time [h]\"] = df.apply(lambda row: round(row[\"time [s]\"] / 60 / 60, 2), axis=1)\n",
    "    df[\"time [d]\"] = df.apply(lambda row: round(row[\"time [s]\"] / 60 / 60 / 24, 2), axis=1)\n",
    "    df[\"time [wd]\"] = df.apply(lambda row: round(row[\"time [s]\"] / 60 / 60 / 7, 2), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6941b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=50, without parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=50, with_parallelization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ecdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=50, with parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=50, with_parallelization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=150, without parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=150, with_parallelization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=150, with parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=150, with_parallelization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=150, without parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=None, with_parallelization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size=5000, batch_size=150, with parallelization\n",
    "display_total_time(dataset_size=5000, batch_size=None, with_parallelization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28d75d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a new figure.\n",
    "fig_plot_total_time: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "axis_plot_total_time = fig_plot_total_time.gca()\n",
    "\n",
    "# Set range of axis.\n",
    "axis_plot_total_time.set_xlim(xmin=-50, xmax=5050)\n",
    "axis_plot_total_time.set_ylim(ymin=-2, ymax=122)\n",
    "\n",
    "###\n",
    "### Sequential.\n",
    "###\n",
    "\n",
    "# Plot total time (batch 50).\n",
    "axis_plot_total_time.plot(\n",
    "    range(1000, 5001, 100),  # x\n",
    "    [\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=50, with_parallelization=False)[\"total\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y\n",
    "    label=\"Temps total nécessaire en annotant après le clustering par paquet de 50\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"red\",\n",
    "    markersize=3,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "axis_plot_total_time.fill_between(\n",
    "    x=range(1000, 5001, 100),  # x\n",
    "    y1=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=50, with_parallelization=False)[\"total-min\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=50, with_parallelization=False)[\"total-max\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y2\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot total time (batch 100).\n",
    "axis_plot_total_time.plot(\n",
    "    range(1000, 5001, 100),  # x\n",
    "    [\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=100, with_parallelization=False)[\"total\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y\n",
    "    label=\"Temps total nécessaire en annotant après le clustering par paquet de 100\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"orange\",\n",
    "    markersize=3,\n",
    "    color=\"orange\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "axis_plot_total_time.fill_between(\n",
    "    x=range(1000, 5001, 100),  # x\n",
    "    y1=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=100, with_parallelization=False)[\"total-min\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=100, with_parallelization=False)[\"total-max\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y2\n",
    "    color=\"orange\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot total time (batch 150).\n",
    "axis_plot_total_time.plot(\n",
    "    range(1000, 5001, 100),  # x\n",
    "    [\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=150, with_parallelization=False)[\"total\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y\n",
    "    label=\"Temps total nécessaire en annotant après le clustering par paquet de 150\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"khaki\",\n",
    "    markersize=3,\n",
    "    color=\"khaki\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "axis_plot_total_time.fill_between(\n",
    "    x=range(1000, 5001, 100),  # x\n",
    "    y1=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=150, with_parallelization=False)[\"total-min\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=150, with_parallelization=False)[\"total-max\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y2\n",
    "    color=\"khaki\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_total_time.set_xlabel(\"nombre de données [#]\", fontsize=18,)\n",
    "axis_plot_total_time.set_ylabel(\"temps [h]\", fontsize=18,)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_total_time.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_total_time.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_total_time.savefig(\n",
    "    \"../results/etude-temps-total-1-modelisation-sequentielle.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "###\n",
    "### Parallelization.\n",
    "###\n",
    "\n",
    "# Plot total time (batch optimal).\n",
    "axis_plot_total_time.plot(\n",
    "    range(1000, 5001, 100),  # x\n",
    "    [\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=None, with_parallelization=True)[\"total\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y\n",
    "    label=\"Temps total nécessaire en annotant en parallèle de l'exécution du clustering\",\n",
    "    marker=\"\",\n",
    "    markerfacecolor=\"green\",\n",
    "    markersize=3,\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "axis_plot_total_time.fill_between(\n",
    "    x=range(1000, 5001, 100),  # x\n",
    "    y1=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=None, with_parallelization=True)[\"total-min\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y1\n",
    "    y2=[\n",
    "        estimate_total_time(dataset_size=dataset_size, batch_size=None, with_parallelization=True)[\"total-max\"]/60/60\n",
    "        for dataset_size in range(1000, 5001, 100)  # x\n",
    "    ],  # y2\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Set axis name.\n",
    "axis_plot_total_time.set_xlabel(\"nombre de données [#]\", fontsize=18,)\n",
    "axis_plot_total_time.set_ylabel(\"temps [h]\", fontsize=18,)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot the legend.\n",
    "axis_plot_total_time.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "# Plot the grid.\n",
    "axis_plot_total_time.grid(True)\n",
    "    \n",
    "# Store the graph.\n",
    "fig_plot_total_time.savefig(\n",
    "    \"../results/etude-temps-total-2-modelisation-parallele.png\",\n",
    "    dpi=300,\n",
    "    transparent=True,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ac244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
