{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063673fd",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : BUSINESS RELEVANCE STUDY ====\n",
    "> ### Stage 1 : Evaluate business relevance on previous experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12982cf",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e43b6e",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at compute business relevance on interactive clustering experiments**.\n",
    "- Computations are based on previous experiments (cf. efficience study) exported in `./previous` folder.\n",
    "- Environments results are stored in their `.JSON` files in the `/experiments` folder.\n",
    "\n",
    "Then, **go to the notebook `2_Plot_some_figures.ipynb` to plot several figures according to these computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f1eb",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "The several computations are :\n",
    "- `consistency_score`: Test a TF-IDF + Linear SVM model on its own trainset with a high prediction score threshold in order to check data consistency. The consistency score is the f1-score computed. If f1-score is low (i.e. less than `0.75`), then trainset can be inconsistent : Data may be badly labeled or classes may overlap. Otherwise, f1-score should tend to 100%.\n",
    "- `fmc_description`: Use the Features Maximization Contrast m√©thod of feature selection in order to describe a cluster by its relevant linguistic patterns.\n",
    "- `llm_summarization`: Use a large language model (`openai/GPT3.5`) to summarize each cluster as a one sentence description.\n",
    "\n",
    "Some combinations of parameters are studied:\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `hier_avg-10c`: best to reach 90% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: best to reach 100% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `in_same-50` + `kmeans_COP-10c`: best to reach annotation completeness (cf. efficiency study)\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: choice of author (cf. cost study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec8386",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Any, Optional\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b310fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf441af",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. LOAD EXPERIMENTS TO STUDY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055773",
   "metadata": {},
   "source": [
    "Find all implementations to analyze and associated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of run tasks to parallelize.\n",
    "LIST_OF_TASKS: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"implementation\": implementation,  # Environment of experiment.\n",
    "        \"experiments\": [\n",
    "            exp_path\n",
    "            for exp_path in os.listdir(\"../previous/\"+implementation)\n",
    "            if \".json\" in exp_path\n",
    "        ]\n",
    "    }\n",
    "    for implementation in os.listdir(\"../previous\")\n",
    "    if (\n",
    "        os.path.isdir(\"../previous/\" + implementation)\n",
    "        and implementation in [\n",
    "            \"bank_cards_v1_-_settings_0_partial\",  # \"bank_cards_v1\" + \"settings_0_partial\" # \"bank_cards_v1\" + \"simple_prep_-_tfidf\" + \"closest-50\" + \"hier_avg-10c\", best to reach 90% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_1_sufficient\",  # \"bank_cards_v1\" + \"lemma_prep_-_tfidf\" + \"closest-50\" + \"kmeans_COP-10c\", best to reach 100% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_2_exhaustive\",  # \"bank_cards_v1\" + \"lemma_prep_-_tfidf\" + \"in_same-50\" + \"kmeans_COP-10c\", best to reach annotation completeness (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_3_favorite\",  # \"bank_cards_v1\" + \"simple_prep_-_tfidf\" + \"closest-50\" + \"kmeans_COP-10c\", choice of author (cf. cost study)\n",
    "            #\"bank_cards_v1_-_simple_prep_-_tfidf\",  # Mean.\n",
    "            #\"bank_cards_v1\",  # Mean.\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "print(\"There are\", \"`\" + str(len(LIST_OF_TASKS)) + \"`\", \"implementations to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d258b8",
   "metadata": {},
   "source": [
    "Create one folder per implementation to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in LIST_OF_TASKS:\n",
    "    \n",
    "    # If folder exists: continue.\n",
    "    if os.path.isdir(\"../experiments/\" + task[\"implementation\"]):\n",
    "        continue\n",
    "    \n",
    "    # Create folder for analyses.\n",
    "    os.mkdir(\"../experiments/\" + task[\"implementation\"])\n",
    "    \n",
    "    # Copy data.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        with open(\"../previous/\" + task[\"implementation\"] + \"/\" + experiment, \"r\") as file_previous_results_r:\n",
    "            previous_results = json.load(file_previous_results_r)\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"w\") as file_previous_results_w:\n",
    "            json.dump(\n",
    "                previous_results,\n",
    "                file_previous_results_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026de5d4",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 3. START ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1ef00",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.1. Analyze clustering consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a303184",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from consistency_score import compute_consistency_score, display_consistency_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25c816",
   "metadata": {},
   "source": [
    "Compute all consistency scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67801e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "    \n",
    "        # If folder exists: continue.\n",
    "        if os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment):\n",
    "            continue\n",
    "        \n",
    "        # Load data\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "            \n",
    "        # Compute consistency score of groundtruth.\n",
    "        groundtruth_consistency_score: float = compute_consistency_score(\n",
    "            x_train = [\n",
    "                dict_of_preprocessed_texts[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            y_train = [\n",
    "                dict_of_true_intents[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            prediction_score_threshold = 0.75,\n",
    "        )\n",
    "\n",
    "        # Compute consistency score of clustering.\n",
    "        clustering_consistency_score_evolution: Dict[str, float] = {\n",
    "            iteration: compute_consistency_score(\n",
    "                x_train = [\n",
    "                    dict_of_preprocessed_texts[text_ID]\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                y_train = [\n",
    "                    str(dict_of_clustering_results[iteration][text_ID])\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                prediction_score_threshold = 0.75,\n",
    "            )\n",
    "            for iteration in dict_of_clustering_results.keys()\n",
    "        }\n",
    "            \n",
    "        # Store results.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment, \"w\") as file_constistency_score_w:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"groundtruth\": groundtruth_consistency_score,\n",
    "                    \"evolution\": clustering_consistency_score_evolution,\n",
    "                },\n",
    "                file_constistency_score_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444285b",
   "metadata": {},
   "source": [
    "Display consistency score evolution in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fa976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_consistency_score = {\n",
    "    \"bank_cards_v1_-_settings_0_partial\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation partielle (90% de v-measure).\",\n",
    "        \"plot_color\": \"green\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-partielle.png\",\n",
    "    },  # best to reach 90% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_1_sufficient\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation suffisante (100% de v-measure).\",\n",
    "        \"plot_color\": \"blue\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-suffisante.png\",\n",
    "    },  # best to reach 100% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_2_exhaustive\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation exhaustive (toutes les contraintes).\",\n",
    "        \"plot_color\": \"red\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-exhaustive.png\",\n",
    "    },  # best to reach annotation completeness (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_3_favorite\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant notre param√©trage favori pour atteindre\\n90% de v-measure avec un co√ªt global minimal.\",\n",
    "        \"plot_color\": \"gold\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-favori.png\",\n",
    "    },  # choice of author (cf. cost study)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64ae4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"], \"\\n    (\", config_consistency_score[task[\"implementation\"]][\"graph_filename\"], \")\")\n",
    "    display_consistency_score(\n",
    "        implementation=task[\"implementation\"],\n",
    "        list_of_experiments=task[\"experiments\"],\n",
    "        list_of_iterations=[str(i).zfill(4) for i in range(50)],\n",
    "        plot_label=config_consistency_score[task[\"implementation\"]][\"plot_label\"],\n",
    "        plot_color=config_consistency_score[task[\"implementation\"]][\"plot_color\"],\n",
    "        graph_filename=config_consistency_score[task[\"implementation\"]][\"graph_filename\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4f1db",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.2. Describe clusters by their relevant linguistic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c75534",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cognitivefactory.features_maximization_metric.fmc import FeaturesMaximizationMetric\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c60f7",
   "metadata": {},
   "source": [
    "Describe clusters with FMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ff5ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "        \n",
    "        # Intialize FMC descriptions if need.\n",
    "        if not os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment):\n",
    "            # Define list of iteration to analyze.\n",
    "            list_of_iteration: List[str] = [\n",
    "                i\n",
    "                for i in dict_of_clustering_results.keys()\n",
    "                if (\n",
    "                    int(i) % 5 == 0\n",
    "                    or i == max(dict_of_clustering_results.keys())\n",
    "                )\n",
    "            ]\n",
    "            # Define initial FMC descriptions for each cluster of each iteration to analyze.\n",
    "            initial_fmc_descriptions: Dict[str, Dict[str, Optional[List[str]]]] = {\n",
    "                iteration: {\n",
    "                    str(cluster_id): None  # need to force cluster_id to str for serialization\n",
    "                    for cluster_id in sorted(set(dict_of_clustering_results[iteration].values()))\n",
    "                }\n",
    "                for iteration in list_of_iteration\n",
    "            }\n",
    "            # Store initial FMC descriptions.\n",
    "            with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"w\") as file_initial_fmc_description_w:\n",
    "                json.dump(initial_fmc_descriptions, file_initial_fmc_description_w)\n",
    "        \n",
    "        # Load FMC descriptions already done.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"r\") as file_fmc_description_r:\n",
    "            descriptions: Dict[str, Dict[str, Optional[str]]] = json.load(file_fmc_description_r)\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in descriptions.keys():\n",
    "            print(\"    \", \"    \", \"iteration:\", iteration)\n",
    "            \n",
    "            # Define vectorizer.\n",
    "            vectorizer = TfidfVectorizer(min_df=0, ngram_range=(1, 3), analyzer=\"word\", sublinear_tf=True)\n",
    "            matrix_of_vectors: csr_matrix = vectorizer.fit_transform(\n",
    "                [\n",
    "                    str(dict_of_preprocessed_texts[text_id])\n",
    "                    for text_id in dict_of_preprocessed_texts.keys()\n",
    "                ]\n",
    "            )\n",
    "            list_of_possible_vectors_features: List[str] = list(vectorizer.get_feature_names_out())\n",
    "\n",
    "            # Define FMC modelization.\n",
    "            fmc_computer: FeaturesMaximizationMetric = FeaturesMaximizationMetric(\n",
    "                data_vectors=matrix_of_vectors,\n",
    "                data_classes=[dict_of_clustering_results[iteration][text_id] for text_id in dict_of_preprocessed_texts.keys()],\n",
    "                list_of_possible_features=list_of_possible_vectors_features,\n",
    "                amplification_factor=1,\n",
    "            )\n",
    "            \n",
    "            # For each cluster\n",
    "            for cluster_id in descriptions[iteration].keys():\n",
    "                print(\"    \", \"    \", \"    \", cluster_id, end=\": \")\n",
    "                \n",
    "                # If already done: continue\n",
    "                if descriptions[iteration][str(cluster_id)] is not None:\n",
    "                    print(\"(already done)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get FMC description of cluster.\n",
    "                cluster_description: List[str] = [\n",
    "                    linguistic_pattern\n",
    "                    for linguistic_pattern in fmc_computer.get_most_active_features_by_a_classe(\n",
    "                        classe=int(cluster_id),\n",
    "                        activation_only=True,\n",
    "                        sort_by='fmeasure',  # \"contrast\"\n",
    "                        max_number=50,\n",
    "                    )\n",
    "                    if fmc_computer.get_most_activated_classes_by_a_feature(linguistic_pattern) == [int(cluster_id)]\n",
    "                ]\n",
    "                print(cluster_description)\n",
    "                \n",
    "                # Store updated descriptions.\n",
    "                descriptions[iteration][str(cluster_id)] = cluster_description\n",
    "                with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"w\") as file_fmc_description_w:\n",
    "                    json.dump(descriptions, file_fmc_description_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d94476",
   "metadata": {},
   "source": [
    "Export in XLSX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_token_in_text(text: str, dict_of_translation: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "        In a text, replace all token according to a translation dictionnary.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to translate.\n",
    "            dict_of_translation (Dict[str, str]): The list of translations.\n",
    "        \n",
    "        Return:\n",
    "            str: The translated text.\n",
    "    \"\"\"\n",
    "    for token, translation in dict_of_translation.items():\n",
    "        text = text.replace(token, translation)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349da6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        #if os.path.isfile(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment.split(\".\")[0] + \".xlsx\"):\n",
    "        #    print(\"    \", \"experiment:\", experiment, \"--> SKIP: export already done\")\n",
    "        #    continue\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        list_of_possible_true_labels: List[str] = sorted(set(dict_of_true_intents.values()))\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "\n",
    "        # Load FMC descriptions.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"r\") as file_fmc_description_r:\n",
    "            descriptions: Dict[str, Dict[str, Optional[str]]] = json.load(file_fmc_description_r)\n",
    "        \n",
    "        # Initialize list of exports.\n",
    "        df_exports: Dict[str, pd.DataFrame] = {}\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in descriptions.keys():\n",
    "            \n",
    "            # Initialize export data.\n",
    "            df_exports[iteration] = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"fmc_description\", \"cluster_emphasized\", \"confusion\",\n",
    "                    \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] FMC description relevant?\", \"[Q] FMC description main topic?\",\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # For each cluster.\n",
    "            for cluster_id in descriptions[iteration].keys():\n",
    "                \n",
    "                # Estimate confusion.\n",
    "                list_of_present_true_labels: List[str] = sorted(set(\n",
    "                    dict_of_true_intents[text_id]\n",
    "                    for text_id in dict_of_texts.keys()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ))\n",
    "                \n",
    "                # Update export data.\n",
    "                data_summary: Dict[str, Any] = {\n",
    "                    \"implementation_id\": task[\"implementation\"],\n",
    "                    \"experiment_id\": experiment.split(\".\")[0],\n",
    "                    \"iteration_id\": iteration,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"cluster\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}\".format(text)\n",
    "                        for text_id, text in dict_of_texts.items()\n",
    "                        if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                    ]),\n",
    "                    \"cluster_emphasized\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}\".format(\n",
    "                            translate_token_in_text(\n",
    "                                text=text,\n",
    "                                dict_of_translation={\n",
    "                                    token: \"[{0}]\".format(token.upper())\n",
    "                                    for token in descriptions[iteration][cluster_id]\n",
    "                                }\n",
    "                            )\n",
    "                        )\n",
    "                        for text_id, text in dict_of_preprocessed_texts.items()\n",
    "                        if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                    ]),\n",
    "                    \"fmc_description\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}\".format(pattern)\n",
    "                        for pattern in descriptions[iteration][cluster_id]\n",
    "                    ]),\n",
    "                    \"confusion\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}: {1}\".format(\n",
    "                            true_label,\n",
    "                            len([\n",
    "                                text_id\n",
    "                                for text_id in dict_of_texts.keys()\n",
    "                                if (\n",
    "                                    dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                                    and dict_of_true_intents[text_id] == true_label\n",
    "                                )\n",
    "                            ])\n",
    "                        )\n",
    "                        for true_label in list_of_present_true_labels\n",
    "                    ]),\n",
    "                    \"[Q] cluster well-designed?\": \"\",\n",
    "                    \"[Q] cluster main topic?\": \"\",\n",
    "                    \"[Q] FMC description relevant?\": \"\",\n",
    "                    \"[Q] FMC description main topic?\": \"\",\n",
    "                }\n",
    "                df_exports[iteration] = df_exports[iteration].append(\n",
    "                    data_summary,\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "        # Export summaries.\n",
    "        with pd.ExcelWriter(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment.split(\".\")[0] + \".xlsx\") as writer:\n",
    "            \n",
    "            # Export all iterations, one iteration per sheet.\n",
    "            for iteration_key, df_export in df_exports.items():\n",
    "                # Export data.\n",
    "                df_export.to_excel(\n",
    "                    excel_writer=writer,\n",
    "                    sheet_name=iteration_key,\n",
    "                    index=False,\n",
    "                    #engine=\"openpyxl\",\n",
    "                )\n",
    "                # Format data.\n",
    "                workbook = writer.book\n",
    "                format_header = workbook.add_format({\"text_wrap\": True, \"bold\": True})\n",
    "                format_cluster = workbook.add_format({\"text_wrap\": True})\n",
    "                format_description = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"font_color\": \"orange\"})\n",
    "                format_confusion = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "                format_cluster_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#97D2D4\"})\n",
    "                format_fmc_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#EBDB5E\"})\n",
    "                worksheet = writer.sheets[iteration_key]\n",
    "                worksheet.set_row(row=0, height=None, cell_format=format_header)\n",
    "                worksheet.set_column(first_col=0, last_col=3, width=10, cell_format=None)\n",
    "                worksheet.set_column(first_col=4, last_col=4, width=70, cell_format=format_cluster)\n",
    "                worksheet.set_column(first_col=5, last_col=5, width=25, cell_format=format_description)\n",
    "                worksheet.set_column(first_col=6, last_col=6, width=70, cell_format=format_cluster)\n",
    "                worksheet.set_column(first_col=7, last_col=7, width=25, cell_format=format_confusion)\n",
    "                worksheet.set_column(first_col=8, last_col=9, width=20, cell_format=format_cluster_analysis),\n",
    "                worksheet.set_column(first_col=10, last_col=11, width=20, cell_format=format_fmc_analysis),\n",
    "                for i in range(len(df_export)):\n",
    "                    worksheet.set_row(row=1+i, height=150, cell_format=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98862f89",
   "metadata": {},
   "source": [
    "Manually analyze each clusters and their FMC descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19059c",
   "metadata": {},
   "source": [
    "1. **Questions for annotators/reviewers**:\n",
    "    - **on cluster analysis**:\n",
    "        - `is this cluster well-designed / consistent / suited for training ?` (_all well-designed clusters are accepted, even if its not the groundtruth_).\n",
    "        - `if so, what is the single main topic of this cluster ?`\n",
    "    - **on FMC description analysis**:\n",
    "        - `does these linguistics patterns represent a single main topic ?`\n",
    "        - `if so, what is the single main topic of this description ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db44c62",
   "metadata": {},
   "source": [
    "2. **Notations scale**:\n",
    "    - `not_exploitable`:\n",
    "        - definition is not clear: the cluster has no main topic or too many topics.\n",
    "        - so content is not trainable: the cluster is too small or too big.\n",
    "        - $\\Rightarrow$ it's absolutely not usable.\"\n",
    "    - `partially_exploitable` :\n",
    "        - definition can be deduced: the cluster has several topics ideas but lacks relevance.\n",
    "        - so content is not trainable: the cluster is not usable as is.\n",
    "        - $\\Rightarrow$ it gives some clues to manually define and create an exploitable cluster.\"\n",
    "    - `exploitable`:\n",
    "        - definition is clear: the cluster has one identifiable main topic.\n",
    "        - content is  trainable: the cluster is consistent, has enough data and few intruders.\n",
    "        - $\\Rightarrow$ the cluster can be used with little manual editing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd98c8",
   "metadata": {},
   "source": [
    "/!\\ Annotation are expected in `fmc_description___XXXX_checked.xlsx` file /!\\\n",
    "- **cluster analysis** are annotated in columns `[Q] cluster well-designed?` and `[Q] cluster main topic?`.\n",
    "- **FMC description analysis** are annotated in columns `[Q] FMC description relevant?` and `[Q] FMC description main topic?`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a081663",
   "metadata": {},
   "source": [
    "Conclusions of this manual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721758ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # Check if annotations are performed.\n",
    "    if task[\"implementation\"] != \"bank_cards_v1_-_settings_3_favorite\":\n",
    "        print(\"--> SKIP: task not annotated\")\n",
    "        continue\n",
    "        \n",
    "    ###\n",
    "    ### GET ANNOTATIONS\n",
    "    ###\n",
    "    print(\"    \", \"Get annotations\")\n",
    "        \n",
    "    # Initialize annotations loading.\n",
    "    df_fmc_annotations: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "\n",
    "        # Load clustering summaries.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_fmc_description_r:\n",
    "            fmc_descriptions: Dict[str, Dict[str, Optional[str]]] = json.load(file_fmc_description_r)\n",
    "        \n",
    "        # Load annotations.\n",
    "        df_fmc_annotations[experiment] = {\n",
    "            iteration: pd.read_excel(\n",
    "                io=\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment.split(\".\")[0] + \"_checked.xlsx\",\n",
    "                sheet_name=iteration,\n",
    "                engine=\"openpyxl\",\n",
    "            )\n",
    "            for iteration in fmc_descriptions.keys()\n",
    "        }\n",
    "    \n",
    "    # Group by relevance (annotation label).\n",
    "    df_fmc_annotations_by_relevance: Dict[str, pd.DataFrame] = {\n",
    "        \"exploitable\": pd.DataFrame(\n",
    "            columns=[\n",
    "                \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"fmc_description\", \"cluster_emphasized\", \"confusion\",\n",
    "                \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] FMC description relevant?\", \"[Q] FMC description main topic?\",\n",
    "            ]\n",
    "        ),\n",
    "        \"partially_exploitable\": pd.DataFrame(\n",
    "            columns=[\n",
    "                \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"fmc_description\", \"cluster_emphasized\", \"confusion\",\n",
    "                \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] FMC description relevant?\", \"[Q] FMC description main topic?\",\n",
    "            ]\n",
    "        ),\n",
    "        \"not_exploitable\": pd.DataFrame(\n",
    "            columns=[\n",
    "                \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"fmc_description\", \"cluster_emphasized\", \"confusion\",\n",
    "                \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] FMC description relevant?\", \"[Q] FMC description main topic?\",\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    "    for experiment in df_fmc_annotations.keys():\n",
    "        for iteration in df_fmc_annotations[experiment].keys():\n",
    "            df_fmc_annotations_by_relevance[\"exploitable\"] = df_fmc_annotations_by_relevance[\"exploitable\"].append(\n",
    "                df_fmc_annotations[experiment][iteration][df_fmc_annotations[experiment][iteration][\"[Q] cluster well-designed?\"]==\"exploitable\"],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            df_fmc_annotations_by_relevance[\"partially_exploitable\"] = df_fmc_annotations_by_relevance[\"partially_exploitable\"].append(\n",
    "                df_fmc_annotations[experiment][iteration][df_fmc_annotations[experiment][iteration][\"[Q] cluster well-designed?\"]==\"partially_exploitable\"],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            df_fmc_annotations_by_relevance[\"not_exploitable\"] = df_fmc_annotations_by_relevance[\"not_exploitable\"].append(\n",
    "                df_fmc_annotations[experiment][iteration][df_fmc_annotations[experiment][iteration][\"[Q] cluster well-designed?\"]==\"not_exploitable\"],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "    # Compute number of FMC pattern identified and print Mean for this relavance category.\n",
    "    for key in df_fmc_annotations_by_relevance.keys():\n",
    "        df_fmc_annotations_by_relevance[key][\"len(fmc_description)\"] = df_fmc_annotations_by_relevance[key].apply(\n",
    "            lambda row: len([\n",
    "                pattern.split(\"- \")[1]\n",
    "                for pattern in row[\"fmc_description\"].split(\"_x000D_\\n\")\n",
    "                if len(pattern.split(\"- \"))==2\n",
    "            ]),\n",
    "            axis=1,\n",
    "        )\n",
    "        print(\n",
    "            \"    \", \"FMC description size for '{0}':\".format(key),\n",
    "            \"Mean: {0:.1f}\".format(df_fmc_annotations_by_relevance[key][\"len(fmc_description)\"].mean()),\n",
    "            \"Min: {0:.1f}\".format(min(df_fmc_annotations_by_relevance[key][\"len(fmc_description)\"])),\n",
    "            \"Max: {0:.1f}\".format(max(df_fmc_annotations_by_relevance[key][\"len(fmc_description)\"])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f2f9a",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.3. Summarize clusters by a large language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b8908",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats as scipystats\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aad2f",
   "metadata": {},
   "source": [
    "Load credentials.\n",
    "> Need a file `credentials.py` in projet home (`../..`)\n",
    "> with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import credentials  # Need a file `credentials.py` in projet home (`..`) with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys\n",
    "openai.api_key = credentials.OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42ea6d",
   "metadata": {},
   "source": [
    "Define large language model to call and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to use.\n",
    "OPENAI_MODEL: str = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e702ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts to use\n",
    "prompt_context: str = \"Tu es un expert des secteurs banque, assurance et finance.\"\n",
    "prompt_task: str = \"R√©sume-moi en une phrase la th√©matique trait√©e dans les textes suivants\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d13d6",
   "metadata": {},
   "source": [
    "Summarize clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae99487",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRY: int = 5  # retry when timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990bd89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "        \n",
    "        # Intialize clustering summaries if need.\n",
    "        if not os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment):\n",
    "            # Define list of iteration to analyze.\n",
    "            list_of_iteration: List[str] = [\n",
    "                i\n",
    "                for i in dict_of_clustering_results.keys()\n",
    "                if (\n",
    "                    int(i) % 5 == 0\n",
    "                    or i == max(dict_of_clustering_results.keys())\n",
    "                )\n",
    "            ]\n",
    "            # Define initial clustering summaries for each cluster of each iteration to analyze.\n",
    "            initial_clustering_summaries: Dict[str, Dict[str, Optional[str]]] = {\n",
    "                iteration: {\n",
    "                    str(cluster_id): None  # need to force cluster_id to str for serialization\n",
    "                    for cluster_id in sorted(set(dict_of_clustering_results[iteration].values()))\n",
    "                }\n",
    "                for iteration in list_of_iteration\n",
    "            }\n",
    "            # Store initial clustering summaries.\n",
    "            with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"w\") as file_initial_llm_summaries_w:\n",
    "                json.dump(initial_clustering_summaries, file_initial_llm_summaries_w)\n",
    "\n",
    "        # Load clustering summaries already done.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_llm_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_llm_summary_r)\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in summaries.keys():\n",
    "            print(\"\\n    \", \"    \", \"iteration:\", iteration)\n",
    "            \n",
    "            # For each cluster\n",
    "            for cluster_id in summaries[iteration].keys():\n",
    "                print(\"    \", \"    \", \"    \", cluster_id, end=\": \")\n",
    "                \n",
    "                # If already done: continue\n",
    "                if summaries[iteration][str(cluster_id)] is not None:\n",
    "                    print(\"(already done)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get texts of this cluster.\n",
    "                cluster = [\n",
    "                    text\n",
    "                    for text_id, text in dict_of_texts.items()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ]\n",
    "                print(\"(cluster length: {0})\".format(len(cluster)), end=\" ; \")\n",
    "                \n",
    "                # Call the LLM to summarize the document (use loop to by-pass timeout).\n",
    "                it: int = 0\n",
    "                last_err: Exception = None\n",
    "                while it<MAX_RETRY:\n",
    "                    time.sleep(21)  # To avoid \"Rate limit reached\" => \"default-gpt-3.5-turbo\" limited at 3 request per minute.\n",
    "                    try : \n",
    "                        it += 1\n",
    "                        # Create a chat completion.\n",
    "                        chat_answers = openai.ChatCompletion.create(\n",
    "                            model=OPENAI_MODEL,\n",
    "                            messages=[\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": \"{context}\".format(\n",
    "                                        context=prompt_context,\n",
    "                                    )\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": \"{task} :\\n\\n{data}\".format(\n",
    "                                        task=prompt_task,\n",
    "                                        data=\"- \" + \"\\n- \".join(cluster),\n",
    "                                    )\n",
    "                                }\n",
    "                            ]\n",
    "                        )\n",
    "                        break\n",
    "                    # except OSError :\n",
    "                    except Exception as err:\n",
    "                        last_err = err\n",
    "                        continue\n",
    "                \n",
    "                # If error: continue...\n",
    "                if it==MAX_RETRY:\n",
    "                    print(last_err)\n",
    "                    continue\n",
    "                \n",
    "                # Get summary from answer.\n",
    "                cluster_summary: str = chat_answers.choices[0].message.content\n",
    "                print(cluster_summary[:100], \"[...]\")\n",
    "                \n",
    "                # Store updated summaries.\n",
    "                summaries[iteration][str(cluster_id)] = cluster_summary\n",
    "                with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"w\") as file_llm_summary_w:\n",
    "                    json.dump(summaries, file_llm_summary_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b18f6e",
   "metadata": {},
   "source": [
    "Export in XLSX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33c805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        #if os.path.isfile(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment.split(\".\")[0] + \".xlsx\"):\n",
    "        #    print(\"    \", \"experiment:\", experiment, \"--> SKIP: export already done\")\n",
    "        #    continue\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        list_of_possible_true_labels: List[str] = sorted(set(dict_of_true_intents.values()))\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "\n",
    "        # Load clustering summaries.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_llm_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_llm_summary_r)\n",
    "        \n",
    "        # Initialize list of exports.\n",
    "        df_exports: Dict[str, pd.DataFrame] = {}\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in summaries.keys():\n",
    "            \n",
    "            # Initialize export data.\n",
    "            df_exports[iteration] = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"summary\", \"confusion\",\n",
    "                    \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] summary relevant?\", \"[Q] summary main topic?\",\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # For each cluster.\n",
    "            for cluster_id in summaries[iteration].keys():\n",
    "                \n",
    "                # Estimate confusion.\n",
    "                list_of_present_true_labels: List[str] = sorted(set(\n",
    "                    dict_of_true_intents[text_id]\n",
    "                    for text_id in dict_of_texts.keys()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ))\n",
    "                \n",
    "                # Update export data.\n",
    "                data_summary: Dict[str, Any] = {\n",
    "                    \"implementation_id\": task[\"implementation\"],\n",
    "                    \"experiment_id\": experiment.split(\".\")[0],\n",
    "                    \"iteration_id\": iteration,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"cluster\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}\".format(text)\n",
    "                        for text_id, text in dict_of_texts.items()\n",
    "                        if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                    ]),\n",
    "                    \"summary\": summaries[iteration][cluster_id],\n",
    "                    \"confusion\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}: {1}\".format(\n",
    "                            true_label,\n",
    "                            len([\n",
    "                                text_id\n",
    "                                for text_id in dict_of_texts.keys()\n",
    "                                if (\n",
    "                                    dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                                    and dict_of_true_intents[text_id] == true_label\n",
    "                                )\n",
    "                            ])\n",
    "                        )\n",
    "                        for true_label in list_of_present_true_labels\n",
    "                    ]),\n",
    "                    \"[Q] cluster well-designed?\": \"\",\n",
    "                    \"[Q] cluster main topic?\": \"\",\n",
    "                    \"[Q] summary relevant?\": \"\",\n",
    "                    \"[Q] summary main topic?\": \"\",\n",
    "                }\n",
    "                df_exports[iteration] = df_exports[iteration].append(\n",
    "                    data_summary,\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "        # Export summaries.\n",
    "        with pd.ExcelWriter(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment.split(\".\")[0] + \".xlsx\") as writer:\n",
    "            \n",
    "            # Export all iterations, one iteration per sheet.\n",
    "            for iteration_key, df_export in df_exports.items():\n",
    "                # Export data.\n",
    "                df_export.to_excel(\n",
    "                    excel_writer=writer,\n",
    "                    sheet_name=iteration_key,\n",
    "                    index=False,\n",
    "                    #engine=\"openpyxl\",\n",
    "                )\n",
    "                # Format data.\n",
    "                workbook = writer.book\n",
    "                format_header = workbook.add_format({\"text_wrap\": True, \"bold\": True})\n",
    "                format_cluster = workbook.add_format({\"text_wrap\": True})\n",
    "                format_summary = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"font_color\": \"blue\"})\n",
    "                format_confusion = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "                format_cluster_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#97D2D4\"})\n",
    "                format_llm_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#EBDB5E\"})\n",
    "                worksheet = writer.sheets[iteration_key]\n",
    "                worksheet.set_row(row=0, height=None, cell_format=format_header)\n",
    "                worksheet.set_column(first_col=0, last_col=3, width=10, cell_format=None)\n",
    "                worksheet.set_column(first_col=4, last_col=4, width=70, cell_format=format_cluster)\n",
    "                worksheet.set_column(first_col=5, last_col=5, width=40, cell_format=format_summary)\n",
    "                worksheet.set_column(first_col=6, last_col=6, width=25, cell_format=format_confusion)\n",
    "                worksheet.set_column(first_col=7, last_col=8, width=20, cell_format=format_cluster_analysis),\n",
    "                worksheet.set_column(first_col=9, last_col=10, width=20, cell_format=format_llm_analysis),\n",
    "                for i in range(len(df_export)):\n",
    "                    worksheet.set_row(row=1+i, height=150, cell_format=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd677d5",
   "metadata": {},
   "source": [
    "Manually analyze each clusters and their summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da211906",
   "metadata": {},
   "source": [
    "1. **Questions for annotators/reviewers**:\n",
    "    - **on cluster analysis**:\n",
    "        - `is this cluster well-designed / consistent / suited for training ?` (_all well-designed clusters are accepted, even if its not the groundtruth_).\n",
    "        - `if so, what is the single main topic of this cluster ?`\n",
    "    - **on summary analysis**:\n",
    "        - `does this summary represent a single main topic ?`\n",
    "        - `if so, what is the single main topic of this summary ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fb718",
   "metadata": {},
   "source": [
    "2. **Notations scale**:\n",
    "    - `not_exploitable`:\n",
    "        - definition is not clear: the cluster has no main topic or too many topics.\n",
    "        - so content is not trainable: the cluster is too small or too big.\n",
    "        - $\\Rightarrow$ it's absolutely not usable.\"\n",
    "    - `partially_exploitable` :\n",
    "        - definition can be deduced: the cluster has several topics ideas but lacks relevance.\n",
    "        - so content is not trainable: the cluster is not usable as is.\n",
    "        - $\\Rightarrow$ it gives some clues to manually define and create an exploitable cluster.\"\n",
    "    - `exploitable`:\n",
    "        - definition is clear: the cluster has one identifiable main topic.\n",
    "        - content is  trainable: the cluster is consistent, has enough data and few intruders.\n",
    "        - $\\Rightarrow$ the cluster can be used with little manual editing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b65e1f",
   "metadata": {},
   "source": [
    "/!\\ Annotation are expected in `llm_summary___XXXX_checked.xlsx` file /!\\\n",
    "- **cluster analysis** are annotated in columns `[Q] cluster well-designed?` and `[Q] cluster main topic?`.\n",
    "- **summary analysis** are annotated in columns `[Q] summary relevant?` and `[Q] summary main topic?`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb766fb",
   "metadata": {},
   "source": [
    "Conclusions of this manual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ac9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change and create a python file for llm summary relevance display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ab002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plot_of_evolution_per_iteration_to_graph(\n",
    "    axis,\n",
    "    list_of_x: List[str],\n",
    "    dict_of_y: Dict[str, float],\n",
    "    dict_of_y_err: Optional[Dict[str, float]] = None,\n",
    "    label: str = \"\",\n",
    "    marker: str = \"\",\n",
    "    markersize: int = 5,\n",
    "    color: str = \"black\",\n",
    "    linewidth: int = 2,\n",
    "    linestyle: str = \"-\",\n",
    "    alpha: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a plot to an axis of a graph.\n",
    "    \n",
    "    Args:\n",
    "        axis (): TODO.\n",
    "        list_of_x (List[str]): TODO.\n",
    "        dict_of_y (Dict[str, float]): TODO.\n",
    "        dict_of_y_err (Optional[Dict[str, float]]): TODO. Defaults to `None`.\n",
    "        label (str): TODO. Defaults to `\"\"`.\n",
    "        marker (str): TODO. Defaults to `\"\"`.\n",
    "        markersize (int): TODO. Defaults to `5`.\n",
    "        color (str): TODO. Defaults to `\"black\"`.\n",
    "        linewidth (int): TODO. Defaults to `2`.\n",
    "        linestyle (str): TODO. Defaults to `\"-\"`.\n",
    "        alpha (float): TODO. Defaults to `0.2`.\n",
    "    \"\"\"\n",
    "    # Add curve.\n",
    "    axis.plot(\n",
    "        [float(x) for x in list_of_x],  # x\n",
    "        [dict_of_y[x] for x in list_of_x],  # y\n",
    "        label=label,\n",
    "        marker=marker,\n",
    "        markerfacecolor=color,\n",
    "        markersize=markersize,\n",
    "        color=color,\n",
    "        linewidth=linewidth,\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    # Add curve error bars.\n",
    "    if dict_of_y_err is not None:\n",
    "        axis.fill_between(\n",
    "            [float(x) for x in list_of_x],  # x\n",
    "            y1=[(dict_of_y[x] - dict_of_y_err[x]) for x in list_of_x],  # y1\n",
    "            y2=[(dict_of_y[x] + dict_of_y_err[x]) for x in list_of_x],  # y2\n",
    "            color=color,\n",
    "            alpha=alpha,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # Check if annotations are performed.\n",
    "    if task[\"implementation\"] != \"bank_cards_v1_-_settings_3_favorite\":\n",
    "        print(\"--> SKIP: task not annotated\")\n",
    "        continue\n",
    "        \n",
    "    ###\n",
    "    ### GET ANNOTATIONS\n",
    "    ###\n",
    "    print(\"    \", \"Get annotations\")\n",
    "        \n",
    "    # Initialize annotations loading.\n",
    "    df_llm_annotations: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "\n",
    "        # Load clustering summaries.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_llm_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_llm_summary_r)\n",
    "        \n",
    "        # Load annotations.\n",
    "        df_llm_annotations[experiment] = {\n",
    "            iteration: pd.read_excel(\n",
    "                io=\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment.split(\".\")[0] + \"_checked.xlsx\",\n",
    "                sheet_name=iteration,\n",
    "                engine=\"openpyxl\",\n",
    "            )\n",
    "            for iteration in summaries.keys()\n",
    "        }\n",
    "        \n",
    "    # Get list of iterations to analyze.\n",
    "    max_iteration: str = max(set(\n",
    "        iteration_2\n",
    "        for experiment_2 in df_llm_annotations.keys()\n",
    "        for iteration_2 in df_llm_annotations[experiment_2].keys()\n",
    "    ))\n",
    "    list_of_iterations: List[str] = [\n",
    "        str(iteration_3).zfill(4)\n",
    "        for iteration_3 in range(0, 51, 5)\n",
    "    ]\n",
    "        \n",
    "    ###\n",
    "    ### GET PERFORMANCE\n",
    "    ###\n",
    "    print(\"    \", \"Get performance\")\n",
    "        \n",
    "    # Initialize storage of experiment performances for all iterations.\n",
    "    dict_of_performances_evolution_per_iteration: Dict[str, List[float]] = {\n",
    "        iter_perf: []\n",
    "        for iter_perf in list_of_iterations\n",
    "    }\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment_perf in task[\"experiments\"]:\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment_perf, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_clustering_performances: Dict[str, Dict[str, float]] = experiment_data[\"dict_of_clustering_performances\"]\n",
    "\n",
    "        # For each requested iteration...\n",
    "        for iter_perf in list_of_iterations:\n",
    "\n",
    "            # Append the clustering performance for the current experiment and for this iteration.\n",
    "            if iter_perf in dict_of_clustering_performances.keys():\n",
    "                dict_of_performances_evolution_per_iteration[iter_perf].append(\n",
    "                    dict_of_clustering_performances[iter_perf][\"v_measure\"]\n",
    "                )\n",
    "            # If iteration isn't reached by this experiment, duplicate the last known results.\n",
    "            # Most of the time: the experiment has reached annotation completeness and there is no more iteration because clustering is \"perfect\" (v-measure==1.0).\n",
    "            else:\n",
    "                dict_of_performances_evolution_per_iteration[iter_perf].append(1.0)\n",
    "                \n",
    "    # Compute mean of performance evolution.\n",
    "    dict_of_performances_evolution_per_iteration_MEAN: Dict[str, Dict[str, float]] = {\n",
    "        iteration_0m: np.mean(dict_of_performances_evolution_per_iteration[iteration_0m])\n",
    "        for iteration_0m in dict_of_performances_evolution_per_iteration.keys()\n",
    "    }\n",
    "        \n",
    "    # Compute sem of performance evolution.\n",
    "    dict_of_performances_evolution_per_iteration_SEM: Dict[str, Dict[str, float]] = {\n",
    "        iteration_0s: scipystats.sem(dict_of_performances_evolution_per_iteration[iteration_0s])\n",
    "        for iteration_0s in dict_of_performances_evolution_per_iteration.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### ANALYZE CLUSTERING ANNOTATIONS\n",
    "    ###\n",
    "    print(\"    \", \"Analyze clustering annotation\")\n",
    "        \n",
    "    # Analyze clustering annotations.\n",
    "    dict_of_clustering_relevance: Dict[str, Dict[str, List[float]]] = {}\n",
    "    for iteration_4 in list_of_iterations:\n",
    "        dict_of_clustering_relevance[iteration_4] = {\n",
    "            \"not_exploitable\": [],\n",
    "            \"partially_exploitable\": [],\n",
    "            \"exploitable\": [],\n",
    "        }\n",
    "        # For all experiment...\n",
    "        for experiment_4 in df_llm_annotations.keys():\n",
    "            # Case of iteration exist in this experiment.\n",
    "            if iteration_4 in df_llm_annotations[experiment_4].keys():\n",
    "                # Count ratio of not exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"not_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"not_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of partially exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"partially_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"partially_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "            # Case of iteration doesn't exist in this experiment, so take the last existing iteration.\n",
    "            else:\n",
    "                # Define the last existing iteration.\n",
    "                max_experiment_iteration_4: str = max(df_llm_annotations[experiment_4].keys())\n",
    "                # Count ratio of not exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"not_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"not_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][max_experiment_iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][max_experiment_iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of partially exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"partially_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"partially_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][max_experiment_iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][max_experiment_iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of exploitable clusters.\n",
    "                dict_of_clustering_relevance[iteration_4][\"exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_4][max_experiment_iteration_4][\"[Q] cluster well-designed?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_4][max_experiment_iteration_4][\"cluster_id\"])\n",
    "                )\n",
    "                \n",
    "    # Compute mean of clustering relevance.\n",
    "    dict_of_clustering_relevance_MEAN: Dict[str, Dict[str, float]] = {\n",
    "        iteration_4m: {\n",
    "            level: np.mean(dict_of_clustering_relevance[iteration_4m][level])\n",
    "            for level in dict_of_clustering_relevance[iteration_4m].keys()\n",
    "        }\n",
    "        for iteration_4m in dict_of_clustering_relevance.keys()\n",
    "    }\n",
    "        \n",
    "    # Compute sem of clustering relevance.\n",
    "    dict_of_clustering_relevance_SEM: Dict[str, Dict[str, float]] = {\n",
    "        iteration_4s: {\n",
    "            level: scipystats.sem(dict_of_clustering_relevance[iteration_4s][level])\n",
    "            for level in dict_of_clustering_relevance[iteration_4s].keys()\n",
    "        }\n",
    "        for iteration_4s in dict_of_clustering_relevance.keys()\n",
    "    }\n",
    "    \n",
    "    # Create a new figure.\n",
    "    fig_plot_clustering_relevance: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "    axis_plot_clustering_relevance = fig_plot_clustering_relevance.gca()\n",
    "\n",
    "    # Set range of axis.\n",
    "    axis_plot_clustering_relevance.set_xlim(xmin=-0.5, xmax=int(max(list_of_iterations))+0.5)\n",
    "    axis_plot_clustering_relevance.set_ylim(ymin=-0.01, ymax=1.01)\n",
    "    \n",
    "    # Plot not_exploitable clustering relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_clustering_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_clustering_relevance_MEAN[iter_plot][\"not_exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_clustering_relevance_SEM[iter_plot][\"not_exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_SEM.keys()\n",
    "        },\n",
    "        label=\"Non exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"red\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    \n",
    "    # Plot partially_exploitable clustering relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_clustering_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_clustering_relevance_MEAN[iter_plot][\"partially_exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_clustering_relevance_SEM[iter_plot][\"partially_exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_SEM.keys()\n",
    "        },\n",
    "        label=\"Partiellement exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    \n",
    "    # Plot exploitable clustering relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_clustering_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_clustering_relevance_MEAN[iter_plot][\"exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_clustering_relevance_SEM[iter_plot][\"exploitable\"]\n",
    "            for iter_plot in dict_of_clustering_relevance_SEM.keys()\n",
    "        },\n",
    "        label=\"Exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"green\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    # Set axis name.\n",
    "    axis_plot_clustering_relevance.set_xlabel(\"it√©ration [#]\", fontsize=18,)\n",
    "    axis_plot_clustering_relevance.set_ylabel(\"ratio de clusters [%]\", fontsize=18,)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    # Plot the legend.\n",
    "    axis_plot_clustering_relevance.legend(fontsize=15, loc=\"lower right\",)\n",
    "\n",
    "    # Plot the grid.\n",
    "    axis_plot_clustering_relevance.grid(True)\n",
    "\n",
    "    # Store the graph.\n",
    "    fig_plot_clustering_relevance.savefig(\n",
    "        \"../results/etude-pertinence-llm-check-clustering-annotation-favori.png\",\n",
    "        dpi=300,\n",
    "        transparent=True,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### ANALYSE SUMMARY ANNOTATIONS\n",
    "    ###\n",
    "    print(\"    \", \"Analyze summary annotation\")\n",
    "    \n",
    "    # Analyze summary annotations.\n",
    "    dict_of_summary_relevance: Dict[str, Dict[str, List[float]]] = {}\n",
    "    for iteration_5 in list_of_iterations:\n",
    "        dict_of_summary_relevance[iteration_5] = {\n",
    "            \"not_exploitable\": [],\n",
    "            \"partially_exploitable\": [],\n",
    "            \"exploitable\": [],\n",
    "        }\n",
    "        # For all experiment...\n",
    "        for experiment_5 in df_llm_annotations.keys():\n",
    "            # Case of iteration exist in this experiment.\n",
    "            if iteration_5 in df_llm_annotations[experiment_5].keys():\n",
    "                # Count ratio of not exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"not_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"not_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of partially exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"partially_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"partially_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "            # Case of iteration doesn't exist in this experiment, so take the last existing iteration.\n",
    "            else:\n",
    "                # Define the last existing iteration.\n",
    "                max_experiment_iteration_5: str = max(df_llm_annotations[experiment_5].keys())\n",
    "                # Count ratio of not exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"not_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"not_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][max_experiment_iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][max_experiment_iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of partially exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"partially_exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"partially_exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][max_experiment_iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][max_experiment_iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "                # Count ratio of exploitable clusters.\n",
    "                dict_of_summary_relevance[iteration_5][\"exploitable\"].append(\n",
    "                    sum([\n",
    "                        annotation == \"exploitable\"\n",
    "                        for annotation in df_llm_annotations[experiment_5][max_experiment_iteration_5][\"[Q] summary relevant?\"]\n",
    "                    ]) / len(df_llm_annotations[experiment_5][max_experiment_iteration_5][\"cluster_id\"])\n",
    "                )\n",
    "\n",
    "    # Compute mean of summary relevance.\n",
    "    dict_of_summary_relevance_MEAN: Dict[str, Dict[str, float]] = {\n",
    "        iteration_5m: {\n",
    "            level: np.mean(dict_of_summary_relevance[iteration_5m][level])\n",
    "            for level in dict_of_summary_relevance[iteration_5m].keys()\n",
    "        }\n",
    "        for iteration_5m in dict_of_summary_relevance.keys()\n",
    "    }\n",
    "\n",
    "    # Compute sem of summary relevance.\n",
    "    dict_of_summary_relevance_SEM: Dict[str, Dict[str, float]] = {\n",
    "        iteration_5s: {\n",
    "            level: scipystats.sem(dict_of_summary_relevance[iteration_5s][level])\n",
    "            for level in dict_of_summary_relevance[iteration_5s].keys()\n",
    "        }\n",
    "        for iteration_5s in dict_of_summary_relevance.keys()\n",
    "    }\n",
    "        \n",
    "    # Create a new figure.\n",
    "    fig_plot_summary_relevance: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "    axis_plot_summary_relevance = fig_plot_summary_relevance.gca()\n",
    "\n",
    "    # Set range of axis.\n",
    "    axis_plot_summary_relevance.set_xlim(xmin=-0.5, xmax=int(max(list_of_iterations))+0.5)\n",
    "    axis_plot_summary_relevance.set_ylim(ymin=-0.01, ymax=1.01)\n",
    "    \n",
    "    # Plot not_exploitable summary relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_summary_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_summary_relevance_MEAN[iter_plot][\"not_exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_summary_relevance_SEM[iter_plot][\"not_exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_SEM.keys()\n",
    "        },\n",
    "        label=\"Non exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"red\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    \n",
    "    # Plot partially_exploitable summary relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_summary_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_summary_relevance_MEAN[iter_plot][\"partially_exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_summary_relevance_SEM[iter_plot][\"partially_exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_SEM.keys()\n",
    "        },\n",
    "        label=\"Partiellement exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    \n",
    "    # Plot exploitable summary relevance evolution.\n",
    "    add_plot_of_evolution_per_iteration_to_graph(\n",
    "        axis=axis_plot_summary_relevance,\n",
    "        list_of_x=list_of_iterations,\n",
    "        dict_of_y={\n",
    "            iter_plot: dict_of_summary_relevance_MEAN[iter_plot][\"exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_MEAN.keys()\n",
    "        },\n",
    "        dict_of_y_err={\n",
    "            iter_plot: dict_of_summary_relevance_SEM[iter_plot][\"exploitable\"]\n",
    "            for iter_plot in dict_of_summary_relevance_MEAN.keys()\n",
    "        },\n",
    "        label=\"Exploitable\",\n",
    "        marker=\"\",\n",
    "        markersize=3,\n",
    "        color=\"green\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    # Set axis name.\n",
    "    axis_plot_summary_relevance.set_xlabel(\"it√©ration [#]\", fontsize=18,)\n",
    "    axis_plot_summary_relevance.set_ylabel(\"ratio de r√©sum√©s [%]\", fontsize=18,)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    # Plot the legend.\n",
    "    axis_plot_summary_relevance.legend(fontsize=15, loc=\"lower right\",)\n",
    "\n",
    "    # Plot the grid.\n",
    "    axis_plot_summary_relevance.grid(True)\n",
    "\n",
    "    # Store the graph.\n",
    "    fig_plot_summary_relevance.savefig(\n",
    "        \"../results/etude-pertinence-llm-check-resume-annotation-favori.png\",\n",
    "        dpi=300,\n",
    "        transparent=True,\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa280e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
