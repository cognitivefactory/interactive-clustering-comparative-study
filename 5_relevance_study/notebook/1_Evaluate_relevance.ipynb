{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063673fd",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : BUSINESS RELEVANCE STUDY ====\n",
    "> ### Stage 1 : Evaluate business relevance on previous experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12982cf",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e43b6e",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at compute business relevance on interactive clustering experiments**.\n",
    "- Computations are based on previous experiments (cf. efficience study) exported in `./previous` folder.\n",
    "- Environments results are stored in their `.JSON` files in the `/experiments` folder.\n",
    "\n",
    "Then, **go to the notebook `2_Plot_some_figures.ipynb` to plot several figures according to these computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f1eb",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "The several computations are :\n",
    "- `consistency_score`: Test a TF-IDF + Linear SVM model on its own trainset with a high prediction score threshold in order to check data consistency. The consistency score is the f1-score computed. If f1-score is low (i.e. less than `0.75`), then trainset can be inconsistent : Data may be badly labeled or classes may overlap. Otherwise, f1-score should tend to 100%.\n",
    "- `fmc_describtion`: Use the Features Maximization Contrast m√©thod of feature selection in order to describe a cluster by its relevant linguistic patterns.\n",
    "- `llm_summarization`: Use a large language model (`openai/GPT3.5`) to summarize each cluster as a one sentence description.\n",
    "\n",
    "Some combinations of parameters are studied:\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `hier_avg-10c`: best to reach 90% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: best to reach 100% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `in_same-50` + `kmeans_COP-10c`: best to reach annotation completeness (cf. efficiency study)\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: choice of author (cf. cost study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec8386",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Any, Optional\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b310fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf441af",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. LOAD EXPERIMENTS TO STUDY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055773",
   "metadata": {},
   "source": [
    "Find all implementations to analyze and associated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of run tasks to parallelize.\n",
    "LIST_OF_TASKS: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"implementation\": implementation,  # Environment of experiment.\n",
    "        \"experiments\": [\n",
    "            exp_path\n",
    "            for exp_path in os.listdir(\"../previous/\"+implementation)\n",
    "            if \".json\" in exp_path\n",
    "        ]\n",
    "    }\n",
    "    for implementation in os.listdir(\"../previous\")\n",
    "    if (\n",
    "        os.path.isdir(\"../previous/\" + implementation)\n",
    "        and implementation in [\n",
    "            \"bank_cards_v1_-_settings_0_partial\",  # \"bank_cards_v1\" + \"settings_0_partial\" # \"bank_cards_v1\" + \"simple_prep_-_tfidf\" + \"closest-50\" + \"hier_avg-10c\", best to reach 90% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_1_sufficient\",  # \"bank_cards_v1\" + \"lemma_prep_-_tfidf\" + \"closest-50\" + \"kmeans_COP-10c\", best to reach 100% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_2_exhaustive\",  # \"bank_cards_v1\" + \"lemma_prep_-_tfidf\" + \"in_same-50\" + \"kmeans_COP-10c\", best to reach annotation completeness (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_settings_3_favorite\",  # \"bank_cards_v1\" + \"simple_prep_-_tfidf\" + \"closest-50\" + \"kmeans_COP-10c\", choice of author (cf. cost study)\n",
    "            #\"bank_cards_v1_-_simple_prep_-_tfidf\",  # Mean.\n",
    "            #\"bank_cards_v1\",  # Mean.\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "print(\"There are\", \"`\" + str(len(LIST_OF_TASKS)) + \"`\", \"implementations to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d258b8",
   "metadata": {},
   "source": [
    "Create one folder per implementation to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in LIST_OF_TASKS:\n",
    "    \n",
    "    # If folder exists: continue.\n",
    "    if os.path.isdir(\"../experiments/\" + task[\"implementation\"]):\n",
    "        continue\n",
    "    \n",
    "    # Create folder for analyses.\n",
    "    os.mkdir(\"../experiments/\" + task[\"implementation\"])\n",
    "    \n",
    "    # Copy data.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        with open(\"../previous/\" + task[\"implementation\"] + \"/\" + experiment, \"r\") as file_previous_results_r:\n",
    "            previous_results = json.load(file_previous_results_r)\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"w\") as file_previous_results_w:\n",
    "            json.dump(\n",
    "                previous_results,\n",
    "                file_previous_results_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026de5d4",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 3. START ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1ef00",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.1. Analyze clustering consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a303184",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from consistency_score import compute_consistency_score, display_consistency_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25c816",
   "metadata": {},
   "source": [
    "Compute all consistency scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67801e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "    \n",
    "        # If folder exists: continue.\n",
    "        if os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment):\n",
    "            continue\n",
    "        \n",
    "        # Load data\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "            \n",
    "        # Compute consistency score of groundtruth.\n",
    "        groundtruth_consistency_score: float = compute_consistency_score(\n",
    "            x_train = [\n",
    "                dict_of_preprocessed_texts[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            y_train = [\n",
    "                dict_of_true_intents[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            prediction_score_threshold = 0.75,\n",
    "        )\n",
    "\n",
    "        # Compute consistency score of clustering.\n",
    "        clustering_consistency_score_evolution: Dict[str, float] = {\n",
    "            iteration: compute_consistency_score(\n",
    "                x_train = [\n",
    "                    dict_of_preprocessed_texts[text_ID]\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                y_train = [\n",
    "                    str(dict_of_clustering_results[iteration][text_ID])\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                prediction_score_threshold = 0.75,\n",
    "            )\n",
    "            for iteration in dict_of_clustering_results.keys()\n",
    "        }\n",
    "            \n",
    "        # Store results.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment, \"w\") as file_constistency_score_w:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"groundtruth\": groundtruth_consistency_score,\n",
    "                    \"evolution\": clustering_consistency_score_evolution,\n",
    "                },\n",
    "                file_constistency_score_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444285b",
   "metadata": {},
   "source": [
    "Display consistency score evolution in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fa976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_consistency_score = {\n",
    "    \"bank_cards_v1_-_settings_0_partial\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation partielle (90% de v-measure).\",\n",
    "        \"plot_color\": \"green\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-partielle.png\",\n",
    "    },  # best to reach 90% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_1_sufficient\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation suffisante (100% de v-measure).\",\n",
    "        \"plot_color\": \"blue\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-suffisante.png\",\n",
    "    },  # best to reach 100% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_2_exhaustive\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant le meilleur param√©trage moyen pour atteindre\\nune annotation exhaustive (toutes les contraintes).\",\n",
    "        \"plot_color\": \"red\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-exhaustive.png\",\n",
    "    },  # best to reach annotation completeness (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_settings_3_favorite\": {\n",
    "        \"plot_label\": \"Score de coh√©rence moyen des tentatives\\nayant notre param√©trage favori pour atteindre\\n90% de v-measure avec un co√ªt global minimal.\",\n",
    "        \"plot_color\": \"gold\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-favori.png\",\n",
    "    },  # choice of author (cf. cost study)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64ae4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"], \"\\n    (\", config_consistency_score[task[\"implementation\"]][\"graph_filename\"], \")\")\n",
    "    display_consistency_score(\n",
    "        implementation=task[\"implementation\"],\n",
    "        list_of_experiments=task[\"experiments\"],\n",
    "        list_of_iterations=[str(i).zfill(4) for i in range(50)],\n",
    "        plot_label=config_consistency_score[task[\"implementation\"]][\"plot_label\"],\n",
    "        plot_color=config_consistency_score[task[\"implementation\"]][\"plot_color\"],\n",
    "        graph_filename=config_consistency_score[task[\"implementation\"]][\"graph_filename\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4f1db",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.2. Describe clusters by their relevant linguistic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c75534",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cognitivefactory.features_maximization_metric.fmc import FeaturesMaximizationMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c60f7",
   "metadata": {},
   "source": [
    "Describe clusters with FMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ff5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "        \n",
    "        # Intialize FMC descriptions if need.\n",
    "        if not os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment):\n",
    "            # Define list of iteration to analyze.\n",
    "            list_of_iteration: List[str] = [\n",
    "                i\n",
    "                for i in dict_of_clustering_results.keys()\n",
    "                if (\n",
    "                    int(i) % 5 == 0\n",
    "                    or i == max(dict_of_clustering_results.keys())\n",
    "                )\n",
    "            ]\n",
    "            # Define initial FMC descriptions for each cluster of each iteration to analyze.\n",
    "            initial_fmc_descriptions: Dict[str, Dict[str, Optional[List[str]]]] = {\n",
    "                iteration: {\n",
    "                    str(cluster_id): None  # need to force cluster_id to str for serialization\n",
    "                    for cluster_id in sorted(set(dict_of_clustering_results[iteration].values()))\n",
    "                }\n",
    "                for iteration in list_of_iteration\n",
    "            }\n",
    "            # Store initial FMC descriptions.\n",
    "            with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"w\") as file_initial_fmc_description_w:\n",
    "                json.dump(initial_fmc_descriptions, file_initial_fmc_description_w)\n",
    "        \n",
    "        # Load FMC descriptions already done.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"r\") as file_fmc_description_r:\n",
    "            descriptions: Dict[str, Dict[str, Optional[str]]] = json.load(file_fmc_description_r)\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in descriptions.keys():\n",
    "            print(\"    \", \"    \", \"iteration:\", iteration)\n",
    "            \n",
    "            # Define vectorizer.\n",
    "            vectorizer = TfidfVectorizer(min_df=0, ngram_range=(1, 3), analyzer=\"word\", sublinear_tf=True)\n",
    "            matrix_of_vectors: csr_matrix = vectorizer.fit_transform(\n",
    "                [\n",
    "                    str(dict_of_preprocessed_texts[text_id])\n",
    "                    for text_id in dict_of_preprocessed_texts.keys()\n",
    "                ]\n",
    "            )\n",
    "            list_of_possible_vectors_features: List[str] = list(vectorizer.get_feature_names_out())\n",
    "\n",
    "            # Define FMC modelization.\n",
    "            fmc_computer: FeaturesMaximizationMetric = FeaturesMaximizationMetric(\n",
    "                data_vectors=matrix_of_vectors,\n",
    "                data_classes=[dict_of_clustering_results[iteration][text_id] for text_id in dict_of_preprocessed_texts.keys()],\n",
    "                list_of_possible_features=list_of_possible_vectors_features,\n",
    "                amplification_factor=1,\n",
    "            )\n",
    "            \n",
    "            # For each cluster\n",
    "            for cluster_id in descriptions[iteration].keys():\n",
    "                print(\"    \", \"    \", \"    \", cluster_id, end=\": \")\n",
    "                \n",
    "                # If already done: continue\n",
    "                if descriptions[iteration][str(cluster_id)] is not None:\n",
    "                    print(\"(already done)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get FMC description of cluster.\n",
    "                cluster_description: List[str] = [\n",
    "                    linguistic_pattern\n",
    "                    for linguistic_pattern in fmc_computer.get_most_active_features_by_a_classe(\n",
    "                        classe=int(cluster_id),\n",
    "                        activation_only=True,\n",
    "                        sort_by='fmeasure',  # \"contrast\"\n",
    "                        max_number=50,\n",
    "                    )\n",
    "                    if fmc_computer.get_most_activated_classes_by_a_feature(linguistic_pattern) == [int(cluster_id)]\n",
    "                ]\n",
    "                print(cluster_description)\n",
    "                \n",
    "                # Store updated descriptions.\n",
    "                descriptions[iteration][str(cluster_id)] = cluster_description\n",
    "                with open(\"../experiments/\" + task[\"implementation\"] + \"/fmc_description___\" + experiment, \"w\") as file_fmc_description_w:\n",
    "                    json.dump(descriptions, file_fmc_description_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f2f9a",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.3. Summarize clusters by a large language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b8908",
   "metadata": {},
   "source": [
    "Load Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aad2f",
   "metadata": {},
   "source": [
    "Load credentials.\n",
    "> Need a file `credentials.py` in projet home (`../..`)\n",
    "> with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import credentials  # Need a file `credentials.py` in projet home (`..`) with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys\n",
    "openai.api_key = credentials.OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42ea6d",
   "metadata": {},
   "source": [
    "Define large language model to call and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to use.\n",
    "OPENAI_MODEL: str = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e702ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts to use\n",
    "prompt_context: str = \"Tu es un expert des secteurs banque, assurance et finance.\"\n",
    "prompt_task: str = \"R√©sume-moi en une phrase la th√©matique trait√©e dans les textes suivants\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d13d6",
   "metadata": {},
   "source": [
    "Summarize clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae99487",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRY: int = 5  # retry when timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990bd89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "        \n",
    "        # Intialize clustering summaries if need.\n",
    "        if not os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment):\n",
    "            # Define list of iteration to analyze.\n",
    "            list_of_iteration: List[str] = [\n",
    "                i\n",
    "                for i in dict_of_clustering_results.keys()\n",
    "                if (\n",
    "                    int(i) % 5 == 0\n",
    "                    or i == max(dict_of_clustering_results.keys())\n",
    "                )\n",
    "            ]\n",
    "            # Define initial clustering summaries for each cluster of each iteration to analyze.\n",
    "            initial_clustering_summaries: Dict[str, Dict[str, Optional[str]]] = {\n",
    "                iteration: {\n",
    "                    str(cluster_id): None  # need to force cluster_id to str for serialization\n",
    "                    for cluster_id in sorted(set(dict_of_clustering_results[iteration].values()))\n",
    "                }\n",
    "                for iteration in list_of_iteration\n",
    "            }\n",
    "            # Store initial clustering summaries.\n",
    "            with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"w\") as file_initial_llm_summaries_w:\n",
    "                json.dump(initial_clustering_summaries, file_initial_llm_summaries_w)\n",
    "\n",
    "        # Load clustering summaries already done.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_llm_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_llm_summary_r)\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in summaries.keys():\n",
    "            print(\"\\n    \", \"    \", \"iteration:\", iteration)\n",
    "            \n",
    "            # For each cluster\n",
    "            for cluster_id in summaries[iteration].keys():\n",
    "                print(\"    \", \"    \", \"    \", cluster_id, end=\": \")\n",
    "                \n",
    "                # If already done: continue\n",
    "                if summaries[iteration][str(cluster_id)] is not None:\n",
    "                    print(\"(already done)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get texts of this cluster.\n",
    "                cluster = [\n",
    "                    text\n",
    "                    for text_id, text in dict_of_texts.items()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ]\n",
    "                print(\"(cluster length: {0})\".format(len(cluster)), end=\" ; \")\n",
    "                \n",
    "                # Call the LLM to summarize the document (use loop to by-pass timeout).\n",
    "                it: int = 0\n",
    "                last_err: Exception = None\n",
    "                while it<MAX_RETRY:\n",
    "                    time.sleep(21)  # To avoid \"Rate limit reached\" => \"default-gpt-3.5-turbo\" limited at 3 request per minute.\n",
    "                    try : \n",
    "                        it += 1\n",
    "                        # Create a chat completion.\n",
    "                        chat_answers = openai.ChatCompletion.create(\n",
    "                            model=OPENAI_MODEL,\n",
    "                            messages=[\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": \"{context}\".format(\n",
    "                                        context=prompt_context,\n",
    "                                    )\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": \"{task} :\\n\\n{data}\".format(\n",
    "                                        task=prompt_task,\n",
    "                                        data=\"- \" + \"\\n- \".join(cluster),\n",
    "                                    )\n",
    "                                }\n",
    "                            ]\n",
    "                        )\n",
    "                        break\n",
    "                    # except OSError :\n",
    "                    except Exception as err:\n",
    "                        last_err = err\n",
    "                        continue\n",
    "                \n",
    "                # If error: continue...\n",
    "                if it==MAX_RETRY:\n",
    "                    print(last_err)\n",
    "                    continue\n",
    "                \n",
    "                # Get summary from answer.\n",
    "                cluster_summary: str = chat_answers.choices[0].message.content\n",
    "                print(cluster_summary[:100], \"[...]\")\n",
    "                \n",
    "                # Store updated summaries.\n",
    "                summaries[iteration][str(cluster_id)] = cluster_summary\n",
    "                with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"w\") as file_llm_summary_w:\n",
    "                    json.dump(summaries, file_llm_summary_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b18f6e",
   "metadata": {},
   "source": [
    "Export in XLSX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33c805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        if (\n",
    "            os.path.isfile(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_0___\" + experiment.split(\".\")[0] + \".xlsx\")\n",
    "            or os.path.isfile(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_1___\" + experiment.split(\".\")[0] + \".xlsx\")\n",
    "            or os.path.isfile(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_2___\" + experiment.split(\".\")[0] + \".xlsx\")\n",
    "        ):\n",
    "            print(\"    \", \"experiment:\", experiment, \"--> SKIP: export already done\")\n",
    "            continue\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        list_of_possible_true_labels: List[str] = sorted(set(dict_of_true_intents.values()))\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "\n",
    "        # Load clustering summaries.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary___\" + experiment, \"r\") as file_llm_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_llm_summary_r)\n",
    "        \n",
    "        # Initialize list of exports.\n",
    "        df_exports: Dict[str, pd.DataFrame] = {}\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in summaries.keys():\n",
    "            \n",
    "            # Initialize export data.\n",
    "            df_exports[iteration] = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\", \"cluster\", \"summary\", \"confusion\",\n",
    "                    \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\", \"[Q] summary relevant?\", \"[Q] summary main topic?\",\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # For each cluster.\n",
    "            for cluster_id in summaries[iteration].keys():\n",
    "                \n",
    "                # Estimate confusion.\n",
    "                list_of_present_true_labels: List[str] = sorted(set(\n",
    "                    dict_of_true_intents[text_id]\n",
    "                    for text_id in dict_of_texts.keys()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ))\n",
    "                \n",
    "                # Update export data.\n",
    "                data_summary: Dict[str, Any] = {\n",
    "                    \"implementation_id\": task[\"implementation\"],\n",
    "                    \"experiment_id\": experiment.split(\".\")[0],\n",
    "                    \"iteration_id\": iteration,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"cluster\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}\".format(text)\n",
    "                        for text_id, text in dict_of_texts.items()\n",
    "                        if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                    ]),\n",
    "                    \"summary\": summaries[iteration][cluster_id],\n",
    "                    \"confusion\": \"'\" + \"\\r\\n\".join([\n",
    "                        \"- {0}: {1}\".format(\n",
    "                            true_label,\n",
    "                            len([\n",
    "                                text_id\n",
    "                                for text_id in dict_of_texts.keys()\n",
    "                                if (\n",
    "                                    dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                                    and dict_of_true_intents[text_id] == true_label\n",
    "                                )\n",
    "                            ])\n",
    "                        )\n",
    "                        for true_label in list_of_present_true_labels\n",
    "                    ]),\n",
    "                    \"[Q] cluster well-designed?\": None,\n",
    "                    \"[Q] cluster main topic?\": None,\n",
    "                    \"[Q] summary relevant?\": None,\n",
    "                    \"[Q] summary main topic?\": None,\n",
    "                }\n",
    "                df_exports[iteration] = df_exports[iteration].append(\n",
    "                    data_summary,\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "        # Export summaries.\n",
    "        with pd.ExcelWriter(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_0___\" + experiment.split(\".\")[0] + \".xlsx\") as writer:\n",
    "            \n",
    "            # Export all iterations, one iteration per sheet.\n",
    "            for iteration_key, df_export in df_exports.items():\n",
    "                # Export data.\n",
    "                df_export.to_excel(\n",
    "                    excel_writer=writer,\n",
    "                    sheet_name=iteration_key,\n",
    "                    index=False,\n",
    "                    #engine=\"openpyxl\",\n",
    "                )\n",
    "                # Format data.\n",
    "                workbook = writer.book\n",
    "                format_header = workbook.add_format({\"text_wrap\": True, \"bold\": True})\n",
    "                format_cluster = workbook.add_format({\"text_wrap\": True})\n",
    "                format_summary = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"font_color\": \"blue\"})\n",
    "                format_confusion = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "                format_cluster_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#97D2D4\"})\n",
    "                format_summary_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#EBDB5E\"})\n",
    "                worksheet = writer.sheets[iteration_key]\n",
    "                worksheet.set_row(row=0, height=None, cell_format=format_header)\n",
    "                worksheet.set_column(first_col=0, last_col=3, width=10, cell_format=None)\n",
    "                worksheet.set_column(first_col=4, last_col=4, width=70, cell_format=format_cluster)\n",
    "                worksheet.set_column(first_col=5, last_col=5, width=40, cell_format=format_summary)\n",
    "                worksheet.set_column(first_col=6, last_col=6, width=25, cell_format=format_confusion)\n",
    "                worksheet.set_column(first_col=7, last_col=8, width=20, cell_format=format_cluster_analysis)\n",
    "                worksheet.set_column(first_col=9, last_col=10, width=20, cell_format=format_summary_analysis)\n",
    "                for i in range(len(df_export)):\n",
    "                    worksheet.set_row(row=1+i, height=150, cell_format=None)\n",
    "                    \n",
    "        # Export for clusters analysis.\n",
    "        with pd.ExcelWriter(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_1___\" + experiment.split(\".\")[0] + \".xlsx\") as writer:\n",
    "            \n",
    "            # Export all iterations, one iteration per sheet.\n",
    "            for iteration_key, df_export in df_exports.items():\n",
    "                # Export data.\n",
    "                df_export[[\n",
    "                    \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\",\n",
    "                    \"cluster\", \"confusion\", \"[Q] cluster well-designed?\", \"[Q] cluster main topic?\",\n",
    "                ]].to_excel(\n",
    "                    excel_writer=writer,\n",
    "                    sheet_name=iteration_key,\n",
    "                    index=False,\n",
    "                    #engine=\"openpyxl\",\n",
    "                )\n",
    "                # Format data.\n",
    "                workbook = writer.book\n",
    "                format_header = workbook.add_format({\"text_wrap\": True, \"bold\": True})\n",
    "                format_cluster = workbook.add_format({\"text_wrap\": True})\n",
    "                format_summary = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"font_color\": \"blue\"})\n",
    "                format_confusion = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "                format_cluster_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#97D2D4\"})\n",
    "                format_summary_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#EBDB5E\"})\n",
    "                worksheet = writer.sheets[iteration_key]\n",
    "                worksheet.set_row(row=0, height=None, cell_format=format_header)\n",
    "                worksheet.set_column(first_col=0, last_col=3, width=10, cell_format=None)\n",
    "                worksheet.set_column(first_col=4, last_col=4, width=70, cell_format=format_cluster)\n",
    "                worksheet.set_column(first_col=5, last_col=5, width=35, cell_format=format_confusion)\n",
    "                worksheet.set_column(first_col=6, last_col=7, width=20, cell_format=format_cluster_analysis)\n",
    "                for i in range(len(df_export)):\n",
    "                    worksheet.set_row(row=1+i, height=150, cell_format=None)\n",
    "                    \n",
    "        # Export for summaries analysis.\n",
    "        with pd.ExcelWriter(\"../experiments/\" + task[\"implementation\"] + \"/llm_summary_2___\" + experiment.split(\".\")[0] + \".xlsx\") as writer:\n",
    "            \n",
    "            # Export all iterations, one iteration per sheet.\n",
    "            for iteration_key, df_export in df_exports.items():\n",
    "                # Export data.\n",
    "                df_export[[\n",
    "                    \"implementation_id\", \"experiment_id\", \"iteration_id\", \"cluster_id\",\n",
    "                    \"summary\", \"[Q] summary relevant?\", \"[Q] summary main topic?\",\n",
    "                ]].to_excel(\n",
    "                    excel_writer=writer,\n",
    "                    sheet_name=iteration_key,\n",
    "                    index=False,\n",
    "                    #engine=\"openpyxl\",\n",
    "                )\n",
    "                # Format data.\n",
    "                workbook = writer.book\n",
    "                format_header = workbook.add_format({\"text_wrap\": True, \"bold\": True})\n",
    "                format_cluster = workbook.add_format({\"text_wrap\": True})\n",
    "                format_summary = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"font_color\": \"blue\"})\n",
    "                format_confusion = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "                format_cluster_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#97D2D4\"})\n",
    "                format_summary_analysis = workbook.add_format({\"text_wrap\": True, \"bold\": True, \"valign\": \"top\", \"bg_color\": \"#EBDB5E\"})\n",
    "                worksheet = writer.sheets[iteration_key]\n",
    "                worksheet.set_row(row=0, height=None, cell_format=format_header)\n",
    "                worksheet.set_column(first_col=0, last_col=3, width=10, cell_format=None)\n",
    "                worksheet.set_column(first_col=4, last_col=4, width=70, cell_format=format_summary)\n",
    "                worksheet.set_column(first_col=5, last_col=6, width=20, cell_format=format_summary_analysis)\n",
    "                for i in range(len(df_export)):\n",
    "                    worksheet.set_row(row=1+i, height=75, cell_format=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd677d5",
   "metadata": {},
   "source": [
    "Manually analyze each clusters and their summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da211906",
   "metadata": {},
   "source": [
    "1. **Questions for annotators/reviewers**:\n",
    "- **cluster analysis**:\n",
    "    - `is this cluster well-designed / consistent / suited for training ?` (all well-designed clusters are accepted, even if its not the groundtruth).\n",
    "    - `if so, what is the single main topic of this cluster ?`\n",
    "- **summary analysis**:\n",
    "    - `does this summary represent a single main topic ?`\n",
    "    - `if so, what is the single main topic of this summary ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fb718",
   "metadata": {},
   "source": [
    "2. **Notations scale**:\n",
    "\n",
    "- `0` / `0%`: `garbage cluster`:\n",
    "    - *definition is not clear*: the cluster has no main topic *or* too many topics.\n",
    "    - *so content is not trainable*: the cluster is too small *or* too big.\n",
    "    - $\\Rightarrow$ it's absolutely **not usable**.\n",
    "\n",
    "- `1` / `33%`: `not really exploitable`:\n",
    "    - *definition can be deduced*: the cluster has several topics ideas but lacks relevance.\n",
    "    - *so content is not trainable*: the cluster is not usable as is.\n",
    "    - $\\Rightarrow$ it gives some clues to **manually define and create** an exploitable cluster.\n",
    "\n",
    "- `2` / `66%`: `partially exploitable`:\n",
    "    - *definition seems clear*: the cluster has one main topic.\n",
    "    - *but content is still not trainable*: the cluster is too small (<25 data) or needs to filter some data.\n",
    "    - $\\Rightarrow$ the cluster can be used **with some manual editing**.\n",
    "\n",
    "- `3` / `100%`: `exploitable`:\n",
    "    - *definition is clear*: the cluster has one identifiable main topic.\n",
    "    - *content is now trainable*: the cluster has enough data (>25 data) and few intruders (< 5 data).\n",
    "    - $\\Rightarrow$ the cluster can be used **with little manual editing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88f0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
