{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063673fd",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : BUSINESS RELEVANCE STUDY ====\n",
    "> ### Stage 1 : Evaluate business relevance on previous experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12982cf",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e43b6e",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at compute business relevance on interactive clustering experiments**.\n",
    "- Computations are based on previous experiments (cf. efficience study) exported in `./previous` folder.\n",
    "- Environments results are stored in their `.JSON` files in the `/experiments` folder.\n",
    "\n",
    "Then, **go to the notebook `2_Plot_some_figures.ipynb` to plot several figures according to these computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f1eb",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "The several computations are :\n",
    "- `consistency_score`: Test a TF-IDF + Linear SVM model on its own trainset with a high prediction score threshold in order to check data consistency. The consistency score is the f1-score computed. If f1-score is low (i.e. less than `0.75`), then trainset can be inconsistent : Data may be badly labeled or classes may overlap. Otherwise, f1-score should tend to 100%.\n",
    "- `llm_summarization`: Use a large language model to summarize each cluster as a one sentence description.\n",
    "\n",
    "Some combinations of parameters are studied:\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `hier_avg-10c`: best to reach 90% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: best to reach 100% of v-measure (cf. efficiency study)\n",
    "- `lemma_prep` + `tfidf` + `in_same-50` + `kmeans_COP-10c`: best to reach annotation completeness (cf. efficiency study)\n",
    "- `simple_prep` + `tfidf` + `closest-50` + `kmeans_COP-10c`: choice of author (cf. cost study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec8386",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional\n",
    "import os\n",
    "import json\n",
    "from consistency_score import compute_consistency_score, display_consistency_score\n",
    "import time\n",
    "import haystack\n",
    "from haystack.nodes import PromptTemplate\n",
    "from haystack.nodes import PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b310fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf441af",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. LOAD EXPERIMENTS TO STUDY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055773",
   "metadata": {},
   "source": [
    "Find all implementations to analyze and associated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of run tasks to parallelize.\n",
    "LIST_OF_TASKS: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"implementation\": implementation,  # Environment of experiment.\n",
    "        \"experiments\": [\n",
    "            exp_path\n",
    "            for exp_path in os.listdir(\"../previous/\"+implementation)\n",
    "            if \".json\" in exp_path\n",
    "        ]\n",
    "    }\n",
    "    for implementation in os.listdir(\"../previous\")\n",
    "    if (\n",
    "        os.path.isdir(\"../previous/\" + implementation)\n",
    "        and implementation in [\n",
    "            \"bank_cards_v1_-_simple_prep_-_tfidf_-_closest-50_-_hier_avg-10c\",  # best to reach 90% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_lemma_prep_-_tfidf_-_closest-50_-_kmeans_COP-10c\",  # best to reach 100% of v-measure (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_lemma_prep_-_tfidf_-_in_same-50_-_kmeans_COP-10c\",  # best to reach annotation completeness (cf. efficiency study)\n",
    "            \"bank_cards_v1_-_simple_prep_-_tfidf_-_closest-50_-_kmeans_COP-10c\",  # choice of author (cf. cost study)\n",
    "            #\"bank_cards_v1_-_simple_prep_-_tfidf\",  # Mean.\n",
    "            #\"bank_cards_v1\",  # Mean.\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "print(\"There are\", \"`\" + str(len(LIST_OF_TASKS)) + \"`\", \"implementations to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d258b8",
   "metadata": {},
   "source": [
    "Create one folder per implementation to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in LIST_OF_TASKS:\n",
    "    \n",
    "    # If folder exists: continue.\n",
    "    if os.path.isdir(\"../experiments/\" + task[\"implementation\"]):\n",
    "        continue\n",
    "    \n",
    "    # Create folder for analyses.\n",
    "    os.mkdir(\"../experiments/\" + task[\"implementation\"])\n",
    "    \n",
    "    # Copy data.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        with open(\"../previous/\" + task[\"implementation\"] + \"/\" + experiment, \"r\") as file_previous_results_r:\n",
    "            previous_results = json.load(file_previous_results_r)\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"w\") as file_previous_results_w:\n",
    "            json.dump(\n",
    "                previous_results,\n",
    "                file_previous_results_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026de5d4",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 3. START ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1ef00",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.1. Analyze clustering consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25c816",
   "metadata": {},
   "source": [
    "Compute all consistency scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67801e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "    \n",
    "        # If folder exists: continue.\n",
    "        if os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment):\n",
    "            continue\n",
    "        \n",
    "        # Load data\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = experiment_data[\"dict_of_preprocessed_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "            \n",
    "        # Compute consistency score of groundtruth.\n",
    "        groundtruth_consistency_score: float = compute_consistency_score(\n",
    "            x_train = [\n",
    "                dict_of_preprocessed_texts[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            y_train = [\n",
    "                dict_of_true_intents[text_ID]\n",
    "                for text_ID in dict_of_preprocessed_texts.keys()\n",
    "            ],\n",
    "            prediction_score_threshold = 0.75,\n",
    "        )\n",
    "\n",
    "        # Compute consistency score of clustering.\n",
    "        clustering_consistency_score_evolution: Dict[str, float] = {\n",
    "            iteration: compute_consistency_score(\n",
    "                x_train = [\n",
    "                    dict_of_preprocessed_texts[text_ID]\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                y_train = [\n",
    "                    str(dict_of_clustering_results[iteration][text_ID])\n",
    "                    for text_ID in dict_of_preprocessed_texts.keys()\n",
    "                ],\n",
    "                prediction_score_threshold = 0.75,\n",
    "            )\n",
    "            for iteration in dict_of_clustering_results.keys()\n",
    "        }\n",
    "            \n",
    "        # Store results.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/constistency_score___\" + experiment, \"w\") as file_constistency_score_w:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"groundtruth\": groundtruth_consistency_score,\n",
    "                    \"evolution\": clustering_consistency_score_evolution,\n",
    "                },\n",
    "                file_constistency_score_w\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444285b",
   "metadata": {},
   "source": [
    "Display consistency score evolution in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fa976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_consistency_score = {\n",
    "    \"bank_cards_v1_-_simple_prep_-_tfidf_-_closest-50_-_hier_avg-10c\": {\n",
    "        \"plot_label\": \"Score de cohérence moyen des tentatives ayant le meilleur paramétrage moyen\\npour atteindre une annotation partielle (90% de v-measure).\",\n",
    "        \"plot_color\": \"green\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-partielle.png\",\n",
    "    },  # best to reach 90% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_lemma_prep_-_tfidf_-_closest-50_-_kmeans_COP-10c\": {\n",
    "        \"plot_label\": \"Score de cohérence moyen des tentatives ayant le meilleur paramétrage moyen\\npour atteindre une annotation suffisante (100% de v-measure).\",\n",
    "        \"plot_color\": \"blue\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-suffisante.png\",\n",
    "    },  # best to reach 100% of v-measure (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_lemma_prep_-_tfidf_-_in_same-50_-_kmeans_COP-10c\": {\n",
    "        \"plot_label\": \"Score de cohérence moyen des tentatives ayant le meilleur paramétrage moyen\\npour atteindre une annotation exhaustive (toutes les contraintes).\",\n",
    "        \"plot_color\": \"red\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-exhaustive.png\",\n",
    "    },  # best to reach annotation completeness (cf. efficiency study)\n",
    "    \"bank_cards_v1_-_simple_prep_-_tfidf_-_closest-50_-_kmeans_COP-10c\": {\n",
    "        \"plot_label\": \"Score de cohérence moyen des tentatives ayant le paramétrage favori\\n(atteindre 90% de v-measure avec un coût global minimal).\",\n",
    "        \"plot_color\": \"gold\",\n",
    "        \"graph_filename\": \"etude-pertinence-consistence-annotation-retenue.png\",\n",
    "    },  # choice of author (cf. cost study)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64ae4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"], \"\\n    (\", config_consistency_score[task[\"implementation\"]][\"graph_filename\"], \")\")\n",
    "    display_consistency_score(\n",
    "        implementation=task[\"implementation\"],\n",
    "        list_of_experiments=task[\"experiments\"],\n",
    "        list_of_iterations=[str(i).zfill(4) for i in range(40)],\n",
    "        plot_label=config_consistency_score[task[\"implementation\"]][\"plot_label\"],\n",
    "        plot_color=config_consistency_score[task[\"implementation\"]][\"plot_color\"],\n",
    "        graph_filename=config_consistency_score[task[\"implementation\"]][\"graph_filename\"],\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f2f9a",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3.2. Summarize clusters by a large language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aad2f",
   "metadata": {},
   "source": [
    "Load credentials.\n",
    "> Need a file `credentials.py` in projet home (`../..`)\n",
    "> with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import credentials  # Need a file `credentials.py` in projet home (`..`) with content `OPENAI_API_TOKEN = \"...\"` from https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42ea6d",
   "metadata": {},
   "source": [
    "Define learge language model to call and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get language model.\n",
    "language_model: PromptNode = PromptNode(\n",
    "    model_name_or_path=\"text-davinci-003\",  # \"text-davinci-003\", \"gpt-3.5-turbo\"\n",
    "    api_key=credentials.OPENAI_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template.\n",
    "template: PromptTemplate = PromptTemplate(\n",
    "    prompt_text=\"\"\"\n",
    "    Trouver la thématique traitée dans le texte donné.\n",
    "    Texte : $text\n",
    "    \"\"\",\n",
    "    name=\"Description d'un topic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d13d6",
   "metadata": {},
   "source": [
    "Summarize clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae99487",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRY: int = 50  # for proxy error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990bd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each implementation.\n",
    "for task in LIST_OF_TASKS:\n",
    "    print(\"IMPLEMENTATION:\", task[\"implementation\"])\n",
    "    \n",
    "    # For each experiments.\n",
    "    for experiment in task[\"experiments\"]:\n",
    "        print(\"    \", \"experiment:\", experiment)\n",
    "        \n",
    "        # Load data.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/previous_results___\" + experiment, \"r\") as file_experiment_data_r:\n",
    "            experiment_data: Dict[str, Any] = json.load(file_experiment_data_r)\n",
    "        dict_of_texts: Dict[str, str] = experiment_data[\"dict_of_texts\"]\n",
    "        dict_of_true_intents: Dict[str, str] = experiment_data[\"dict_of_true_intents\"]\n",
    "        dict_of_clustering_results: Dict[str, Dict[str, str]] = experiment_data[\"dict_of_clustering_results\"]\n",
    "        \n",
    "        # Intialize clustering summaries if need.\n",
    "        if not os.path.exists(\"../experiments/\" + task[\"implementation\"] + \"/clustering_summary___\" + experiment):\n",
    "            # Define list of iteration to analyze.\n",
    "            list_of_iteration: List[str] = [\n",
    "                i\n",
    "                #for i in [\"0000\", \"0005\", \"0010\", \"0015\", \"0020\", \"0025\", \"0030\", \"0035\", \"0040\", max(dict_of_clustering_results.keys())]\n",
    "                for i in [max(dict_of_clustering_results.keys())]\n",
    "                if i in dict_of_clustering_results.keys()\n",
    "            ]\n",
    "            # Define initial clustering summaries for each cluster of each iteration to analyze.\n",
    "            initial_clustering_summaries: Dict[str, Dict[str, Optional[str]]] = {\n",
    "                iteration: {\n",
    "                    str(cluster_id): None  # need to force cluster_id to str for serialization\n",
    "                    for cluster_id in sorted(set(dict_of_clustering_results[iteration].values()))\n",
    "                }\n",
    "                for iteration in list_of_iteration\n",
    "            }\n",
    "            # Store initial clustering summaries.\n",
    "            with open(\"../experiments/\" + task[\"implementation\"] + \"/clustering_summary___\" + experiment, \"w\") as file_initial_clustering_summaries_w:\n",
    "                json.dump(initial_clustering_summaries, file_initial_clustering_summaries_w)\n",
    "        \n",
    "        # Load clustering summaries already done.\n",
    "        with open(\"../experiments/\" + task[\"implementation\"] + \"/clustering_summary___\" + experiment, \"r\") as file_clustering_summary_r:\n",
    "            summaries: Dict[str, Dict[str, Optional[str]]] = json.load(file_clustering_summary_r)\n",
    "        \n",
    "        # For each iteration...\n",
    "        for iteration in summaries.keys():\n",
    "            print(\"    \", \"    \", \"iteration:\", iteration)\n",
    "            \n",
    "            # For each cluster\n",
    "            for cluster_id in summaries[iteration].keys():\n",
    "                print(\"    \", \"    \", \"    \", cluster_id, end=\": \")\n",
    "                \n",
    "                # If already done: continue\n",
    "                if summaries[iteration][str(cluster_id)] is not None:\n",
    "                    print(\"(already done)\")\n",
    "                    continue                    \n",
    "                \n",
    "                # Get texts of this cluster.\n",
    "                cluster = [\n",
    "                    text\n",
    "                    for text_id, text in dict_of_texts.items()\n",
    "                    if dict_of_clustering_results[iteration][text_id] == int(cluster_id)\n",
    "                ]\n",
    "                print(\"(cluster length: {0})\".format(len(cluster)), end=\" ; \")\n",
    "                \n",
    "                # Call the LLM to summarize the document (use loop to by-pass timeout).\n",
    "                it: int = 0\n",
    "                last_err: Exception = None\n",
    "                while it<MAX_RETRY:\n",
    "                    time.sleep(.01)\n",
    "                    try : \n",
    "                        it += 1\n",
    "                        answer: List[str] = language_model.prompt(\n",
    "                            prompt_template=template,\n",
    "                            #text=\"\\n\".join(cluster),\n",
    "                            #text=\" \\n \".join(cluster),\n",
    "                            text=cluster,\n",
    "                        )\n",
    "                        break\n",
    "                    # except OSError :\n",
    "                    except Exception as err:\n",
    "                        last_err = err\n",
    "                        continue\n",
    "                \n",
    "                # If error: continue...\n",
    "                if it==MAX_RETRY:\n",
    "                    print(last_err)\n",
    "                    continue\n",
    "                \n",
    "                # Get summary from answer.\n",
    "                cluster_summary: str = answer[0]\n",
    "                print(cluster_summary)\n",
    "                \n",
    "                # Store updated summaries.\n",
    "                #summaries[iteration][str(cluster_id)] = cluster_summary\n",
    "                #with open(\"../experiments/\" + task[\"implementation\"] + \"/clustering_summary___\" + experiment, \"w\") as file_clustering_summary_w:\n",
    "                #    json.dump(summaries, file_clustering_summary_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3661d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
