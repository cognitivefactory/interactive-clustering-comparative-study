{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : ANNOTATION ERROR STUDY ====\n",
    "> ### Stage 1 : Initialize computation environments for experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at create environments needed to run annotation error study experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[CLUSTERING]/[CONSTRAINTS_SELECTION]/[ERRORS_SIMULATION]`.\n",
    "- The path is composed of A. the clustering used (cf. convergence study results to get the \"best implementation\"), B. the constraints management (randomly selected), and C. errors rate (randomly simulated).\n",
    "\n",
    "At beginning of the comparative study, **run this notebook to set up experiments you want**.\n",
    "\n",
    "Then, **go to the notebook `2_Simulate_errors_and_run_clustering.ipynb` to run and evaluate each experiment you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "- 2.1. **Set up `Dataset` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters for the dataset and pre-format dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of datatset environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_texts.json`: texts from the dataset;\n",
    "        - `dict_of_true_intents.json`: true intent from the dataset;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available datasets_:\n",
    "        - [French trainset for chatbots dealing with usual requests on bank cards v1.0.0](http://doi.org/10.5281/zenodo.4769949)\n",
    "\n",
    "- 2.2. **Set up `Clustering` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters, then preprocess the dataset and vectorize the preprocessed dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of preprocessing + vectorization + clustering environments.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_preprocessed_texts.json`: preprocessed texts computed from `dict_of_texts.json`;\n",
    "        - `config.json`: a json file with all preprocessing parameters.\n",
    "        - `dict_of_vectors.pkl`: vectors computed from `dict_of_preprocessed_texts.json`;\n",
    "    - _Available preprocessing settings_:\n",
    "        - enable preprocessing;\n",
    "        - apply simple preprocessing (lowercase, punctuation, accent, whitespace);\n",
    "        - apply filter on dependency parsing;\n",
    "        - apply lemmatization;\n",
    "        - and all combination.\n",
    "    - _Available vectorization settings_:\n",
    "        - TF-IDF vectorizer;\n",
    "        - spaCy `fr_core_news_md` language model.\n",
    "    - _Available clustering settings_:\n",
    "        - apply constrained kmeans clustering (model COP);\n",
    "        - apply constrained hierarchical clustering (several linkage);\n",
    "        - apply constrained spectral clustering (model SPEC).\n",
    "\n",
    "- 2.3. **Set up `Constraints selection` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters with the random seed and select randomly a set of constraints.\n",
    "    - _Setting_: A dictionary define all possible configurations of constraints selection environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all parameters.\n",
    "        - `list_of_constraints.json`: all selected constraints.\n",
    "    - _Available experiment settings_:\n",
    "        - define the number of constraints.\n",
    "        - define the type of constraints sampling\n",
    "        - define the random seed.\n",
    "\n",
    "- 2.4. **Set up `Errors simulation` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters with the random seed and simulate randomly a set of annotation errors.\n",
    "    - _Setting_: A dictionary define all possible configurations of annotation errors simulation environments.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all parameters.\n",
    "        - `list_of_errors.json`: all selected errors.\n",
    "    - _Available experiment settings_:\n",
    "        - define the rate of errors.\n",
    "        - define the random seed.\n",
    "        - conflicts resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faker\n",
    "import listing_envs\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import pickle  # noqa: S403\n",
    "from cognitivefactory.interactive_clustering.utils.preprocessing import (\n",
    "    preprocess,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.utils.vectorization import (\n",
    "    vectorize,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.constraints.factory import (\n",
    "    managing_factory\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.sampling.factory import (\n",
    "    sampling_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. CREATE COMPUTATION ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.1. Set `Dataset` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATASET_TO_IMPORT: List[str] = [\n",
    "    #'bank_cards_v1'\n",
    "] + [\n",
    "    \"bank_cards_v2-size_{size_str}-rand_{rand_str}\".format(size_str=str(size), rand_str=str(rand))\n",
    "    for size in range(1000, 5001, 500)\n",
    "    for rand in [1]\n",
    "] + [\n",
    "    \"mlsum_fr_train_subset_v1-size_{size_str}-rand_{rand_str}\".format(size_str=str(size), rand_str=str(rand))\n",
    "    for size in range(1000, 5001, 500)\n",
    "    for rand in [1]\n",
    "]\n",
    "print(\"There is\", len(LIST_OF_DATASET_TO_IMPORT), \"datasets to import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `dataset` environments using `ENVIRONMENTS_FOR_DATASETS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for ENV_NAME_dataset in LIST_OF_DATASET_TO_IMPORT:\n",
    "    \n",
    "    # Get first export file to get configs.\n",
    "    first_export_file = os.listdir(\"../previous/\"+ENV_NAME_dataset+\"/\")[0]\n",
    "    \n",
    "    # Load export file.\n",
    "    with open(\"../previous/\"+ENV_NAME_dataset+\"/\"+first_export_file, \"r\") as file_d0:\n",
    "        export_data: Dict[str, Any] = json.load(file_d0)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "    ### ### ### ### ###\n",
    "    \n",
    "    # get the configuration.\n",
    "    CONFIG_dataset: Dict[str, Any] = export_data[\"dataset_config\"]\n",
    "\n",
    "    # Name the configuration.\n",
    "    CONFIG_dataset[\"_ENV_NAME\"] = ENV_NAME_dataset\n",
    "    CONFIG_dataset[\"_ENV_PATH\"] = \"../experiments/\" + ENV_NAME_dataset + \"/\"\n",
    "\n",
    "    # Check if the environment already exists.\n",
    "    if os.path.exists(str(CONFIG_dataset[\"_ENV_PATH\"])):\n",
    "        continue\n",
    "\n",
    "    # Create directory for this environment.\n",
    "    os.mkdir(str(CONFIG_dataset[\"_ENV_PATH\"]))\n",
    "\n",
    "    # Store configuration file.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_d1:\n",
    "        json.dump(CONFIG_dataset, file_d1)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### STORE DATASET.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Store `dict_of_texts` and `dict_of_true_intents`.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_texts.json\", \"w\") as file_d2:\n",
    "        json.dump(export_data[\"dict_of_texts\"], file_d2)\n",
    "\n",
    "    with open(\n",
    "        str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_true_intents.json\", \"w\"\n",
    "    ) as file_d3:\n",
    "        json.dump(export_data[\"dict_of_true_intents\"], file_d3)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Dataset environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.2. Set `algorithm (Preprocessing + Vectorization + Clustering)` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `dataset` environments in which create `algorithm` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of dataset environments.\n",
    "LIST_OF_DATASET_ENVIRONMENTS: List[str] = [\n",
    "    env\n",
    "    for env in listing_envs.get_list_of_dataset_env_paths()\n",
    "    if \"bank_cards_v1\" not in env\n",
    "]\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_DATASET_ENVIRONMENTS)) + \"`\",\n",
    "    \"created dataset environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_DATASET_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_ALGORITHM: Dict[str, Any] = {\n",
    "    # best implementation to reach 80% of V-measure.\n",
    "    \"simple_tfidf_kmeans_cop\": {\n",
    "        \"_TYPE\": \"algorithm\",\n",
    "        \"_DESCRIPTION\": \"Simple preprocessing + TFIDF vectorization + KMeans clustering, {0} clusters, model 'COP'.\",\n",
    "        \"preprocessing\": {\n",
    "            \"apply_preprocessing\": True,\n",
    "            \"apply_lemmatization\": False,\n",
    "            \"apply_parsing_filter\": False,\n",
    "            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "        },\n",
    "        \"vectorization\": {\n",
    "            \"vectorizer_type\": \"tfidf\",\n",
    "            \"spacy_language_model\": None,\n",
    "        },\n",
    "        \"clustering\": {\n",
    "            \"algorithm\": \"kmeans\",\n",
    "            \"init**kargs\": {\n",
    "                \"model\": \"COP\",\n",
    "                \"max_iteration\": 150,\n",
    "                \"tolerance\": 1e-4,\n",
    "            },\n",
    "            \"random_seed\": 42,\n",
    "            #\"nb_clusters\": None,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `preprocessing + vectorization + clustering` environments using `ENVIRONMENTS_FOR_ALGORITHM` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_dataset in LIST_OF_DATASET_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_algorithm,\n",
    "        CONFIG_algorithm,\n",
    "    ) in ENVIRONMENTS_FOR_ALGORITHM.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_algorithm[\"_ENV_NAME\"] = ENV_NAME_algorithm\n",
    "        CONFIG_algorithm[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_dataset + ENV_NAME_algorithm + \"/\"\n",
    "        )\n",
    "        CONFIG_algorithm[\"_DESCRIPTION\"] = CONFIG_algorithm[\"_DESCRIPTION\"].format(nb_clusters)\n",
    "        \n",
    "        # Set number of clusters from the dataset configuration.\n",
    "        with open(PARENT_ENV_PATH_dataset + \"config.json\", \"r\") as file_a0:\n",
    "            nb_clusters = json.load(file_a0)[\"nb_clusters\"]\n",
    "        CONFIG_algorithm[\"clustering\"][\"nb_clusters\"] = nb_clusters\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_algorithm[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_algorithm[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_a1:\n",
    "            json.dump(CONFIG_algorithm, file_a1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### PREPROCESS DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load dataset.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"../dict_of_texts.json\", \"r\"\n",
    "        ) as file_a2:\n",
    "            texts: Dict[str, str] = json.load(file_a2)\n",
    "\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = {}\n",
    "\n",
    "        # Case with preprocessing.\n",
    "        if bool(CONFIG_algorithm[\"preprocessing\"][\"apply_preprocessing\"]):\n",
    "            dict_of_preprocessed_texts = preprocess(\n",
    "                dict_of_texts=texts,\n",
    "                apply_lemmatization=bool(CONFIG_algorithm[\"preprocessing\"][\"apply_lemmatization\"]),\n",
    "                apply_parsing_filter=bool(CONFIG_algorithm[\"preprocessing\"][\"apply_parsing_filter\"]),\n",
    "                spacy_language_model=str(CONFIG_algorithm[\"preprocessing\"][\"spacy_language_model\"]),\n",
    "            )\n",
    "\n",
    "        # Case without preprocessing.\n",
    "        else:\n",
    "            dict_of_preprocessed_texts = texts\n",
    "\n",
    "        # Store preprocessed texts.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"dict_of_preprocessed_texts.json\",\n",
    "            \"w\",\n",
    "        ) as file_a3:\n",
    "            json.dump(dict_of_preprocessed_texts, file_a3)\n",
    "            \n",
    "        ### ### ### ### ###\n",
    "        ### VECTORIZE DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Vectorize dataset.\n",
    "        dict_of_vectors: Dict[str, csr_matrix] = vectorize(\n",
    "            dict_of_texts=dict_of_preprocessed_texts,\n",
    "            vectorizer_type=str(CONFIG_algorithm[\"vectorization\"][\"vectorizer_type\"]),\n",
    "            spacy_language_model=str(CONFIG_algorithm[\"vectorization\"][\"spacy_language_model\"]),\n",
    "        )\n",
    "\n",
    "        # Store vectors.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"dict_of_vectors.pkl\", \"wb\"\n",
    "        ) as file_a4:\n",
    "            pickle.dump(dict_of_vectors, file_a4)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Algorithm environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.3. Set `Constraints selection` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `algorithm` environments in which create `constraints selection` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of algorithm environments.\n",
    "LIST_OF_ALGORITHM_ENVIRONMENTS: List[str] = [\n",
    "    env\n",
    "    for env in listing_envs.get_list_of_algorithm_env_paths()\n",
    "    if \"bank_cards_v1\" not in env\n",
    "]\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_ALGORITHM_ENVIRONMENTS)) + \"`\",\n",
    "    \"created algorithm environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_ALGORITHM_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `constraints selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION: Dict[str, Dict[str, int]] = {\n",
    "    # previous selection from \"1_convergence_study\" experiements with closest neighbors in different clusters sampling.\n",
    "    \"nb_{nb_str}-closest_{rand_str}\".format(nb_str=nb, rand_str=rand): {\n",
    "            \"_TYPE\": \"constraints_selection\",\n",
    "            \"_DESCRIPTION\": \"Selection with closest neighbors in different clusters of {nb_str} constraints (random seed at {rand_str})\".format(nb_str=nb, rand_str=rand),\n",
    "            \"sampling\": \"closest_in_different_clusters\",\n",
    "            \"nb_constraints\": nb,\n",
    "            \"random_seed\": rand,\n",
    "    }\n",
    "    for nb in range(250, 30001, 250)\n",
    "    for rand in [1, 2,]\n",
    "}\n",
    "len(ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `constraints selection` environments using `ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_algorithm in LIST_OF_ALGORITHM_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_constraints_selection,\n",
    "        CONFIG_constraints_selection,\n",
    "    ) in ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        # Get dataset name and number of clusters from the dataset configuration.\n",
    "        with open(PARENT_ENV_PATH_algorithm + \"../config.json\", \"r\") as file_cs0:\n",
    "            config_dataset = json.load(file_cs0)\n",
    "            dataset_name: str = config_dataset[\"_ENV_NAME\"]\n",
    "            nb_clusters: int = config_dataset[\"nb_clusters\"]\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_constraints_selection[\"_ENV_NAME\"] = ENV_NAME_constraints_selection\n",
    "        CONFIG_constraints_selection[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_algorithm + ENV_NAME_constraints_selection + \"/\"\n",
    "        )\n",
    "        CONFIG_constraints_selection[\"previous_sampling_file\"] = CONFIG_constraints_selection[\"previous_sampling_file\"].format(\n",
    "            dataset_str=str(dataset_name),\n",
    "            nb_cluster_str=str(nb_clusters),\n",
    "        )\n",
    "        \n",
    "        # Add previous file sampling if needed\n",
    "        if CONFIG_constraints_selection[\"sampling\"] == \"closest_in_different_clusters\":\n",
    "            CONFIG_constraints_selection[\"previous_sampling_file\"] = \"../previous/{dataset_str}/simple_prep_-_tfidf_-_closest-50_-_kmeans_COP-{nb_cluster_str}c_-_{rand_str}.json\".format(\n",
    "                dataset_str=str(dataset_name),\n",
    "                nb_cluster_str=str(nb_clusters),\n",
    "                rand_str=str(CONFIG_constraints_selection[\"random_seed\"]).zfill(4),\n",
    "            )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_constraints_selection[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_constraints_selection[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_cs1:\n",
    "            json.dump(CONFIG_constraints_selection, file_cs1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### SELECT CONSTRAINTS.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        list_of_sampling: List[Tuple[str, str]] = []\n",
    "        \n",
    "        ###\n",
    "        ### Case of \"closest_in_different_clusters\"\n",
    "        ###\n",
    "        if CONFIG_constraints_selection[\"sampling\"] == \"closest_in_different_clusters\":\n",
    "\n",
    "            # Load previously samplied constraints (from \"1_convergence_study\" experiments).\n",
    "            with open(\n",
    "                str(CONFIG_constraints_selection[\"previous_sampling_file\"]), \"r\"\n",
    "            ) as file_cs2:\n",
    "                dict_of_previous_sampling: Dict[str, List[Tuple[str, str, str]]] = json.load(file_cs2)[\"dict_of_constraints_annotations\"]\n",
    "\n",
    "            # Get constraints.\n",
    "            for iteration in dict_of_previous_sampling.keys():\n",
    "                for sample in dict_of_previous_sampling[iteration]:\n",
    "                    list_of_sampling.append([sample[0], sample[1]])\n",
    "\n",
    "            # Limit constraints size.\n",
    "            list_of_sampling = list_of_sampling[:CONFIG_constraints_selection[\"nb_constraints\"]]\n",
    "\n",
    "        ###\n",
    "        ### Case of \"random\"\n",
    "        ###\n",
    "        else:  # if CONFIG_constraints_selection[\"sampling\"] == \"random\":\n",
    "\n",
    "            # Load preprocess dataset.\n",
    "            with open(\n",
    "                str(CONFIG_constraints_selection[\"_ENV_PATH\"]) + \"../dict_of_preprocessed_texts.json\", \"r\"\n",
    "            ) as file_cs3:\n",
    "                dict_of_preprocessed_texts: Dict[str, str] = json.load(file_cs3)\n",
    "\n",
    "            # Randomly select constraints.\n",
    "            list_of_sampling = sampling_factory(\n",
    "                algorithm=\"random\",\n",
    "                random_seed=CONFIG_constraints_selection[\"random_seed\"],\n",
    "            ).sample(\n",
    "                constraints_manager=managing_factory(\n",
    "                    manager=\"binary\",\n",
    "                    list_of_data_IDs=list(dict_of_preprocessed_texts.keys()),\n",
    "                ),\n",
    "                nb_to_select=CONFIG_constraints_selection[\"nb_constraints\"],\n",
    "            )\n",
    "\n",
    "        ###\n",
    "        ### Store constraints selected.\n",
    "        ###\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"]) + \"list_of_sampling.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs4:\n",
    "            json.dump(list_of_sampling, file_cs4)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Constraints selection environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.4. Set `errors simulation` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `constraints selections` environments in which create `errors simulation` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of constraints selection environments.\n",
    "LIST_OF_CONSTRAINTS_SELECTION_ENVIRONMENTS: List[str] = [\n",
    "    env\n",
    "    for env in listing_envs.get_list_of_constraints_selection_env_paths()\n",
    "    if \"bank_cards_v1\" not in env\n",
    "]\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_CONSTRAINTS_SELECTION_ENVIRONMENTS)) + \"`\",\n",
    "    \"created constraints selection environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_CONSTRAINTS_SELECTION_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `errors simulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_ERRORS_SIMULATION: Dict[str, Dict[str, int]] = {\n",
    "    \"rate_{rate_str:.2f}-rand_{rand_str}-{fix_str}\".format(\n",
    "        rate_str=rate,\n",
    "        rand_str=rand,\n",
    "        fix_str=(\"with_fix\" if fix else \"without_fix\"),\n",
    "    ): {\n",
    "        \"_TYPE\": \"errors_simulation\",\n",
    "        \"_DESCRIPTION\": \"Random simulation of {rate_str:2d}% errors (random seed at {rand_str}) {fix_str}.\".format(\n",
    "            rate_str=int(100*rate),\n",
    "            rand_str=rand,\n",
    "            fix_str=(\"with conflicts fix\" if fix else \"without conflicts fix\")\n",
    "        ),\n",
    "        \"error_rate\": rate,\n",
    "        \"random_seed\": rand,\n",
    "        \"with_fix\": fix,\n",
    "    }\n",
    "    #for rate in [0.0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,]\n",
    "    for rate in [0.0, 0.05, 0.10, 0.15, 0.20, 0.25,]\n",
    "    for rand in [1, 2,]\n",
    "    for fix in [True,]\n",
    "}\n",
    "ENVIRONMENTS_FOR_ERRORS_SIMULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `errors simulation` environments using `ENVIRONMENTS_FOR_ERRORS_SIMULATION` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_constraints_selection in LIST_OF_CONSTRAINTS_SELECTION_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_errors_simulation,\n",
    "        CONFIG_errors_simulation,\n",
    "    ) in ENVIRONMENTS_FOR_ERRORS_SIMULATION.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_errors_simulation[\"_ENV_NAME\"] = ENV_NAME_errors_simulation\n",
    "        CONFIG_errors_simulation[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_constraints_selection + ENV_NAME_errors_simulation + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_errors_simulation[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_errors_simulation[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_errors_simulation[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_p1:\n",
    "            json.dump(CONFIG_errors_simulation, file_p1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### SELECT ERRORS.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load constraints selected.\n",
    "        with open(\n",
    "            str(CONFIG_errors_simulation[\"_ENV_PATH\"]) + \"../list_of_sampling.json\", \"r\"\n",
    "        ) as file_p2:\n",
    "            list_of_sampling: List[Tuple[str, str]] = json.load(file_p2)\n",
    "                \n",
    "        # Randomly select errors.\n",
    "        random.seed(CONFIG_errors_simulation[\"random_seed\"])\n",
    "        list_of_errors: List[Tuple[str, str]] = random.sample(\n",
    "            list_of_sampling,\n",
    "            k=int(len(list_of_sampling)*CONFIG_errors_simulation[\"error_rate\"])\n",
    "        )\n",
    "\n",
    "        # Store error selected.\n",
    "        with open(\n",
    "            str(CONFIG_errors_simulation[\"_ENV_PATH\"]) + \"list_of_errors.json\",\n",
    "            \"w\",\n",
    "        ) as file_p3:\n",
    "            json.dump(list_of_errors, file_p3)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Errors simulation environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 3. Get all created environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[str] = [\n",
    "    env\n",
    "    for env in listing_envs.get_list_of_errors_simulation_env_paths()\n",
    "    if \"bank_cards_v1\" not in env\n",
    "]\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "#LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
