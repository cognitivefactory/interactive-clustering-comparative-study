{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : ANNOTATION ERROR STUDY ====\n",
    "> ### Stage 2 : Perform constraints annotation (with errors simulation and conflict fix), constrained clustering and clustering evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at run all experiment environments, estimate impact of annotation errors during interactive clusterings, plot overviews of experiments and synthesize interactive clustering experiments in a CSV file**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[CLUSTERING]/[CONSTRAINTS_SELECTION]/[ERRORS_SIMULATION]`.\n",
    "- An experiment run is composed of constraints annotation with errors simulation, then constrained clustering and clustering evaluation.\n",
    "\n",
    "Before running, **run the notebook `1_Initialize_annotation_errors_experiments.ipynb` to set up experiments you want**.\n",
    "\n",
    "Then, **go to the notebook `3_Modelize_errors_and_Plot_some_figures.ipynb` to modelize error simulations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, find all experiment environments.\n",
    "- A loop look at all subdirectories in the `/experiments` folder that have a `config.json` file.\n",
    "\n",
    "Then, **apply constraints annotation and constrained clustering** (2.A) for all experiment environments :\n",
    "- Each experiment annotate constraints according to groundtruth, but simulate some errors (cf. error rate). If conflict occurs, we just propagate the infered constraint. Then, a constrained clustering is run.\n",
    "- All computations are stored in the following files:\n",
    "    - constraints in `../experiments/[EXPERIMENT_PATH]/list_of_constraints.json`;\n",
    "    - clustering result `../experiments/[EXPERIMENT_PATH]/dict_of_clustering.json`;\n",
    "    - clustering performances `../experiments/[EXPERIMENT_PATH]/dict_of_clustering_performances.json`;\n",
    "- _NB_:\n",
    "    - The script used to run an experiment is available in the `workerA_run.py` file.\n",
    "    - For these computations, **multiprocessing is used to parallelize tasks**:\n",
    "        - Each environment is represented by a task, and tasks are launched as workers on available logical CPUs.\n",
    "        - The scripts for theses workers are available in the `notebook` directory.\n",
    "        - **WARNING**: _Number of workers should reprensent the number of logical CPU reserved to avoid slow execution._\n",
    "\n",
    "Then, **apply experiment synthesis** (2.C) for all experiments:\n",
    "- Create a CSV file to format evaluations evolutions in order to analyze main effects and post-hoc of interactive clustering convergence speed using a `R` script (cf. notebook `3_Analyze_main_effects_and_post_hoc.ipynb`);\n",
    "- Evolutions are stored in the `../results/experiment_sysnthesis.csv` file.\n",
    "- _NB_:\n",
    "    - The script used to do experiments synthesis is available in the `workerC_synthesis.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import listing_envs\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import tqdm\n",
    "import workerA_run\n",
    "import workerC_synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. RUN ANNOTATION ERROR STUDY EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all experiment environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[\n",
    "    str\n",
    "] = listing_envs.get_list_of_errors_simulation_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.A. Simulate all constraints annotation defined by an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each annotation simulation by a task to launch. Tasks define the run parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# List of run tasks to parallelize.\n",
    "list_of_run_tasks: List[Dict[str, Union[str, int, None]]] = [\n",
    "    {\n",
    "        \"ENV_PATH\": env_to_run,  # Environment of experiment.\n",
    "    }\n",
    "    for counter_of_run_task, env_to_run in enumerate(LIST_OF_EXPERIMENT_ENVIRONMENTS)\n",
    "]\n",
    "print(\"There are\", \"`\" + str(len(list_of_run_tasks)) + \"`\", \"run tasks to launch.\")\n",
    "##### list_of_run_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all defined tasks with `multiprocessing` : Each task is given to a worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of worker (logical CPU).\n",
    "number_of_workers_for_run: int = mp.cpu_count()  # TODO: set it manually !\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(number_of_workers_for_run) + \"`\",\n",
    "    \"logical CPUs used for evaluation experiments.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tasks in parallel.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the pool of workers.\n",
    "    pool_for_run = mp.Pool(number_of_workers_for_run)\n",
    "\n",
    "    # Map the list of tasks with the pool of workers. Show a progress bar with `tqdm`.\n",
    "    for _ in tqdm.tqdm(  # noqa: WPS352\n",
    "        pool_for_run.imap_unordered(workerA_run.experiment_run, list_of_run_tasks),\n",
    "        total=len(list_of_run_tasks),\n",
    "    ):\n",
    "        pass  # noqa: WPS420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.C. Synthesize experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WARNING***: _Start by launching the experiment runs before the experiments synthesis !_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesize performance in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run synthesis computation.\n",
    "workerC_synthesis.experiments_synthesis(\n",
    "    list_of_experiment_environments=LIST_OF_EXPERIMENT_ENVIRONMENTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## NOTA BENE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Show the CPU usage***: `htop`\n",
    "- ***Show disk usage***: `du -sh -- *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh -- ../../*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Zip results***: `tar -czf ../experiments.tar.gz ../experiments/`\n",
    "- ***Unzip results***: `tar -xzf ../experiments.tar.gz -C ../`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
