{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : ANNOTATION ERROR STUDY ====\n",
    "> ### Stage 3 : Modelize annotation errors impact on Interactive Clustering and Plot some figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at modelize annotation errors impact during interactive clusterings, plot overviews of experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET]/[ALGORITHM]/[ERROR]/[CONSTRAINTS_SELECTION]`.\n",
    "\n",
    "Before running, **run the notebook `2_Simulate_errors_and_run_clustering.ipynb` on each algorithm you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, **load experiments** that have been computed with the last notebook.\n",
    "- A config file contains parameters used for each experiment and annotation error simulations to analyze.\n",
    "- For each algorithm and constraints selection method, print performance evolution according to constraints number and error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import json\n",
    "import listing_envs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy import stats as scipystats\n",
    "import statistics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[str] = listing_envs.get_list_of_constraints_selection_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Display evolution of performances according to errors simulations and dataset sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAX_constraints_number(\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get the maximum number of constraints accross experiments.\n",
    "    Return constraint max constraints number.\n",
    "    \n",
    "    Args:\n",
    "        local_LIST_OF_EXPERIMENT_ENVIRONMENTS (List[str]): The list of experiments to consider.\n",
    "    Returns:\n",
    "        str: Maximum constraints number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare histograms.\n",
    "    list_of_stop_case_constraints = []\n",
    "\n",
    "    # For each environment...\n",
    "    for env in local_LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "\n",
    "        # Load clustering performance for the current experiment.\n",
    "        with open(\n",
    "            env + \"dict_of_clustering_performances.json\", \"r\"\n",
    "        ) as evaluations_file:\n",
    "            dict_of_clustering_evaluations: Dict[\n",
    "                str, Dict[str, float]\n",
    "            ] = json.load(evaluations_file)\n",
    "\n",
    "        # Update histogram for convergence.\n",
    "        max_iteration: str = max(dict_of_clustering_evaluations.keys())\n",
    "        list_of_stop_case_constraints.append(max_iteration)\n",
    "\n",
    "    # Return\n",
    "    return str(max(list_of_stop_case_constraints)).zfill(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MEAN_SEM_of_performance_evolution_per_constraints_number(\n",
    "    local_LIST_OF_CONSTRAINTS_NUMBERS: List[str],\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS: List[str],\n",
    ") -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute evolution of performance accross constraints number.\n",
    "    Return Mean and Standard error of the mean evolutions\n",
    "    \n",
    "    Args:\n",
    "        local_LIST_OF_CONSTRAINTS_NUMBERS (List[str]): The list of constraints number to consider.\n",
    "        local_LIST_OF_EXPERIMENT_ENVIRONMENTS (List[str]): The list of experiments to consider.\n",
    "    Returns:\n",
    "        Tuple[Dict[str, float], Dict[str, float]]: Evolutions of Mean and Standard error of the mean accross constraints number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize storage of experiment performances for all constraints number.\n",
    "    dict_of_global_performances_evolution_per_constraints_number: Dict[str, List[float]] = {\n",
    "        constraints_number: [] for constraints_number in local_LIST_OF_CONSTRAINTS_NUMBERS\n",
    "    }\n",
    "    # Initialize storage of performance mean for all constraints number.\n",
    "    dict_of_global_performances_evolution_per_constraints_number_MEAN: Dict[str, float] = {\n",
    "        constraints_number: 0 for constraints_number in local_LIST_OF_CONSTRAINTS_NUMBERS\n",
    "    }\n",
    "    # Initialize storage of performance standard error of the mean for all constraints number.\n",
    "    dict_of_global_performances_evolution_per_constraints_number_SEM: Dict[str, float] = {\n",
    "        constraints_number: 0 for constraints_number in local_LIST_OF_CONSTRAINTS_NUMBERS\n",
    "    }\n",
    "\n",
    "    # For each experiment...\n",
    "    for env_a in local_LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "\n",
    "        # Load clustering evaluations.\n",
    "        with open(\n",
    "            env_a + \"dict_of_clustering_performances.json\", \"r\"\n",
    "        ) as evaluation_file:\n",
    "            dict_of_clustering_performances: Dict[str, float] = json.load(evaluation_file)\n",
    "        \n",
    "        # For each requested constraints number...\n",
    "        max_constraints_a: str = local_LIST_OF_CONSTRAINTS_NUMBERS[0]\n",
    "        for nb_constraints_a in local_LIST_OF_CONSTRAINTS_NUMBERS:\n",
    "\n",
    "            # Append the clustering performancre for the current experiment and for this constraints number.\n",
    "            if nb_constraints_a in dict_of_clustering_performances.keys():\n",
    "                max_constraints_a = nb_constraints_a\n",
    "                dict_of_global_performances_evolution_per_constraints_number[nb_constraints_a].append(\n",
    "                    dict_of_clustering_performances[nb_constraints_a][\"v_measure\"]\n",
    "                )\n",
    "            # If iteration isn't reached by this experiment, duplicate the last known results.\n",
    "            # Most of the time: the experiment has reached annotation completeness and there is no more iteration because clustering is \"perfect\" (v-measure==1.0).\n",
    "            else:\n",
    "                dict_of_global_performances_evolution_per_constraints_number[nb_constraints_a].append(\n",
    "                    dict_of_clustering_performances[max_constraints_a][\"v_measure\"]\n",
    "                )\n",
    "\n",
    "    # Compute mean and sem of performance for each constraints number.\n",
    "    for nb_constraints_b in dict_of_global_performances_evolution_per_constraints_number.keys():\n",
    "\n",
    "        # Compute mean of performance for this constraints number.\n",
    "        dict_of_global_performances_evolution_per_constraints_number_MEAN[nb_constraints_b] = np.mean(dict_of_global_performances_evolution_per_constraints_number[nb_constraints_b])\n",
    "        # Compute sem of performance for this constraints number.\n",
    "        dict_of_global_performances_evolution_per_constraints_number_SEM[nb_constraints_b] = scipystats.sem(dict_of_global_performances_evolution_per_constraints_number[nb_constraints_b])\n",
    "        \n",
    "    # Return.\n",
    "    return (\n",
    "        dict_of_global_performances_evolution_per_constraints_number_MEAN,\n",
    "        dict_of_global_performances_evolution_per_constraints_number_SEM\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plot_of_performance_evolution_per_constraints_number_to_graph(\n",
    "    axis,\n",
    "    list_of_x: List[str],\n",
    "    dict_of_y: Dict[str, float],\n",
    "    dict_of_y_err: Optional[Dict[str, float]] = None,\n",
    "    label: str = \"\",\n",
    "    label_in_curve: Optional[str] = None,\n",
    "    marker: str = \"\",\n",
    "    markersize: int = 5,\n",
    "    color: str = \"black\",\n",
    "    linewidth: int = 2,\n",
    "    linestyle: str = \"-\",\n",
    "    alpha: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a plot to an axis of a graph.\n",
    "    \n",
    "    Args:\n",
    "        axis (): TODO.\n",
    "        list_of_x (List[str]): TODO.\n",
    "        dict_of_y (Dict[str, float]): TODO.\n",
    "        dict_of_y_err (Optional[Dict[str, float]]): TODO. Defaults to `None`.\n",
    "        label (str): TODO. Defaults to `\"\"`.\n",
    "        label_in_curve (Optional[str]): TODO. Defaults to `None`.\n",
    "        marker (str): TODO. Defaults to `\"\"`.\n",
    "        markersize (int): TODO. Defaults to `5`.\n",
    "        color (str): TODO. Defaults to `\"black\"`.\n",
    "        linewidth (int): TODO. Defaults to `2`.\n",
    "        linestyle (str): TODO. Defaults to `\"-\"`.\n",
    "        alpha (float): TODO. Defaults to `0.2`.\n",
    "    \"\"\"\n",
    "    # Add curve.\n",
    "    axis.plot(\n",
    "        [int(x) for x in list_of_x],  # x\n",
    "        [dict_of_y[x] for x in list_of_x],  # y\n",
    "        label=label,\n",
    "        marker=marker,\n",
    "        markerfacecolor=color,\n",
    "        markersize=markersize,\n",
    "        color=color,\n",
    "        linewidth=linewidth,\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    # Add curve name.\n",
    "    if label_in_curve is not None:\n",
    "        axis.text(\n",
    "            x=int(list_of_x[-1]),\n",
    "            y=dict_of_y[list_of_x[-1]],\n",
    "            s=label_in_curve,\n",
    "        )\n",
    "    # Add curve error bars.\n",
    "    if dict_of_y_err is not None:\n",
    "        axis.fill_between(\n",
    "            [int(x) for x in list_of_x],  # x\n",
    "            y1=[(dict_of_y[x] - dict_of_y_err[x]) for x in list_of_x],  # y1\n",
    "            y2=[(dict_of_y[x] + dict_of_y_err[x]) for x in list_of_x],  # y2\n",
    "            color=color,\n",
    "            alpha=alpha,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MEAN_SEM_accordance_between_clusterings(\n",
    "    constraints_number_id: str,\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS_REFERENCE: List[str],\n",
    "    local_LIST_OF_EXPERIMENT_ENVIRONMENTS: List[str],\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute accordance between clusterings.\n",
    "    Return Mean and Standard error of the mean accordance\n",
    "    \n",
    "    Args:\n",
    "        constraints_number_id (str): The iteration to analyze.\n",
    "        local_LIST_OF_EXPERIMENT_ENVIRONMENTS_REFERENCE (List[str]):  The list of experiments to consider as reference.\n",
    "        local_LIST_OF_EXPERIMENT_ENVIRONMENTS (List[str]): The list of experiments to consider.\n",
    "    Returns:\n",
    "        Tuple[float, float]: Clustering accordance Mean and Standard error.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage of clustering accordance (2 by 2).\n",
    "    list_of_clustering_vmeasures: List[float] = []\n",
    "    \n",
    "    # For all environment of reference.\n",
    "    for env_reference in local_LIST_OF_EXPERIMENT_ENVIRONMENTS_REFERENCE:\n",
    "        \n",
    "        # Load reference clustering results\n",
    "        with open(\n",
    "            env_reference + \"dict_of_clustering_results.json\", \"r\"\n",
    "        ) as clustering_reference_file:\n",
    "            dict_of_clustering_results_reference: Dict[str, Dict[str, int]] = json.load(clustering_reference_file)\n",
    "        clustering_reference: Dict[str, float] = (\n",
    "            dict_of_clustering_results_reference[constraints_number_id]\n",
    "            if constraints_number_id in dict_of_clustering_results_reference.keys()\n",
    "            else dict_of_clustering_results_reference[sorted(dict_of_clustering_results_reference.keys())[-1]]\n",
    "        )\n",
    "        \n",
    "        # For all environment to compare.\n",
    "        for env_comparison in local_LIST_OF_EXPERIMENT_ENVIRONMENTS:\n",
    "            \n",
    "            # Check that environments use the sam dataset.\n",
    "            if env_reference.split(\"/\")[2] != env_comparison.split(\"/\")[2]:\n",
    "                continue\n",
    "            \n",
    "            # Load clustering results to compare.\n",
    "            with open(\n",
    "                env_comparison + \"dict_of_clustering_results.json\", \"r\"\n",
    "            ) as clustering_comparison_file:\n",
    "                dict_of_clustering_results_comparison: Dict[str, Dict[str, int]] = json.load(clustering_comparison_file)\n",
    "            clustering_comparison: Dict[str, float] = (\n",
    "                dict_of_clustering_results_comparison[constraints_number_id]\n",
    "                if constraints_number_id in dict_of_clustering_results_comparison.keys()\n",
    "                else dict_of_clustering_results_comparison[sorted(dict_of_clustering_results_comparison.keys())[-1]]\n",
    "            )\n",
    "                    \n",
    "            # Compare clustering and add accordance.\n",
    "            list_of_clustering_vmeasures.append(\n",
    "                metrics.v_measure_score(\n",
    "                    [clustering_reference[data_id] for data_id in clustering_reference.keys()],\n",
    "                    [clustering_comparison[data_id] for data_id in clustering_reference.keys()],\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Return mean and standard error of the mean of clustering accordance\n",
    "    return np.mean(list_of_clustering_vmeasures), scipystats.sem(list_of_clustering_vmeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define configuration for graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.\n",
    "LIST_OF_DATASET_SIZES: List[int] = [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "RATIO_CONSTRAINTS_NEEDED: float = 3.15\n",
    "RATIO_STD_CONSTRAINTS_NEEDED: float = 0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate.\n",
    "LIST_OF_ERROR_RATES: List[float] = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]\n",
    "MAX_ERROR_RATE: float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color map.\n",
    "list_of_colors = matplotlib.colormaps[\"RdYlGn\"](\n",
    "    np.linspace(1, 0, len(LIST_OF_ERROR_RATES))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for makers.\n",
    "markers: Dict[float, str] = {error_rate: \".\" for error_rate in LIST_OF_ERROR_RATES}\n",
    "markers[0.00] = \"^\"\n",
    "    #markers[0.50] = \"v\"\n",
    "# Config for maker sizes.\n",
    "markersizes: Dict[float, int] = {error_rate: 5 for error_rate in LIST_OF_ERROR_RATES}\n",
    "markersizes[0.00] = 5\n",
    "    #markersizes[0.50] = 5\n",
    "# Config for linewidths.\n",
    "linewidths: Dict[float, int] = {error_rate: 1 for error_rate in LIST_OF_ERROR_RATES}\n",
    "linewidths[0.00] = 1\n",
    "    #linewidths[0.50] = 1\n",
    "# Config for linestyles.\n",
    "linestyles: Dict[float, str] = {error_rate: \"-\" for error_rate in LIST_OF_ERROR_RATES}\n",
    "linestyles[0.00] = \"-\"\n",
    "    #linestyles[0.50] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evolution of performances per dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize dataframe of performances review.\n",
    "df_performances_evolution: Dict[int, Dict[float, float]] = {}\n",
    "    \n",
    "# Initialize dataframe of clustering accordance review.\n",
    "df_accordance_evolution: Dict[int, Dict[float, float]] = {}\n",
    "\n",
    "# For all dataset size...\n",
    "for DATASET_SIZE in LIST_OF_DATASET_SIZES:\n",
    "    print(\"-----\")\n",
    "    print(\"ANALYZE DATASET WITH SIZE {0}\".format(DATASET_SIZE))\n",
    "\n",
    "    # Get list of environments to analyse.\n",
    "    LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE = [\n",
    "        env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS\n",
    "        if (\"size_{size}\".format(size=DATASET_SIZE) in env.split(\"/\")[2])\n",
    "    ]\n",
    "        \n",
    "    ###\n",
    "    ### ANALYZE PERFORMANCE EVOLUTION AND ERROR IMPACT\n",
    "    ###\n",
    "\n",
    "    # Get maximum constrains number for these experiments.\n",
    "    MAX_CONSTRAINTS_NUMBER: str = get_MAX_constraints_number(\n",
    "        LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE\n",
    "    )\n",
    "    MAX_CONSTRAINTS_NUMBER = str(min(int(MAX_CONSTRAINTS_NUMBER), 10*DATASET_SIZE)).zfill(6)\n",
    "    LIST_OF_CONSTRAINTS_NUMBERS: List[str] = [\n",
    "        str(i).zfill(6)\n",
    "        for i in range(0, int(MAX_CONSTRAINTS_NUMBER)+250, 250)\n",
    "    ]\n",
    "        \n",
    "    # Compute default performance.\n",
    "    performances_MEAN_default, performances_SEM_default = get_MEAN_SEM_of_performance_evolution_per_constraints_number(\n",
    "        local_LIST_OF_CONSTRAINTS_NUMBERS=LIST_OF_CONSTRAINTS_NUMBERS,\n",
    "        local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "            env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE\n",
    "            if (\"rate_0.00\" in env.split(\"/\")[4])\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Get theorical number of constraints needed to reach 90% of v-measure.\n",
    "    #THEORICAL_CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE: str = str(round(RATIO_CONSTRAINTS_NEEDED * DATASET_SIZE/250)*250).zfill(6)\n",
    "    #print(\"   \", \"Theorique:\", THEORICAL_CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE)\n",
    "        \n",
    "    # Get number of constraints needed to reach 90% of v-measure.\n",
    "    CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE: str = sorted(performances_MEAN_default.keys())[-1]\n",
    "    for constraint_number, performance in performances_MEAN_default.items():\n",
    "        if 0.90 <= performance:\n",
    "            CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE = constraint_number\n",
    "            break\n",
    "    print(\"   \", \"Reel:\", CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE)\n",
    "    \n",
    "    # Initialize dataframe of performances review for this dataset size.\n",
    "    df_performances_evolution[\n",
    "        \"size:{0}\".format(DATASET_SIZE)\n",
    "    ] = {\n",
    "        \"constraints_needed\": int(CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE)\n",
    "    }\n",
    "    \n",
    "    # Initialize dataframe of clustering accordance review.\n",
    "    df_accordance_evolution[\n",
    "        \"size:{0}\".format(DATASET_SIZE)\n",
    "    ] = {\n",
    "        \"constraints_needed\": int(CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE)\n",
    "    }\n",
    "\n",
    "        \n",
    "    ###\n",
    "    ### PLOT FIGURE WITH PERFORMANCE EVOLUTION\n",
    "    ###\n",
    "    \n",
    "    # Create a new figure.\n",
    "    fig_plot_error_simulation: Figure = plt.figure(figsize=(15, 7.5), dpi=300)\n",
    "    axis_plot_error_simulation = fig_plot_error_simulation.gca()\n",
    "\n",
    "    # Set range of axis.\n",
    "    axis_plot_error_simulation.set_xlim(xmin=-250, xmax=int(MAX_CONSTRAINTS_NUMBER)+250)\n",
    "    axis_plot_error_simulation.set_ylim(ymin=-0.01, ymax=1.01)\n",
    "\n",
    "    # Plot error simulation.\n",
    "    for k, error_rate_k in enumerate(LIST_OF_ERROR_RATES):\n",
    "        if error_rate_k > MAX_ERROR_RATE:\n",
    "            continue\n",
    "\n",
    "        # Compute performance MEAN and SEM for this error rate.\n",
    "        performances_MEAN, performances_SEM = get_MEAN_SEM_of_performance_evolution_per_constraints_number(\n",
    "            local_LIST_OF_CONSTRAINTS_NUMBERS=LIST_OF_CONSTRAINTS_NUMBERS,\n",
    "            local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "                env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE\n",
    "                if (\"rate_{rate:.2f}\".format(rate=error_rate_k) in env.split(\"/\")[4])\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        # Complete dataframe of performances review for this dataset size.\n",
    "        df_performances_evolution[\n",
    "            \"size:{0}\".format(DATASET_SIZE)\n",
    "        ][\n",
    "            \"error:{0:.2f}%\".format(error_rate_k*100)\n",
    "        ] = (\n",
    "            \"{0:.2f} (+/-{1:.2f})\".format(\n",
    "                performances_MEAN[CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE]*100,\n",
    "                performances_SEM[CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE]*100,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Complete dataframe of clustering accordance review for this dataset size.\n",
    "        accordance_MEAN, accordance_SEM = get_MEAN_SEM_accordance_between_clusterings(\n",
    "            constraints_number_id=CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE,\n",
    "            local_LIST_OF_EXPERIMENT_ENVIRONMENTS_REFERENCE=[\n",
    "                env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE\n",
    "                if (\"rate_0.00\" in env.split(\"/\")[4])\n",
    "            ],\n",
    "            local_LIST_OF_EXPERIMENT_ENVIRONMENTS=[\n",
    "                env for env in LIST_OF_EXPERIMENT_ENVIRONMENTS_WITH_SPECIFIC_SIZE\n",
    "                if (\"rate_{rate:.2f}\".format(rate=error_rate_k) in env.split(\"/\")[4])\n",
    "            ],\n",
    "        )\n",
    "        df_accordance_evolution[\n",
    "            \"size:{0}\".format(DATASET_SIZE)\n",
    "        ][\n",
    "            \"error:{0:.2f}%\".format(error_rate_k*100)\n",
    "        ] = (\n",
    "            \"{0:.2f} (+/-{1:.2f})\".format(\n",
    "                accordance_MEAN*100,\n",
    "                accordance_SEM*100\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add plot.\n",
    "        add_plot_of_performance_evolution_per_constraints_number_to_graph(\n",
    "            axis=axis_plot_error_simulation,\n",
    "            list_of_x=LIST_OF_CONSTRAINTS_NUMBERS,\n",
    "            dict_of_y=performances_MEAN,\n",
    "            dict_of_y_err=performances_SEM,\n",
    "            label=\"{rate:2d}% d'erreurs\".format(rate=int(error_rate_k*100)),\n",
    "            label_in_curve=\"{rate:2d}%\".format(rate=int(error_rate_k*100)),\n",
    "            marker=markers[error_rate_k],\n",
    "            markersize=markersizes[error_rate_k],\n",
    "            color=list_of_colors[k],\n",
    "            linewidth=linewidths[error_rate_k],\n",
    "            linestyle=linestyles[error_rate_k],\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        \n",
    "    # Plot number of constraints needed to reach 90% of v-measure.\n",
    "    axis_plot_error_simulation.vlines(\n",
    "        x=int(CONSTRAINTS_NEEDED_TO_REACH_90VMEASURE),\n",
    "        ymin=-0.00,\n",
    "        ymax=1.00,\n",
    "        label=\"seuil 90% v-measure\",\n",
    "        colors=\"black\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\"\n",
    "    )\n",
    "\n",
    "    # Set axis name.\n",
    "    axis_plot_error_simulation.set_xlabel(\"nombre de contraintes [#]\", fontsize=18,)\n",
    "    axis_plot_error_simulation.set_ylabel(\"v-measure [%]\", fontsize=18,)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    # Plot the legend.\n",
    "    axis_plot_error_simulation.legend(ncol=2, loc=\"lower right\", fontsize=12,)\n",
    "\n",
    "    # Plot the grid.\n",
    "    axis_plot_error_simulation.grid(True)\n",
    "\n",
    "    # Store the graph.\n",
    "    fig_plot_error_simulation.savefig(\n",
    "        \"../results/etude-erreur-simulation-impact-size-{size}.png\".format(size=DATASET_SIZE),\n",
    "        dpi=300,\n",
    "        transparent=True,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display performance delay of error environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(df_performances_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"../results/etude-erreur-simulation-evolution-performances.xlsx\") as writer:  \n",
    "    pd.DataFrame.from_dict(df_performances_evolution).to_excel(\n",
    "        writer,\n",
    "        sheet_name=\"performance\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display clustering accordance (between non-error and error environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(df_accordance_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"../results/etude-erreur-simulation-evolution-accord-entre-clusterings.xlsx\") as writer:  \n",
    "    pd.DataFrame.from_dict(df_accordance_evolution).to_excel(\n",
    "        writer,\n",
    "        sheet_name=\"clustering_accordance\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFT. Analyse of 90% vmeasure iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPION: env, dataset_name, dataset_size, error_rate\n",
    "# ANALYSE: constraints_needed\n",
    "# krippendorffs_alpha, completude, vmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = \"../experiments/mlsum_fr_train_subset_v1-size_5000-rand_1/simple_tfidf_kmeans_cop/rate_0.05-rand_1-with_fix/closest_250_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dict of true intents.\n",
    "with open(\n",
    "    env_test + \"../../../dict_of_true_intents.json\", \"r\"\n",
    ") as file_true_intents:\n",
    "    dict_of_true_intents: Dict[str, str] = json.load(file_true_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of constraints sampled.\n",
    "with open(\n",
    "    env_test + \"dict_of_samplings.json\", \"r\"\n",
    ") as file_sampling_r:\n",
    "    dict_of_samplings: Dict[str, List[Tuple[str, str]]] = json.load(file_sampling_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of effective constraints in manager.\n",
    "with open(env_test + \"dict_of_constraints_effective.json\", \"r\") as file_constraints_r:\n",
    "    dict_of_constraints_effective: Dict[str, List[Tuple[str, str]]] = json.load(file_constraints_r)\n",
    "    new_dict_of_constraints_effective = {\n",
    "        c: {\n",
    "            \"({0},{1})\".format(x[0], x[1]): x\n",
    "            for x in l\n",
    "        }\n",
    "        for c, l in dict_of_constraints_effective.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_constraints_numbers: List[str] = sorted(dict_of_samplings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            constraints_number,\n",
    "            \"simulation\",\n",
    "            \"({0},{1})\".format(annotation_id0, annotation_id1),\n",
    "            new_dict_of_constraints_effective[constraints_number][\"({0},{1})\".format(annotation_id0, annotation_id1)][2]\n",
    "        ]\n",
    "        for constraints_number in list_of_constraints_numbers\n",
    "        for annotation_id0, annotation_id1 in dict_of_samplings[constraints_number][-250:]\n",
    "    ] + [\n",
    "        [\n",
    "            constraints_number,\n",
    "            \"groundtruth\",\n",
    "            \"({0},{1})\".format(annotation_id0, annotation_id1),\n",
    "            (\n",
    "                \"MUST_LINK\"\n",
    "                if (dict_of_true_intents[annotation_id0] == dict_of_true_intents[annotation_id1])\n",
    "                else \"CANNOT_LINK\"\n",
    "            )\n",
    "        ]\n",
    "        for constraints_number in list_of_constraints_numbers\n",
    "        for annotation_id0, annotation_id1 in dict_of_samplings[constraints_number][-250:]\n",
    "    ],\n",
    "    columns=[\"iteration\", \"annotator\", \"annotation_id\", \"annotation\"]\n",
    ")\n",
    "df_annotations.head(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in sorted(df_annotations[\"iteration\"].unique()):\n",
    "    \n",
    "    df_annotations_small = df_annotations[df_annotations[\"iteration\"]==iteration]\n",
    "    df_annotations_small_simulation = df_annotations_small[df_annotations_small[\"annotator\"]==\"simulation\"]\n",
    "    df_annotations_small_groundtruth = df_annotations_small[df_annotations_small[\"annotator\"]==\"groundtruth\"]\n",
    "    if list(df_annotations_small_simulation[\"annotation\"]) == list(df_annotations_small_groundtruth[\"annotation\"]):\n",
    "        print(iteration, \"--\", 1.0)\n",
    "        continue\n",
    "    \n",
    "    krippendorffs_alpha: float = simpledorff.calculate_krippendorffs_alpha_for_df(\n",
    "        df_annotations_small,\n",
    "        experiment_col=\"annotation_id\",\n",
    "        annotator_col=\"annotator\",\n",
    "        class_col=\"annotation\",\n",
    "    )\n",
    "        \n",
    "    accordance_status: str = (\n",
    "        \"low\" if krippendorffs_alpha<0.667\n",
    "        else (\"acceptable\" if krippendorffs_alpha<0.8\n",
    "        else \"good\")\n",
    "    )\n",
    "        \n",
    "    print(iteration, \"--\", \"Krippendorff's alpha: {0:.3f} (accordance='{1}')\".format(krippendorffs_alpha, accordance_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = list(df_annotations[df_annotations[\"iteration\"]==\"028000\"][df_annotations[\"annotator\"]==\"groundtruth\"][\"annotation\"])\n",
    "x1 = list(df_annotations[df_annotations[\"iteration\"]==\"028000\"][df_annotations[\"annotator\"]==\"simulation\"][\"annotation\"])\n",
    "print(len([x for x in x1 if x==\"CANNOT_LINK\" ]))\n",
    "for i in range(len(x1)):\n",
    "    if x2[i] != x1[i]:\n",
    "        print(x1[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "krippendorffs_alpha: float = simpledorff.calculate_krippendorffs_alpha_for_df(\n",
    "    df_annotations,\n",
    "    experiment_col=\"annotation_id\",\n",
    "    annotator_col=\"annotator\",\n",
    "    class_col=\"annotation\",\n",
    ")\n",
    "accordance_status: str = (\n",
    "    \"low\" if krippendorffs_alpha<0.667\n",
    "    else (\"acceptable\" if krippendorffs_alpha<0.8\n",
    "    else \"good\")\n",
    ")\n",
    "print(\"Krippendorff's alpha: {0:.3f} (accordance='{1}')\".format(krippendorffs_alpha, accordance_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
