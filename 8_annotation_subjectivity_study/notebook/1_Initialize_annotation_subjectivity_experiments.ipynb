{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : ANNOTATION SUBJECTIVITY STUDY ====\n",
    "> ### Stage 1 : Initialize computation environments for experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at create environments needed to run annotation subjectivity study experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder. A full path to an experiment environment is `/experiments/[DATASET_ALGORITHM]/[ERROR]/[CONSTRAINTS_SELECTION]`.\n",
    "- The path is composed of A. the dataset and the clustering used (cf. convergence study results to get the \"best implementation\"), B. the errors rate (randomly picked), and C. constraints selection (closest in different clusters, reuse previous selections without error).\n",
    "\n",
    "At beginning of the comparative study, **run this notebook to set up experiments you want**.\n",
    "\n",
    "Then, **go to the notebook `2_Simulate_errors_and_run_clustering.ipynb` to run and evaluate each experiment you have set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "- 2.1. **Set up `Dataset` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters for the dataset and pre-format dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible configurations of datatset environments.\n",
    "        - dataset size;\n",
    "        - number of clusters;\n",
    "        - excel columns to use;\n",
    "        - random seed for faker generation.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_texts.json`: texts from the dataset;\n",
    "        - `dict_of_true_intents.json`: true intent from the dataset;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "    - _Available datasets_:\n",
    "        - [French trainset for chatbots dealing with usual requests on bank cards v2.0.0](http://doi.org/10.5281/zenodo.4769949)\n",
    "        - [MLSUM: The Multilingual Summarization Corpus](https://arxiv.org/abs/2004.14900v1), subsetted and filtered by SCHILD E. (v1.0.0).\n",
    "     \n",
    "- 2.2. **Set up `Algorithm` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters for algorithms, then preprocess the dataset and vectorize the preprocessed dataset for next computations.\n",
    "    - _Setting_: A dictionary define all possible preprocessing + vectorization + clustering environments.\n",
    "        - _preprocessing_: simple_prep;\n",
    "        - _vectorization_: tfidf;\n",
    "        - _clustering_: kmeans_cop.\n",
    "    - _Folder content_:\n",
    "        - `dict_of_preprocessed_texts.json`: preprocessed texts computed from `dict_of_texts.json`;\n",
    "        - `dict_of_vectors.pkl`: vectors computed from `dict_of_preprocessed_texts.json`;\n",
    "        - `config.json`: a json file with all parameters.\n",
    "\n",
    "- 2.3. **Set up `Errors` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters with the random seed for error simulation.\n",
    "    - _Setting_: A dictionary define all possible configurations of annotation errors simulation environments.\n",
    "        - define the rate of errors.\n",
    "        - define the random seed for simulation.\n",
    "        - define conflicts resolution method.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all parameters.\n",
    "\n",
    "- 2.3. **Set up `Constraints selection` environments**:\n",
    "    - _Description_: Create a subdirectory, store parameters with the random seed and select constraints from previous experiments.\n",
    "    - _Setting_: A dictionary define all possible configurations of constraints selection environments.\n",
    "        - define the max number of constraints ;\n",
    "        - define the random seed for selection.\n",
    "        - get previous experiment sampling, then use closest_in_diff sampling.\n",
    "    - _Folder content_:\n",
    "        - `config.json`: a json file with all parameters.\n",
    "        - `previous_sampling.json`: all selected constraints.\n",
    "        - `dict_of_samplings.json`: all selected constraints.\n",
    "        - `dict_of_errors.json`: all selected errors.\n",
    "        - `dict_of_constraints_effective.json`: all effective constraintes (after conflict resolution).\n",
    "        - `dict_of_clustering_results.json`: all clustering results.\n",
    "        - `dict_of_clustering_performances.json`: all clustering performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faker\n",
    "import listing_envs\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle  # noqa: S403\n",
    "from cognitivefactory.interactive_clustering.utils.preprocessing import (\n",
    "    preprocess,\n",
    ")\n",
    "from cognitivefactory.interactive_clustering.utils.vectorization import (\n",
    "    vectorize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## 2. CREATE COMPUTATION ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.1. Set `Dataset` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATASET_TO_IMPORT: List[str] = [\n",
    "    #'bank_cards_v1'\n",
    "] + [\n",
    "    \"bank_cards_v2-size_{size_str}-rand_{rand_str}\".format(size_str=str(size), rand_str=str(rand))\n",
    "    for size in range(1000, 5001, 500)\n",
    "    for rand in [1]\n",
    "] + [\n",
    "    \"mlsum_fr_train_subset_v1-size_{size_str}-rand_{rand_str}\".format(size_str=str(size), rand_str=str(rand))\n",
    "    for size in range(1000, 5001, 500)\n",
    "    for rand in [1]\n",
    "]\n",
    "print(\"There is\", len(LIST_OF_DATASET_TO_IMPORT), \"datasets to import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `dataset` environments using `ENVIRONMENTS_FOR_DATASETS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for ENV_NAME_dataset in LIST_OF_DATASET_TO_IMPORT:\n",
    "    \n",
    "    # Get first export file to get configs.\n",
    "    first_export_file = os.listdir(\"../previous/\"+ENV_NAME_dataset+\"/\")[0]\n",
    "    \n",
    "    # Load export file.\n",
    "    with open(\"../previous/\"+ENV_NAME_dataset+\"/\"+first_export_file, \"r\") as file_d0:\n",
    "        export_data: Dict[str, Any] = json.load(file_d0)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "    ### ### ### ### ###\n",
    "    \n",
    "    # get the configuration.\n",
    "    CONFIG_dataset: Dict[str, Any] = export_data[\"dataset_config\"]\n",
    "\n",
    "    # Name the configuration.\n",
    "    CONFIG_dataset[\"_ENV_NAME\"] = ENV_NAME_dataset\n",
    "    CONFIG_dataset[\"_ENV_PATH\"] = \"../experiments/\" + ENV_NAME_dataset + \"/\"\n",
    "\n",
    "    # Check if the environment already exists.\n",
    "    if os.path.exists(str(CONFIG_dataset[\"_ENV_PATH\"])):\n",
    "        continue\n",
    "\n",
    "    # Create directory for this environment.\n",
    "    os.mkdir(str(CONFIG_dataset[\"_ENV_PATH\"]))\n",
    "\n",
    "    # Store configuration file.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"config.json\", \"w\") as file_d1:\n",
    "        json.dump(CONFIG_dataset, file_d1)\n",
    "\n",
    "    ### ### ### ### ###\n",
    "    ### STORE DATASET.\n",
    "    ### ### ### ### ###\n",
    "\n",
    "    # Store `dict_of_texts` and `dict_of_true_intents`.\n",
    "    with open(str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_texts.json\", \"w\") as file_d2:\n",
    "        json.dump(export_data[\"dict_of_texts\"], file_d2)\n",
    "\n",
    "    with open(\n",
    "        str(CONFIG_dataset[\"_ENV_PATH\"]) + \"dict_of_true_intents.json\", \"w\"\n",
    "    ) as file_d3:\n",
    "        json.dump(export_data[\"dict_of_true_intents\"], file_d3)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Dataset environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.2. Set `algorithm (Preprocessing + Vectorization + Clustering)` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `dataset` environments in which create `algorithm` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of dataset environments.\n",
    "LIST_OF_DATASET_ENVIRONMENTS: List[str] = listing_envs.get_list_of_dataset_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_DATASET_ENVIRONMENTS)) + \"`\",\n",
    "    \"created dataset environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_DATASET_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_ALGORITHM: Dict[str, Any] = {\n",
    "    # best implementation to reach 80% of V-measure.\n",
    "    \"simple_tfidf_kmeans_cop\": {\n",
    "        \"_TYPE\": \"algorithm\",\n",
    "        \"_DESCRIPTION\": \"Simple preprocessing + TFIDF vectorization + KMeans clustering.\",\n",
    "        \"preprocessing\": {\n",
    "            \"_NAME\": \"simple_prep\",\n",
    "            \"apply_preprocessing\": True,\n",
    "            \"apply_lemmatization\": False,\n",
    "            \"apply_parsing_filter\": False,\n",
    "            \"spacy_language_model\": \"fr_core_news_md\",\n",
    "        },\n",
    "        \"vectorization\": {\n",
    "            \"_NAME\": \"tfidf\",\n",
    "            \"vectorizer_type\": \"tfidf\",\n",
    "            \"spacy_language_model\": None,\n",
    "        },\n",
    "        \"clustering\": {\n",
    "             \"_TEMPNAME\": \"kmeans_COP-{0}c\",\n",
    "            \"algorithm\": \"kmeans\",\n",
    "            \"init**kargs\": {\n",
    "                \"model\": \"COP\",\n",
    "                \"max_iteration\": 150,\n",
    "                \"tolerance\": 1e-4,\n",
    "            },\n",
    "            \"random_seed\": 42,\n",
    "            #\"nb_clusters\": None,  # define by dataset.\n",
    "        },\n",
    "        \"previous_sampling\": {\n",
    "            \"_NAME\": \"closest-50\",\n",
    "            \"algorithm\": \"closest_in_different_clusters\",\n",
    "            \"nb_to_select\": 50,\n",
    "            #\"random_seed\": None,  # define by constraints selection.\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `preprocessing + vectorization + clustering` environments using `ENVIRONMENTS_FOR_ALGORITHM` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_dataset in LIST_OF_DATASET_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_algorithm,\n",
    "        CONFIG_algorithm2,\n",
    "    ) in ENVIRONMENTS_FOR_ALGORITHM.items():\n",
    "        \n",
    "        # Make copy of configuration.\n",
    "        CONFIG_algorithm = CONFIG_algorithm2.copy()\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_algorithm[\"_ENV_NAME\"] = ENV_NAME_algorithm\n",
    "        CONFIG_algorithm[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_dataset + ENV_NAME_algorithm + \"/\"\n",
    "        )\n",
    "        \n",
    "        # Set number of clusters from the dataset configuration.\n",
    "        with open(PARENT_ENV_PATH_dataset + \"config.json\", \"r\") as file_a0:\n",
    "            nb_clusters = json.load(file_a0)[\"nb_clusters\"]\n",
    "        CONFIG_algorithm[\"clustering\"][\"nb_clusters\"] = nb_clusters\n",
    "        CONFIG_algorithm[\"clustering\"][\"_NAME\"] = CONFIG_algorithm[\"clustering\"][\"_TEMPNAME\"].format(nb_clusters)\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_algorithm[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_algorithm[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_a1:\n",
    "            json.dump(CONFIG_algorithm, file_a1)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### PREPROCESS DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Load dataset.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"../dict_of_texts.json\", \"r\"\n",
    "        ) as file_a2:\n",
    "            texts: Dict[str, str] = json.load(file_a2)\n",
    "\n",
    "        dict_of_preprocessed_texts: Dict[str, str] = {}\n",
    "\n",
    "        # Case with preprocessing.\n",
    "        if bool(CONFIG_algorithm[\"preprocessing\"][\"apply_preprocessing\"]):\n",
    "            dict_of_preprocessed_texts = preprocess(\n",
    "                dict_of_texts=texts,\n",
    "                apply_lemmatization=bool(CONFIG_algorithm[\"preprocessing\"][\"apply_lemmatization\"]),\n",
    "                apply_parsing_filter=bool(CONFIG_algorithm[\"preprocessing\"][\"apply_parsing_filter\"]),\n",
    "                spacy_language_model=str(CONFIG_algorithm[\"preprocessing\"][\"spacy_language_model\"]),\n",
    "            )\n",
    "\n",
    "        # Case without preprocessing.\n",
    "        else:\n",
    "            dict_of_preprocessed_texts = texts\n",
    "\n",
    "        # Store preprocessed texts.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"dict_of_preprocessed_texts.json\",\n",
    "            \"w\",\n",
    "        ) as file_a3:\n",
    "            json.dump(dict_of_preprocessed_texts, file_a3)\n",
    "            \n",
    "        ### ### ### ### ###\n",
    "        ### VECTORIZE DATASET.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Vectorize dataset.\n",
    "        dict_of_vectors: Dict[str, csr_matrix] = vectorize(\n",
    "            dict_of_texts=dict_of_preprocessed_texts,\n",
    "            vectorizer_type=str(CONFIG_algorithm[\"vectorization\"][\"vectorizer_type\"]),\n",
    "            spacy_language_model=str(CONFIG_algorithm[\"vectorization\"][\"spacy_language_model\"]),\n",
    "        )\n",
    "\n",
    "        # Store vectors.\n",
    "        with open(\n",
    "            str(CONFIG_algorithm[\"_ENV_PATH\"]) + \"dict_of_vectors.pkl\", \"wb\"\n",
    "        ) as file_a4:\n",
    "            pickle.dump(dict_of_vectors, file_a4)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Algorithm environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.3. Set `Error` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `algorithm` environments in which create `error` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of algorithm environments.\n",
    "LIST_OF_ALGORITHM_ENVIRONMENTS: List[str] = listing_envs.get_list_of_algorithm_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_ALGORITHM_ENVIRONMENTS)) + \"`\",\n",
    "    \"created algorithm environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_ALGORITHM_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environments with different uses of `error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_ERRORS: Dict[str, Dict[str, int]] = {\n",
    "    \"rate_{rate_str:.2f}-rand_{rand_str}-{fix_str}\".format(\n",
    "        rate_str=rate,\n",
    "        rand_str=rand,\n",
    "        fix_str=(\"with_fix\" if fix else \"without_fix\"),\n",
    "    ): {\n",
    "        \"_TYPE\": \"errors_simulation\",\n",
    "        \"_DESCRIPTION\": \"Random simulation of {rate_str:2d}% errors (random seed at {rand_str}) {fix_str}.\".format(\n",
    "            rate_str=int(100*rate),\n",
    "            rand_str=rand,\n",
    "            fix_str=(\"with conflicts fix\" if fix else \"without conflicts fix\")\n",
    "        ),\n",
    "        \"error_rate\": rate,\n",
    "        \"random_seed\": rand,\n",
    "        \"with_fix\": fix,\n",
    "    }\n",
    "    for rate in [0.0, 0.05, 0.10, 0.15, 0.20, 0.25,]\n",
    "    for rand in [1, 2,]\n",
    "    for fix in [True,]\n",
    "}\n",
    "ENVIRONMENTS_FOR_ERRORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `errors` environments using `ENVIRONMENTS_FOR_ERRORS` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_algorithm in LIST_OF_ALGORITHM_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_errors,\n",
    "        CONFIG_errors,\n",
    "    ) in ENVIRONMENTS_FOR_ERRORS.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_errors[\"_ENV_NAME\"] = ENV_NAME_errors\n",
    "        CONFIG_errors[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_algorithm + ENV_NAME_errors + \"/\"\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_errors[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_errors[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_errors[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_e1:\n",
    "            json.dump(CONFIG_errors, file_e1)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Errors environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 2.4. Set `Constraints selection` subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `Error` environments in which create `constraints selection` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of error environments.\n",
    "LIST_OF_ERROR_ENVIRONMENTS: List[str] = listing_envs.get_list_of_error_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_ERROR_ENVIRONMENTS)) + \"`\",\n",
    "    \"created error environments in `../experiments`\",\n",
    ")\n",
    "LIST_OF_ERROR_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION: Dict[str, Dict[str, int]] = {\n",
    "    # previous selection from \"1_convergence_study\" experiements with closest neighbors in different clusters sampling.\n",
    "    \"closest_250_{rand_str}\".format(rand_str=rand): {\n",
    "        \"_TYPE\": \"constraints_selection\",\n",
    "        \"_DESCRIPTION\": \"Selection from previous sampling then with closest neighbors in different clusters of 250 constraints (random seed at {rand_str})\".format(rand_str=rand),\n",
    "        # \"previous_file\": None,\n",
    "        \"constraints_step\": 250,  # like nb_to_select\n",
    "        \"next_sampling\": {\n",
    "            \"algorithm\": \"custom\",\n",
    "            \"init**kargs\": {\n",
    "                \"clusters_restriction\": \"different_clusters\",  # like \"closest_in_different_clusters\".\n",
    "                \"distance_restriction\": \"closest_neighbors\",  # like \"closest_in_different_clusters\".\n",
    "                \"without_added_constraints\": True,\n",
    "                \"without_inferred_constraints\": False,  # but without inference \"closest_in_different_clusters\".\n",
    "            },\n",
    "        },\n",
    "        \"random_seed\": rand,\n",
    "    }\n",
    "    for rand in [1, 2,]\n",
    "}\n",
    "len(ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `constraints selection` environments using `ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION` configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ### ### ### ###\n",
    "### LOOP FOR ALL ENVIRONMENTS CONFIGURED...\n",
    "### ### ### ### ###\n",
    "for PARENT_ENV_PATH_error in LIST_OF_ERROR_ENVIRONMENTS:\n",
    "    for (\n",
    "        ENV_NAME_constraints_selection,\n",
    "        CONFIG_constraints_selection,\n",
    "    ) in ENVIRONMENTS_FOR_CONSTRAINTS_SELECTION.items():\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### CREATE AND CONFIGURE ENVIRONMENT.\n",
    "        ### ### ### ### ###\n",
    "\n",
    "        # Name the configuration.\n",
    "        CONFIG_constraints_selection[\"_ENV_NAME\"] = ENV_NAME_constraints_selection\n",
    "        CONFIG_constraints_selection[\"_ENV_PATH\"] = (\n",
    "            PARENT_ENV_PATH_error + ENV_NAME_constraints_selection + \"/\"\n",
    "        )\n",
    "        \n",
    "        # Get dataset name and number of clusters from the dataset configuration.\n",
    "        with open(PARENT_ENV_PATH_error + \"../../config.json\", \"r\") as file_cs0:\n",
    "            config_dataset = json.load(file_cs0)\n",
    "            dataset_name: str = config_dataset[\"_ENV_NAME\"]\n",
    "\n",
    "        # Get algorithms names from the algorithms configuration.\n",
    "        with open(PARENT_ENV_PATH_error + \"../config.json\", \"r\") as file_cs1:\n",
    "            config_algorithm = json.load(file_cs1)\n",
    "            preprocessing_name: str = config_algorithm[\"preprocessing\"][\"_NAME\"]\n",
    "            vectorization_name: str = config_algorithm[\"vectorization\"][\"_NAME\"]\n",
    "            clustering_name: str = config_algorithm[\"clustering\"][\"_NAME\"]\n",
    "            sampling_name: str = config_algorithm[\"previous_sampling\"][\"_NAME\"]\n",
    "        \n",
    "        # Add previous file sampling.\n",
    "        CONFIG_constraints_selection[\"previous_file\"] = \"../previous/{dataset_str}/{prep_name_str}_-_{vect_name_str}_-_{samp_name_str}_-_{clust_name_str}_-_{rand_str}.json\".format(\n",
    "            dataset_str=str(dataset_name),\n",
    "            prep_name_str=str(preprocessing_name),\n",
    "            vect_name_str=str(vectorization_name),\n",
    "            samp_name_str=str(sampling_name),\n",
    "            clust_name_str=str(clustering_name),\n",
    "            rand_str=str(CONFIG_constraints_selection[\"random_seed\"]).zfill(4),\n",
    "        )\n",
    "\n",
    "        # Check if the environment already exists.\n",
    "        if os.path.exists(str(CONFIG_constraints_selection[\"_ENV_PATH\"])):\n",
    "            continue\n",
    "\n",
    "        # Create directory for this environment.\n",
    "        os.mkdir(str(CONFIG_constraints_selection[\"_ENV_PATH\"]))\n",
    "\n",
    "        # Store configuration file.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"]) + \"config.json\", \"w\"\n",
    "        ) as file_cs2:\n",
    "            json.dump(CONFIG_constraints_selection, file_cs2)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### LOAD PREVIOUS RESULTS.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        # Load previously samplied constraints (from \"1_convergence_study\" experiments).\n",
    "        list_of_previous_sampling: List[Tuple[str, str]] = []\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"previous_file\"]), \"r\"\n",
    "        ) as file_cs3:\n",
    "            dict_of_previous: Dict[str, Dict[str, Any]] = json.load(file_cs3)\n",
    "\n",
    "        ### ### ### ### ###\n",
    "        ### STORE NEEDED FILES.\n",
    "        ### ### ### ### ###\n",
    "        \n",
    "        # Store previous sampling.\n",
    "        list_of_previous_sampling: List[Tuple[str, str]] = []\n",
    "        for iteration in dict_of_previous[\"dict_of_constraints_annotations\"].keys():\n",
    "            for sample in dict_of_previous[\"dict_of_constraints_annotations\"][iteration]:\n",
    "                list_of_previous_sampling.append([sample[0], sample[1]])\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"]) + \"previous_sampling.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs4:\n",
    "            json.dump(list_of_previous_sampling, file_cs4)\n",
    "\n",
    "        # Store dictionary of samplings.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"])\n",
    "            + \"dict_of_samplings.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs5:\n",
    "            json.dump({\"000000\": []}, file_cs5)\n",
    "\n",
    "        # Store dictionary of errors.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"])\n",
    "            + \"dict_of_errors.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs6:\n",
    "            json.dump({\"000000\": []}, file_cs6)\n",
    "\n",
    "        # Store dictionary of constraints effective.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"])\n",
    "            + \"dict_of_constraints_effective.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs7:\n",
    "            json.dump({\"000000\": []}, file_cs7)\n",
    "\n",
    "        # Store dictionary of clustering results.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"])\n",
    "            + \"dict_of_clustering_results.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs8:\n",
    "            json.dump({\"000000\": dict_of_previous[\"dict_of_clustering_results\"][\"0000\"]}, file_cs8)\n",
    "\n",
    "        # Store dictionary of clustering performances.\n",
    "        with open(\n",
    "            str(CONFIG_constraints_selection[\"_ENV_PATH\"])\n",
    "            + \"dict_of_clustering_performances.json\",\n",
    "            \"w\",\n",
    "        ) as file_cs9:\n",
    "            json.dump({\"000000\": dict_of_previous[\"dict_of_clustering_performances\"][\"0000\"]}, file_cs9)\n",
    "\n",
    "# End\n",
    "print(\"\\n#####\")\n",
    "print(\"END - Constraints selection environments configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### 3. Get all created environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment environments.\n",
    "LIST_OF_EXPERIMENT_ENVIRONMENTS: List[str] = listing_envs.get_list_of_constraints_selection_env_paths()\n",
    "print(\n",
    "    \"There are\",\n",
    "    \"`\" + str(len(LIST_OF_EXPERIMENT_ENVIRONMENTS)) + \"`\",\n",
    "    \"created experiment environments in `../experiments`\",\n",
    ")\n",
    "#LIST_OF_EXPERIMENT_ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
