{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d5b24f",
   "metadata": {},
   "source": [
    "# ==== INTERACTIVE CLUSTERING : INTER-ANNOTATORS SCORE STUDY ====\n",
    "> ### Stage 1 : Analyze inter-annotators score and Plot some figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a43a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26800",
   "metadata": {},
   "source": [
    "## READ-ME BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6d2a",
   "metadata": {},
   "source": [
    "### Quick Description\n",
    "\n",
    "This notebook is **aimed at analyze inter-annotators scores on interactive clustering annotation experiments**.\n",
    "- Environments are represented by subdirectories in the `/experiments` folder.\n",
    "- Each subdirectories of `/experiments` folder represents an annotation experiment with several annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb0f5d",
   "metadata": {},
   "source": [
    "### Description each steps\n",
    "\n",
    "First of all, **load constraints annotated** from JSON files.\n",
    "\n",
    "Then, **compute a contengency matrix** with groundtruth and annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd89d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07718de9",
   "metadata": {},
   "source": [
    "## 1. IMPORT PYTHON DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f876a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import json\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from scipy import stats as scipystats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afbf86",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df5fdb",
   "metadata": {},
   "source": [
    "## 2. LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c7141",
   "metadata": {},
   "source": [
    "### 2.1. Load groundtruth from JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430c693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/texts.json\", \"r\") as fr_texts:\n",
    "    dict_of_texts: Dict[str, Dict[str, Union[str,bool]]] = json.load(fr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/dict_of_true_intents.json\", \"r\") as fr_intents:\n",
    "    dict_of_true_intents: Dict[str, str] = json.load(fr_intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc8374",
   "metadata": {},
   "source": [
    "### 2.2. Load constraints annotated from JSON file."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9b31543",
   "metadata": {},
   "source": [
    "# Case 1 : 1000 random annotations with 14 annotators.\n",
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/constraints_-_template_to_annotate_1.json\", \"r\") as fr_constraints_sampled:\n",
    "    dict_of_constraints_sampled: Dict[str, str] = json.load(fr_constraints_sampled)\n",
    "list_of_annotators: List[str] = [\"1.1\", \"2.1\", \"3.1\", \"4.1\", \"5.1\", \"6.1\", \"7.1\", \"8.1\", \"9.1\", \"10.1\", \"11.1\", \"12.1\", \"13.1\", \"14.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2 : 400 semi-random annotations (200 MUST_LINK, 200 CANNOT_LINK) with 4 annotators.\n",
    "with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/constraints_-_template_to_annotate_2.json\", \"r\") as fr_constraints_sampled:\n",
    "    dict_of_constraints_sampled: Dict[str, str] = json.load(fr_constraints_sampled)\n",
    "list_of_annotators: List[str] = [\"1.2\", \"7.2\", \"9.2\", \"12.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e224c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_constraints: Dict[str, Dict[str, Any]]= {}\n",
    "# Add groundtruth.\n",
    "dict_of_constraints[\"groundtruth\"] = {}\n",
    "for constraint_id, constraint in dict_of_constraints_sampled.items():\n",
    "    text_id1: str = constraint[\"data\"][\"id_1\"]\n",
    "    label_1: str = dict_of_true_intents[text_id1]\n",
    "    text_id2: str = constraint[\"data\"][\"id_2\"]\n",
    "    label_2: str = dict_of_true_intents[text_id2]\n",
    "    constraints_type: bool = (\n",
    "        \"MUST_LINK\"\n",
    "        if label_1 == label_2\n",
    "        else \"CANNOT_LINK\"\n",
    "    )\n",
    "    dict_of_constraints[\"groundtruth\"][constraint_id] = constraints_type\n",
    "\n",
    "# Add annotations.\n",
    "for annotator in list_of_annotators:\n",
    "    dict_of_constraints[annotator] = {}\n",
    "    with open(\"../experiments/mlsum_fr_train_subset_v1.0.0.schild/constraints_-_{0}.json\".format(annotator), \"r\") as fr_constraints_annotated:\n",
    "        constraints_annotated: Dict[str, Any] = json.load(fr_constraints_annotated)\n",
    "        for constraint_id, constraint in constraints_annotated.items():\n",
    "            if constraint_id in dict_of_constraints_sampled.keys():\n",
    "                dict_of_constraints[annotator][constraint_id] = constraint[\"constraint_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af626ddd",
   "metadata": {},
   "source": [
    "### 2.3. Format annotation in pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a6ca7",
   "metadata": {},
   "source": [
    "Define list of annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_annotators = [\"groundtruth\"] + list_of_annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cd18f",
   "metadata": {},
   "source": [
    "Format annotations with one annotator per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28581b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations_by_line = pd.DataFrame(\n",
    "    [\n",
    "        [annotation_id] + [dict_of_constraints[annotator][annotation_id] for annotator in list_of_annotators]\n",
    "        for annotation_id in dict_of_constraints[\"groundtruth\"].keys()\n",
    "    ],\n",
    "    columns=([\"annotation_id\"] + list_of_annotators)\n",
    ").set_index(\"annotation_id\")\n",
    "df_annotations_by_line.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ca3a6",
   "metadata": {},
   "source": [
    "Format annotations in only one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations_one_column = pd.DataFrame(\n",
    "    [\n",
    "        [annotator, annotation_id, annotation]\n",
    "        for annotator, annotations in dict_of_constraints.items()\n",
    "        for annotation_id, annotation in annotations.items()\n",
    "    ],\n",
    "    columns=[\"annotator\", \"annotation_id\", \"annotation\"]\n",
    ")\n",
    "df_annotations_one_column.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fdccf2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcdc40",
   "metadata": {},
   "source": [
    "## 3. COMPUTE INTER-ANNOTATORS SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad8672",
   "metadata": {},
   "source": [
    "### 3.1. Compute basic agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89c738",
   "metadata": {},
   "source": [
    "Define basic agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a35511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_basic_agreement(df: pd.DataFrame) -> str:\n",
    "    df[\"agreement\"] = df.apply(\n",
    "        lambda row: len(set([\n",
    "            row[annotator]\n",
    "            for annotator in df.columns\n",
    "            if (annotator in list_of_annotators) and (row[annotator] is not None)\n",
    "        ]))==1,\n",
    "        axis=1\n",
    "    )\n",
    "    return \"{0:.4f} (+/-{1:.4f})\".format(\n",
    "        np.mean(df[\"agreement\"]),\n",
    "        scipystats.sem(df[\"agreement\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f3010",
   "metadata": {},
   "source": [
    "Compute local basic agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff505e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic_agreement = pd.DataFrame(\n",
    "    [\n",
    "        [annotator1] + [\n",
    "            \"\" if (a1 <= a2) else evaluate_basic_agreement(df_annotations_by_line[[annotator1, annotator2]].copy())\n",
    "            for a2, annotator2 in enumerate(list_of_annotators)\n",
    "        ]\n",
    "        for a1, annotator1 in enumerate(list_of_annotators)\n",
    "        if annotator1 != \"groundtruth\"\n",
    "    ],\n",
    "    columns=[\"annotator\"]+list_of_annotators\n",
    ").set_index(\"annotator\")\n",
    "\n",
    "df_basic_agreement.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e23ab",
   "metadata": {},
   "source": [
    "Compute global basic agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_annotations_by_line[[\"7.2\", \"9.2\", \"12.2\"]].copy()\n",
    "print(\"Global inter-annotators basic agreement: agreement={0}\".format(\n",
    "    evaluate_basic_agreement(df),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_annotations_by_line.copy()\n",
    "print(\"Global inter-annotators basic agreement: agreement={0}\".format(\n",
    "    evaluate_basic_agreement(df),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10406e",
   "metadata": {},
   "source": [
    "### 3.2. Compute Cohen's Kappa"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41ad2467",
   "metadata": {},
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298d293",
   "metadata": {},
   "source": [
    "Define Cohen agreement."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fe68fb2",
   "metadata": {},
   "source": [
    "def evaluate_cohens_kappa_agreement(df: pd.DataFrame) -> str:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0fc89cd",
   "metadata": {},
   "source": [
    "sklearn.metrics.cohen_kappa_score(\n",
    "    y1=[x for i, x in enumerate(df_annotations_by_line[\"groundtruth\"]) if (df_annotations_by_line[\"1.2\"][i] is not None)],\n",
    "    y2=[x for i, x in enumerate(df_annotations_by_line[\"1.2\"]) if (x is not None)],\n",
    "    labels=[\"MUST_LINK\", \"CANNOT_LINK\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa346e",
   "metadata": {},
   "source": [
    "### 3.3. Compute Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67da9c",
   "metadata": {},
   "source": [
    "Define Krippendorff's alpha agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8077929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interprete_krippendorffs_alpha(alpha: float) -> str:\n",
    "    if alpha < 0.0 or 1.0 < alpha: raise ValueError(\"`alpha` must be a value between 0.0 and 1.0. Currently its `{0}`.\".format(alpha))\n",
    "    elif alpha < 0.667: return \"low\"\n",
    "    elif alpha < 0.8:   return \"acceptable\"\n",
    "    elif alpha < 1.0:   return \"good\"\n",
    "    else:               return \"perfect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_krippendorffs_alpha(df: pd.DataFrame) -> str:\n",
    "    alpha: float = simpledorff.calculate_krippendorffs_alpha_for_df(\n",
    "        df,\n",
    "        experiment_col=\"annotation_id\",\n",
    "        annotator_col=\"annotator\",\n",
    "        class_col=\"annotation\",\n",
    "    )\n",
    "    return \"{0:.4f} ({1})\".format(\n",
    "        alpha,\n",
    "        interprete_krippendorffs_alpha(alpha),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c56082",
   "metadata": {},
   "source": [
    "Compute local Krippendorff's alpha agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_krippendorff_alpha_agreement = pd.DataFrame(\n",
    "    [\n",
    "        [annotator1] + [\n",
    "            \"\" if (a1 <= a2) else evaluate_krippendorffs_alpha(df_annotations_one_column[df_annotations_one_column[\"annotator\"].isin([annotator1, annotator2])].copy())\n",
    "            for a2, annotator2 in enumerate(list_of_annotators)\n",
    "        ]\n",
    "        for a1, annotator1 in enumerate(list_of_annotators)\n",
    "        if annotator1 != \"groundtruth\"\n",
    "    ],\n",
    "    columns=[\"annotator\"]+list_of_annotators\n",
    ").set_index(\"annotator\")\n",
    "\n",
    "df_krippendorff_alpha_agreement.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971da31a",
   "metadata": {},
   "source": [
    "Compute global Krippendorff's alpha agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_annotations_one_column[df_annotations_one_column[\"annotator\"].isin([\"7.2\", \"9.2\", \"12.2\"])].copy()\n",
    "print(\"Global inter-annotators Krippendorff's alpha agreement: alpha={0}\".format(\n",
    "    evaluate_krippendorffs_alpha(df),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_annotations_one_column.copy()\n",
    "print(\"Global inter-annotators Krippendorff's alpha agreement: alpha={0}\".format(\n",
    "    evaluate_krippendorffs_alpha(df),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c725d8",
   "metadata": {},
   "source": [
    "### 3.3. Store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8313a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"../results/etude-robustesse-scores-inter-annotateurs.xlsx\") as writer:\n",
    "    df_basic_agreement.to_excel(\n",
    "        writer,\n",
    "        sheet_name=\"bsaic_agreement\",\n",
    "    )\n",
    "    df_krippendorff_alpha_agreement.to_excel(\n",
    "        writer,\n",
    "        sheet_name=\"krippendorff_alpha\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e65789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803e208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
